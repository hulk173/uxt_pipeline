# app_streamlit.py
from __future__ import annotations
import os, json, time, re, io, base64
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
import pandas as pd
import requests
import streamlit as st
import yaml

# =========================
# ---------- THEME --------
# =========================
st.set_page_config(
    page_title="UXT Pipeline ‚Ä¢ Pro Dashboard",
    page_icon="üîé",
    layout="wide",
    initial_sidebar_state="expanded",
)

# --- Small CSS polish ---
st.markdown(
    """
    <style>
    /* tighten default paddings */
    .block-container {padding-top: 1.2rem; padding-bottom: 2rem;}
    /* badge styles */
    .badge {display:inline-block; padding: 0.2rem .55rem; border-radius: 999px; font-size: 0.78rem; font-weight:600; vertical-align: middle;}
    .b-ok{background:#E9FFF2; color:#067647; border:1px solid #BBF7D0;}
    .b-warn{background:#FFF7E6; color:#A15C06; border:1px solid #FDE68A;}
    .b-err{background:#FFECEC; color:#B42318; border:1px solid #FCA5A5;}
    .b-info{background:#EEF6FF; color:#1D4ED8; border:1px solid #BFDBFE;}
    /* metric cards */
    .card{border:1px solid var(--secondary-background-color); border-radius:14px; padding:1rem 1.1rem; background:rgba(127,127,127,0.03);}
    .card h4{margin:0 0 .3rem 0;}
    .muted{color: var(--text-color-secondary, #5e6b7a); font-size: .9rem;}
    .chip{display:inline-flex; gap:.35rem; align-items:center; padding:.15rem .55rem; border-radius:999px; font-size:.78rem; border:1px solid rgba(125,125,125,.15)}
    .chip i{opacity:.7}
    .pill{font-weight:600; padding:.15rem .5rem; border-radius:999px; border:1px solid rgba(125,125,125,.2)}
    .tiny{font-size:.78rem;}
    .kbd{padding:.05rem .35rem; border:1px solid rgba(125,125,125,.4); border-radius:4px; font-family: ui-monospace, SFMono-Regular, Menlo, monospace; font-size:.82em;}
    /* tables */
    .stDataFrame {border-radius: 12px; overflow: hidden;}
    /* footer */
    .footer{opacity:.7; font-size:.85rem; padding-top: 1.2rem;}
    </style>
    """,
    unsafe_allow_html=True,
)

# =========================
# --------- CONFIG --------
# =========================
API_URL = os.environ.get("UXT_API_URL", "http://localhost:8000")
CFG_DIR = Path("configs"); CFG_DIR.mkdir(parents=True, exist_ok=True)
DEFAULT_CFG_PATH = CFG_DIR / "default.yaml"
LOCAL_CFG_PATH   = CFG_DIR / "local.yaml"

BASE_DEFAULTS: Dict[str, Any] = {
    "ingest": {"strategy": "fast", "ocr_languages": "eng+ukr"},
    "chunking": {
        "max_chars": 1000, "overlap": 120, "join_same_type": True,
        "min_text_chars": 30, "strip_whitespace": True
    },
    "index": {
        "backend": "faiss",
        "model_name": "sentence-transformers/all-MiniLM-L6-v2",
        "top_k": 5,
        "normalize": True
    },
}

def deep_merge(a: dict, b: dict) -> dict:
    out = dict(a)
    if not isinstance(b, dict): return out
    for k, v in b.items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = deep_merge(out[k], v)
        else:
            out[k] = v
    return out

def load_defaults() -> Dict[str, Any]:
    base = dict(BASE_DEFAULTS)
    if DEFAULT_CFG_PATH.exists():
        base = deep_merge(base, yaml.safe_load(DEFAULT_CFG_PATH.read_text()) or {})
    if LOCAL_CFG_PATH.exists():
        base = deep_merge(base, yaml.safe_load(LOCAL_CFG_PATH.read_text()) or {})
    return base

def save_defaults(cfg: Dict[str, Any]) -> None:
    LOCAL_CFG_PATH.write_text(yaml.safe_dump(cfg, sort_keys=False, allow_unicode=True), encoding="utf-8")

defaults = load_defaults()

# =========================
# --------- HEADER --------
# =========================
left, right = st.columns([0.75, 0.25], vertical_alignment="center")
with left:
    st.markdown("### üîé UXT Pipeline ‚Ä¢ **Pro Dashboard**")
    st.caption("–ú–µ—Ç–æ–¥ –æ–±—Ä–æ–±–∫–∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö: –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ‚Üí —á–∞–Ω–∫—ñ–Ω–≥ ‚Üí —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è ‚Üí –ø–æ—à—É–∫ / RAG-QA ‚Üí –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞.")
with right:
    ok = False
    try:
        r = requests.get(f"{API_URL}/health", timeout=3)
        ok = r.ok
    except Exception:
        ok = False
    st.markdown(
        f"""
        <div style="text-align:right">
            <span class="badge {'b-ok' if ok else 'b-err'}">
                API { 'ONLINE' if ok else 'OFFLINE'}
            </span>
            &nbsp;&nbsp;<span class="tiny">{API_URL}</span>
        </div>
        """,
        unsafe_allow_html=True,
    )

# HELP / STEPS
with st.expander("‚ÑπÔ∏è –Ø–∫ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å (–ø–æ–∫—Ä–æ–∫–æ–≤–æ) ‚Äî –Ω–∞—Ç–∏—Å–Ω—ñ—Ç—å –¥–ª—è —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó", expanded=False):
    st.markdown(
        """
1. **Ingest**: –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF/DOCX/HTML). –ó–∞ –ø–æ—Ç—Ä–µ–±–∏ –∑–º—ñ–Ω—ñ—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ –ª—ñ–≤—ñ–π –ø–∞–Ω–µ–ª—ñ.
2. **Preview**: –ø–µ—Ä–µ–≥–ª—è–Ω—å—Ç–µ –Ω–∞—Ä—ñ–∑–∞–Ω—ñ —á–∞–Ω–∫–∏. –ó–∞ –±–∞–∂–∞–Ω–Ω—è–º ‚Äî –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–π—Ç–µ –ø–æ doc_id –∞–±–æ —Ç–∏–ø—É.
3. **Search**: –≤–∏–∫–æ–Ω–∞–π—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ ‚Äî –∑–±—ñ–≥–∏ —É —Ç–µ–∫—Å—Ç—ñ –±—É–¥–µ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–æ.
4. **Ask**: –ø–æ—Å—Ç–∞–≤—Ç–µ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è (RAG-QA). –í—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–¥–µ –∑ **—Ü–∏—Ç–∞—Ç–∞–º–∏** –¥–∂–µ—Ä–µ–ª.
5. **Metrics**: –∑–∞–º—ñ—Ä—è–π—Ç–µ `recall@k` / `precision@k` –Ω–∞ –Ω–µ–≤–µ–ª–∏–∫–æ–º—É –∑–æ–ª–æ—Ç–æ–º—É –Ω–∞–±–æ—Ä—ñ.
6. **Clusters / Visualization**: –≥—Ä—É–ø—É–π—Ç–µ —Ç–∞ –¥–∏–≤—ñ—Ç—å—Å—è UMAP-–ø—Ä–æ–µ–∫—Ü—ñ—é –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤.
        """
    )

# =========================
# -------- SIDEBAR --------
# =========================
st.sidebar.subheader("‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")
strategy = st.sidebar.selectbox("–°—Ç—Ä–∞—Ç–µ–≥—ñ—è OCR", ["fast", "hi_res"], index=0, help="–®–≤–∏–¥–∫—ñ—Å—Ç—å/—è–∫—ñ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥—É (unstructured).")
ocr_languages = st.sidebar.text_input("OCR languages", value=defaults["ingest"]["ocr_languages"], help="–ù–∞–ø—Ä. eng+ukr, eng, deu‚Ä¶")

st.sidebar.markdown("---")
st.sidebar.caption("–ß–∞–Ω–∫—ñ–Ω–≥")
max_chars       = st.sidebar.slider("max_chars", 300, 2000, int(defaults["chunking"]["max_chars"]), 50)
overlap         = st.sidebar.slider("overlap", 0, 400, int(defaults["chunking"]["overlap"]), 10)
join_same_type  = st.sidebar.checkbox("join_same_type", value=bool(defaults["chunking"]["join_same_type"]))
min_text_chars  = st.sidebar.slider("min_text_chars", 0, 200, int(defaults["chunking"]["min_text_chars"]), 5)
strip_whitespace= st.sidebar.checkbox("strip_whitespace", value=bool(defaults["chunking"]["strip_whitespace"]))

st.sidebar.markdown("---")
st.sidebar.caption("–Ü–Ω–¥–µ–∫—Å")
index_backend = st.sidebar.selectbox("backend", ["faiss", "sklearn"], index=0, help="–ü—ñ–¥ –∫–∞–ø–æ—Ç–æ–º: FAISS/Sklearn.")
model_name = st.sidebar.selectbox(
    "Sentence model",
    [
        "sentence-transformers/all-MiniLM-L6-v2",
        "sentence-transformers/all-MiniLM-L12-v2",
        "sentence-transformers/paraphrase-MiniLM-L6-v2",
    ], index=0
)
top_k = st.sidebar.slider("top_k", 1, 20, int(defaults["index"]["top_k"]), 1)
normalize = st.sidebar.checkbox("normalize (L2)", value=bool(defaults["index"]["normalize"]), help="–†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–ª—è cosine/IP.")

st.sidebar.markdown("---")
colsa, colsb = st.sidebar.columns(2)
with colsa:
    if st.button("üíæ Save defaults"):
        cfg = {
            "ingest": {"strategy": strategy, "ocr_languages": ocr_languages},
            "chunking": {
                "max_chars": int(max_chars), "overlap": int(overlap),
                "join_same_type": bool(join_same_type),
                "min_text_chars": int(min_text_chars),
                "strip_whitespace": bool(strip_whitespace),
            },
            "index": {
                "backend": index_backend, "model_name": model_name,
                "top_k": int(top_k), "normalize": bool(normalize)
            },
        }
        save_defaults(cfg)
        st.sidebar.success("–ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ configs/local.yaml")
with colsb:
    if st.button("üßπ Clear outputs"):
        try:
            r = requests.delete(f"{API_URL}/outputs"); st.sidebar.success(r.json())
        except Exception as ex:
            st.sidebar.error(ex)

# =========================
# ---------- TABS ---------
# =========================
tabs = st.tabs(["üì• Ingest", "üëÅ Preview", "üîé Search", "üí¨ Ask", "üìä Metrics", "üóÇ Clusters", "üó∫ Visualization", "üï∞ History"])
tab_ingest, tab_preview, tab_search, tab_ask, tab_metrics, tab_clusters, tab_viz, tab_hist = tabs

# ---------- helpers ----------
def highlight(text: str, query: str) -> str:
    if not query: return text
    try:
        patt = re.compile(re.escape(query), re.IGNORECASE)
        return patt.sub(lambda m: f"**{m.group(0)}**", text)
    except Exception:
        return text

def df_to_csv_download(df: pd.DataFrame, label: str, filename: str):
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button(label, data=csv, file_name=filename, mime="text/csv")

def code_copy_box(text: str, label: str="Copy"):
    st.text_area(label, value=text, height=80)

# =========================
# -------- INGEST ----------
# =========================
with tab_ingest:
    st.markdown("#### 1) –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –Ω–∞—Ä—ñ–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
    left, right = st.columns([0.6, 0.4], vertical_alignment="bottom")

    with left:
        f = st.file_uploader("–û–±—Ä–∞—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç", type=["pdf", "docx", "html", "txt"])
        st.caption("–õ—ñ–º—ñ—Ç 200MB/—Ñ–∞–π–ª ‚Ä¢ –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ PDF/DOCX/HTML/TXT")
    with right:
        st.markdown("<div class='card'><h4>–ü–æ—Ä–∞–¥–∏</h4><div class='muted'>‚Ä¢ –î–ª—è —Å–∫–∞–Ω—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ <b>hi_res</b> —Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É –º–æ–≤—É OCR.<br>‚Ä¢ –ù–µ –∑–∞–Ω–∏–∂—É–π—Ç–µ <b>min_text_chars</b> ‚Äî —Ü–µ –∑–º–µ–Ω—à–∏—Ç—å —à—É–º.<br>‚Ä¢ –î–ª—è cosine —Å—Ö–æ–∂–æ—Å—Ç—ñ –∑–∞–ª–∏—à—Ç–µ <b>normalize</b> —É–≤—ñ–º–∫–Ω–µ–Ω–∏–º.</div></div>", unsafe_allow_html=True)

    c1, c2, c3 = st.columns([0.18, 0.18, 0.64])
    with c1:
        run = st.button("üöÄ Ingest", type="primary", disabled=f is None)
    with c2:
        batch = st.file_uploader("Batch (multi)", accept_multiple_files=True, label_visibility="collapsed")

    if run and f is None:
        st.warning("–û–±–µ—Ä—ñ—Ç—å —Ñ–∞–π–ª.")
    elif run and f is not None:
        files = {"file": (f.name or "uploaded.bin", f.getvalue())}
        data = {
            "strategy": strategy, "ocr_languages": ocr_languages,
            "max_chars": int(max_chars), "overlap": int(overlap),
            "join_same_type": json.dumps(bool(join_same_type)),
            "min_text_chars": int(min_text_chars),
            "strip_whitespace": json.dumps(bool(strip_whitespace)),
            "index_backend": index_backend, "model_name": model_name,
            "top_k": int(top_k), "normalize": json.dumps(bool(normalize)),
        }
        try:
            r = requests.post(f"{API_URL}/ingest", files=files, data=data, timeout=60)
            if not r.ok:
                st.error(r.text)
            else:
                job_id = r.json()["job_id"]
                st.info(f"job: {job_id}")
                with st.spinner("–û–±—Ä–æ–±–∫–∞‚Ä¶"):
                    for _ in range(180):
                        s = requests.get(f"{API_URL}/job/{job_id}", timeout=10).json()
                        if s.get("status") == "finished": st.success("–ì–æ—Ç–æ–≤–æ!"); st.json(s); break
                        if s.get("status") == "error": st.error(s.get("error")); break
                        time.sleep(5)
        except Exception as ex:
            st.error(ex)

    if batch:
        files = [("files", (x.name or "file", x.getvalue())) for x in batch]
        data = {
            "strategy": strategy, "ocr_languages": ocr_languages,
            "max_chars": int(max_chars), "overlap": int(overlap),
            "join_same_type": json.dumps(bool(join_same_type)),
            "min_text_chars": int(min_text_chars),
            "strip_whitespace": json.dumps(bool(strip_whitespace)),
            "index_backend": index_backend, "model_name": model_name,
            "top_k": int(top_k), "normalize": json.dumps(bool(normalize)),
        }
        if st.button("üì¶ Ingest batch", type="secondary"):
            r = requests.post(f"{API_URL}/ingest_batch", files=files, data=data, timeout=180)
            st.json(r.json())

# =========================
# -------- PREVIEW --------
# =========================
with tab_preview:
    st.markdown("#### 2) –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø–µ—Ä–µ–≥–ª—è–¥ —á–∞–Ω–∫—ñ–≤")
    p = Path("data/out/chunks.jsonl")
    if not p.exists():
        st.info("–ù–µ–º–∞—î `data/out/chunks.jsonl`. –ó—Ä–æ–±—ñ—Ç—å Ingest.")
    else:
        rows = [json.loads(x) for x in p.read_text(encoding="utf-8").splitlines() if x.strip()]
        df = pd.DataFrame(rows)
        st.caption(f"–†—è–¥–∫—ñ–≤: {len(df)}")
        colf1, colf2, colf3 = st.columns(3)
        with colf1:
            doc_filter = st.text_input("–§—ñ–ª—å—Ç—Ä doc_id (regex)", "")
        with colf2:
            type_filter = st.text_input("–§—ñ–ª—å—Ç—Ä type (regex)", "")
        with colf3:
            text_filter = st.text_input("–ü–æ—à—É–∫ —É —Ç–µ–∫—Å—Ç—ñ (regex)", "")

        fdf = df.copy()
        if doc_filter:  fdf = fdf[fdf["doc_id"].astype(str).str.contains(doc_filter, regex=True, na=False)]
        if type_filter: fdf = fdf[fdf["type"].astype(str).str.contains(type_filter, regex=True, na=False)]
        if text_filter: fdf = fdf[fdf["text"].astype(str).str.contains(text_filter, regex=True, na=False)]

        st.dataframe(fdf, use_container_width=True, height=420)
        df_to_csv_download(fdf, "‚¨áÔ∏è Export CSV", "chunks_filtered.csv")

# =========================
# -------- SEARCH ---------
# =========================
with tab_search:
    st.markdown("#### 3) –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫")
    q = st.text_input("–ó–∞–ø–∏—Ç")
    if st.button("üîé Search", disabled=not q):
        try:
            res = requests.get(f"{API_URL}/search", params={"q": q}, timeout=60).json()
            if not res:
                st.info("–ù—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            for it in res:
                score = float(it["score"])
                badge = "b-ok" if score >= 0.65 else ("b-warn" if score >= 0.45 else "b-info")
                st.markdown(f"<span class='badge {badge}'>score={score:.3f}</span> &nbsp; <span class='pill'>{it['chunk']['doc_id']}</span>", unsafe_allow_html=True)
                st.markdown(highlight(it["chunk"]["text"], q))
                with st.expander("üìé –¶–∏—Ç–∞—Ç–∞ ‚Ä¢ copy"):
                    code_copy_box(it["chunk"]["text"], label="–ü—ñ–¥ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è")
                st.divider()
        except Exception as ex:
            st.error(ex)

# =========================
# ---------- ASK ----------
# =========================
with tab_ask:
    st.markdown("#### 4) RAG-QA ‚Äî –ø–æ—Å—Ç–∞–≤—Ç–µ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –¥–æ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
    cq1, cq2 = st.columns([0.7, 0.3])
    with cq1:
        qq = st.text_input("–ó–∞–ø–∏—Ç–∞–Ω–Ω—è")
    with cq2:
        kk = st.slider("k (–∫–æ–Ω—Ç–µ–∫—Å—Ç)", 1, 10, int(top_k), 1)

    if st.button("üí¨ Ask", disabled=not qq):
        data = requests.get(f"{API_URL}/ask", params={"q": qq, "k": kk}, timeout=180).json()
        st.markdown(f"##### –í—ñ–¥–ø–æ–≤—ñ–¥—å")
        st.markdown(data.get("answer", ""))
        st.markdown("##### –î–∂–µ—Ä–µ–ª–∞")
        src = pd.DataFrame(data.get("sources", []))
        if len(src):
            st.dataframe(src, use_container_width=True, height=240)
            df_to_csv_download(src, "‚¨áÔ∏è Export sources", "ask_sources.csv")
        else:
            st.info("–¶–∏—Ç–∞—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ (–ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ —ñ–Ω–¥–µ–∫—Å/—á–∞–Ω–∫–∏).")

# =========================
# -------- METRICS --------
# =========================
with tab_metrics:
    st.markdown("#### 5) –ú–µ—Ç—Ä–∏–∫–∏ —à–≤–∏–¥–∫–æ—ó –æ—Ü—ñ–Ω–∫–∏ —è–∫–æ—Å—Ç—ñ")
    m1, m2 = st.columns(2)
    with m1: mq = st.text_area("Query", "")
    with m2: expected = st.text_area("Expected snippet", "")
    k = st.slider("k", 1, 20, int(top_k), 1, help="–°–∫—ñ–ª—å–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏.")
    if st.button("üèÅ Evaluate", disabled=not mq):
        data = {"query": mq, "expected_text": expected, "k": k}
        out = requests.post(f"{API_URL}/evaluate", json=data, timeout=60).json()
        c1, c2, c3 = st.columns(3)
        with c1: st.metric("recall@k", f"{out['recall@k']:.3f}")
        with c2: st.metric("precision@k", f"{out['precision@k']:.3f}")
        with c3: st.metric("k", out["k"])
        st.dataframe(pd.DataFrame(out["results"]), use_container_width=True, height=280)

# =========================
# ------- CLUSTERS --------
# =========================
with tab_clusters:
    st.markdown("#### 6) –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è —á–∞–Ω–∫—ñ–≤")
    nc = st.slider("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤", 2, 30, 8, 1)
    if st.button("üóÇ Compute clusters"):
        data = requests.get(f"{API_URL}/clusters", params={"n_clusters": nc}, timeout=180).json()
        if "clusters" in data:
            df = pd.DataFrame(data["clusters"])
            st.dataframe(df, use_container_width=True, height=400)
            df_to_csv_download(df, "‚¨áÔ∏è Export clusters", "clusters.csv")
        else:
            st.info(data)

# =========================
# ------ VISUALIZATION ----
# =========================
with tab_viz:
    st.markdown("#### 7) –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤ (UMAP)")
    p = Path("data/out/chunks.jsonl")
    if not p.exists():
        st.info("–ù–µ–º–∞—î `data/out/chunks.jsonl`. –ó—Ä–æ–±—ñ—Ç—å Ingest.")
    else:
        rows = [json.loads(x) for x in p.read_text(encoding="utf-8").splitlines() if x.strip()]
        texts = [r.get("text", "") for r in rows if r.get("text")]
        ids = [r.get("id") for r in rows if r.get("text")]
        if not texts:
            st.warning("–ù–µ–º–∞—î –ø—Ä–∏–¥–∞—Ç–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤.")
        else:
            vis_model = st.selectbox(
                "–ú–æ–¥–µ–ª—å",
                [
                    "sentence-transformers/all-MiniLM-L6-v2",
                    "sentence-transformers/all-MiniLM-L12-v2",
                    "sentence-transformers/paraphrase-MiniLM-L6-v2",
                ], index=0
            )
            nmax = st.slider("–ú–∞–∫—Å–∏–º—É–º —Ç–æ—á–æ–∫", 200, 5000, min(1200, len(texts)), 100)
            if st.button("üó∫ Compute & Plot", type="primary"):
                try:
                    from sentence_transformers import SentenceTransformer
                    import umap, plotly.express as px
                except Exception as ex:
                    st.error("–ù–µ–æ–±—Ö—ñ–¥–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏: sentence-transformers, umap-learn, plotly"); st.stop()
                model = SentenceTransformer(vis_model)
                X = model.encode(texts[:nmax], convert_to_numpy=True, show_progress_bar=True)
                reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric="cosine", random_state=42)
                emb2d = np.asarray(reducer.fit_transform(X), dtype=float)
                dfv = pd.DataFrame({"x": emb2d[:, 0], "y": emb2d[:, 1], "id": ids[:nmax], "text": texts[:nmax]})
                fig = px.scatter(dfv, x="x", y="y", hover_data=["id", "text"], title="UMAP Embeddings")
                st.plotly_chart(fig, use_container_width=True)

# =========================
# -------- HISTORY --------
# =========================
with tab_hist:
    st.markdown("#### 8) –Ü—Å—Ç–æ—Ä—ñ—è –ø–æ–¥—ñ–π")
    try:
        hist = requests.get(f"{API_URL}/history", timeout=10).json()
    except Exception as ex:
        hist = []
    if not hist:
        st.info("–Ü—Å—Ç–æ—Ä—ñ—è –ø–æ—Ä–æ–∂–Ω—è. –ó–¥—ñ–π—Å–Ω—ñ—Ç—å ingest/search/ask/evaluate.")
    else:
        dfh = pd.DataFrame(hist)
        st.dataframe(dfh, use_container_width=True, height=350)
        df_to_csv_download(dfh, "‚¨áÔ∏è Export history", "history.csv")

# =========================
# -------- FOOTER ---------
# =========================
st.markdown(
    """
    <div class="footer">
        <span class="muted">–ì–æ—Ç–æ–≤–æ ‚úÖ ‚Ä¢ –î–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–≥–æ –∑–≤—ñ—Ç—É —Å–∫—Ä—ñ–Ω–∏: Ingest ‚Üí Preview ‚Üí Search ‚Üí Ask ‚Üí Metrics ‚Üí Clusters ‚Üí Visualization.</span>
        <br><span class="muted">–ü–æ—Ä–∞–¥–∞: –¥–ª—è —É–∫—Ä–∞—ó–Ω–æ–º–æ–≤–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø–∞–º‚Äô—è—Ç–∞–π—Ç–µ –ø—Ä–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ OCR-–º–æ–≤–∏ —Ç–∞ –º–æ–∂–ª–∏–≤–æ –±—ñ–ª—å—à–∏–π max_chars (800‚Äì1200).</span>
    </div>
    """,
    unsafe_allow_html=True,
)
