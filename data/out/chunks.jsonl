{"id": "1_docx#1", "doc_id": "1_docx", "text": "Державний торговельно-економічний університет\nКафедра комп’ютерних наук та інформаційних систем\nКВАЛІФІКАЦІЙНА РОБОТА\nна тему:\n«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.030929", "order": 1, "lang": "uk"}}
{"id": "1_docx#2", "doc_id": "1_docx", "text": "Студента 2 курсу, 6м групи спеціальності 122 «Комп’ютерні науки» __________ підпис студента Оліфіренко Кирило Андрійович Науковий керівник кандидат фізико-математичних наук, доцент ___________ підпис керівника Томашевська Тетяна Володимирівна Гарант освітньої програми доктор фізико-математичних наук, професор ___________ підпис керівника Пурський Олег Іванович", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.033986", "order": 2, "lang": "uk"}}
{"id": "1_docx#3", "doc_id": "1_docx", "text": "Київ 2025\nДержавний торговельно-економічний університет\nФакультет інформаційних технологій\nКафедра комп’ютерних наук та інформаційних систем\nСпеціальність 122 «Комп’ютерні науки»\nОсвітня програма «Комп’ютерні науки»\nЗатверджую\nЗав. кафедри ____________Пурський О.І.\n«» грудня 2025р.\nЗавдання\nна кваліфікаційну роботу студенту\nОліфіренко Кирилу Андрійовичу\n(прізвище, ім’я, по батькові)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.036855", "order": 3, "lang": "uk"}}
{"id": "1_docx#4", "doc_id": "1_docx", "text": "Тема кваліфікаційної роботи", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.038065", "order": 4, "lang": "uk"}}
{"id": "1_docx#5", "doc_id": "1_docx", "text": "«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»\nЗатверджена наказом ректора від «» листопада 2025 р. № 4142", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.043786", "order": 5, "lang": "uk"}}
{"id": "1_docx#6", "doc_id": "1_docx", "text": "2. Строк здачі студентом закінченої роботи 15 листопада 2025 року", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.045846", "order": 6, "lang": "uk"}}
{"id": "1_docx#7", "doc_id": "1_docx", "text": "3. Цільова установка та вихідні дані до роботи\nМета роботи: розробка моделі та інформаційної технології оцінки конкурентоспроможності підприємств електронної торгівлі.\nОб’єкт дослідження: автоматизація процесів оцінювання конкурентоспроможності підприємств електронної торгівлі.\nПредмет дослідження: моделі, методи та інформаційні технології оцінювання конкурентоспроможності.\n4. Перелік графічного матеріалу______________________________________ ______________________________________________________________________________________________________________________________________________________________________________________________________\n5. Консультанти по роботі із зазначенням розділів, за якими здійснюється консультування:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.049173", "order": 7, "lang": "uk"}}
{"id": "1_docx#8", "doc_id": "1_docx", "text": "Розділ Консультант (прізвище, ініціали) Підпис, дата Завдання видав Завдання прийняв 1 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 2 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 3 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р.", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.051788", "order": 8, "lang": "uk"}}
{"id": "1_docx#9", "doc_id": "1_docx", "text": "6. Зміст кваліфікаційної роботи (перелік питань за кожним розділом)\nВСТУП\nРОЗДІЛ 1. ТЕОРЕТИЧНІ АСПЕКТИ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВА\n1.1. Аналіз проблематики та існуючих методів управління конкурентоспроможністю\n1.2. Особливості оцінки та управління конкурентоспроможністю підприємств електронної комерції\n1.3. Концептуальна модель оцінки та управління конкурентоспроможністю підприємств електронної комерції\nРОЗДІЛ 2. МАТЕМАТИЧНІ МОДЕЛІ ОЦІНКИ ТА УПРАВЛІННЯ КОНКУРЕНТОСПРОМОЖНІСТЮ ПІДПРИЄМСТВА\n2.1. Система показників та модель оцінки конкурентоспроможності підприємств електронної комерції\n2.2. Модель управління конкурентоспроможністю підприємства\n2.3. Моделювання процесу оцінки конкурентоспроможності підприємства\nРОЗДІЛ 3. ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВ ЕЛЕКТРОННОЇ ТОРГІВЛІ\n3.1. Інформаційно-логічна модель системи оцінки конкурентоспроможності підприємств\n3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств\n3.3.Технологія використання інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.057318", "order": 9, "lang": "uk"}}
{"id": "1_docx#10", "doc_id": "1_docx", "text": "інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання роботи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.059716", "order": 10, "lang": "uk"}}
{"id": "1_docx#11", "doc_id": "1_docx", "text": "№ Пор. Назва етапів кваліфікаційної роботи Строк виконання етапів роботи За планом фактично 1 2 3 4 1 Вибір теми кваліфікаційної роботи 01.11.2023 01.11.2023 2 Розробка та затвердження завдання на кваліфікаційну роботу 05.12.2023 05.12.2023 3 Вступ 01.05.2024 01.05.2024 4 РОЗДІЛ 1. Теоретичні аспекти оцінки конкурентоспроможності підприємства 14.06.2024 14.06.2024 5 Підготовка статті у збірник наукових статей магістрів 20.06.2024 20.06.2024 6 РОЗДІЛ 2. Математичні моделі оцінки та управління конкурентоспроможністю підприємства 05.09.2024 05.09.2024 7 РОЗДІЛ 3. Інформаційна технологія оцінки конкурентоспроможності підприємств електронної торгівлі 17.10.2024 17.10.2024 8 Висновки 21.10.2024 21.10.2024 9 Здача кваліфікаційної роботи на кафедру науковому керівнику 23.10.2024 23.10.2024 10 Попередній захист кваліфікаційної роботи 28.10.2024 28.10.2024 11 Виправлення зауважень, зовнішнє рецензування кваліфікаційної роботи 30.10.2024 30.10.2024 12 Представлення готової зшитої кваліфікаційної роботи на кафедру 04.11.2024 04.11.2024 13 Публічний захист кваліфікаційної роботи За розкладом роботи ЕК", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.065125", "order": 11, "lang": "uk"}}
{"id": "1_docx#12", "doc_id": "1_docx", "text": "8. Дата видачі завдання «5» грудня 2023 р", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.066727", "order": 12, "lang": "uk"}}
{"id": "1_docx#13", "doc_id": "1_docx", "text": "9. Керівник випускного кваліфікаційної роботи Самойленко Г.Т\n(прізвище, ініціали, підпис)\n10. Гарант освітньої програми Пурський О.І.\n(прізвище, ініціали, підпис)\n11. Завдання прийняв до виконання студент Оліфіренко К. ________\n(прізвище, ініціали, підпис)\n12. Відгук керівника кваліфікаційної роботи\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.069997", "order": 13, "lang": "uk"}}
{"id": "1_docx#14", "doc_id": "1_docx", "text": "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\nКерівник кваліфікаційної роботи\n______________________\n(підпис, дата)\n13. Висновок про кваліфікаційну роботу\nКваліфікаційна робота студента _______ Оліфіренко К.А.\n(прізвище, ініціали)\nможе бути допущена до захисту в екзаменаційній комісії.\nГарант освітньої програми Пурський О.І.\n(підпис, прізвище, ініціали)\nЗавідувач кафедри Пурський О.І.\n(підпис, прізвище, ініціали)\n«_____»_________________2025 р.\nАнотація\nУ кваліфікаційній роботі здійснено комплексну розробку моделей та інформаційної технології моніторингу соціально-економічних показників з", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.073453", "order": 14, "lang": "uk"}}
{"id": "1_docx#15", "doc_id": "1_docx", "text": "метою підвищення ефективності управління регіональним розвитком. Теоретично обґрунтовано основні положення формування і проведення соціально-економічного моніторингу та запропоновано концепцію створення інформаційної системи оціньовання показників соціально-економічного розвитку регіону. Розроблено метод автоматизованого розрахунку комплексної оцінки показників соціально-економічного розвитку. Створено автоматизовану Web-систему оціньовання показників соціально-економічного розвитку регіонів України.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.078218", "order": 15, "lang": "uk"}}
{"id": "1_docx#16", "doc_id": "1_docx", "text": "Ключові слова: соціально-економічний розвиток, математична модель, інтегральні показники, інформаційна технологія.\nAnotation", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.080349", "order": 16, "lang": "uk"}}
{"id": "1_docx#17", "doc_id": "1_docx", "text": "The qualification work is devoted to development of model and information technology of monitoring of social and economic indexes for the purpose of management efficiency increase by regional development The mechanism of the taking into account of differentiation of regional development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of social and economic development is offered and programmed. The Web-system for monitoring indicators of social and economic development of Ukraine regions is created and the technology of its use is developed.\nKeywords: social and economic development, mathematical model, integrated indicators, information technology.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.084024", "order": 17, "lang": "en"}}
{"id": "1_docx#18", "doc_id": "1_docx", "text": "ЗМІСТ\nВСТУП…………………………………………………..…………………………9\nРОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ…………….……..………….12\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних……………………………………………………..……………….….12", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.088289", "order": 18, "lang": "uk"}}
{"id": "1_docx#19", "doc_id": "1_docx", "text": "1.2 Поняття неструктурованих даних, їх роль у сучасних інформаційних системах………………………………………………...………….………..16", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.089992", "order": 19, "lang": "uk"}}
{"id": "1_docx#20", "doc_id": "1_docx", "text": "1.3 Методи та підходи до обробки текстових даних у системах штучного інтелекту ………………………………………………..…………………...20", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.093845", "order": 20, "lang": "uk"}}
{"id": "1_docx#21", "doc_id": "1_docx", "text": "1.4 Алгоритми й моделі обробки природної мови (NLP).……...………...20\n1.5 Огляд інструментів і бібліотек для роботи з текстовими даними………………………………………………...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.096116", "order": 21, "lang": "uk"}}
{"id": "1_docx#22", "doc_id": "1_docx", "text": "1.5.1 Традиційні бібліотеки Python для NLP (NLTK, spaCy, scikit-learn)……………………..…..……………...…………………………….....23", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.098435", "order": 22, "lang": "uk"}}
{"id": "1_docx#23", "doc_id": "1_docx", "text": "1.5.2 Сучасні фреймворки, сервіси (Transformers, OpenAI API, LangChain, Unstructured)……………………..…..…………………….…...23\n1.5.3 Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)….……………………………………...23\n1.6 Проблематика та напрями вдосконалення методів обробки неструктурованих даних………………………...…...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.100934", "order": 23, "lang": "uk"}}
{"id": "1_docx#24", "doc_id": "1_docx", "text": "1.7 Висновки до розділу 1……………………………..…………………...20\nРОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………………………………………………….…..26\n2.1 Постановка задачі, вимоги та функціональні можливості системи…26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.103053", "order": 24, "lang": "uk"}}
{"id": "1_docx#25", "doc_id": "1_docx", "text": "2.2 Архітектура методу обробки неструктурованих текстових даних…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.108345", "order": 25, "lang": "uk"}}
{"id": "1_docx#26", "doc_id": "1_docx", "text": "2.3. Розробка алгоритму розбиття документів на семантичні фрагменти (чанки)………………………………………………………………….……33\n2.4 Методи векторизації тексту та побудова семантичного індексу…...…………………….………………………………….…………26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.111541", "order": 26, "lang": "uk"}}
{"id": "1_docx#27", "doc_id": "1_docx", "text": "2.5 Реалізація бази знань та пошукового механізму ……..…………..…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.112887", "order": 27, "lang": "uk"}}
{"id": "1_docx#28", "doc_id": "1_docx", "text": "2.6 Висновки до розділу 2…………………………...……..…………..…..30\nРОЗДІЛ 3. ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ІНТЕРФЕЙС СИСТЕМИ………………………………………………………….....…………36\n3.1 Загальна структура програмного комплексу UXText Pipeline.………36", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.119817", "order": 28, "lang": "uk"}}
{"id": "1_docx#29", "doc_id": "1_docx", "text": "3.2 Інтеграція з бібліотекою Unstructured.io та FAISS ………….………..38\n3.3 Реалізація API-сервісу (FastAPI) для обробки запитів.………....……42\n3.4 Розробка користувацького інтерфейсу (Streamlit)…….…….………..38", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.121418", "order": 29, "lang": "uk"}}
{"id": "1_docx#30", "doc_id": "1_docx", "text": "3.5 Візуалізація результатів аналізу текстів .………...………...…………42\n3.6 Тестування та оцінювання ефективності роботи системи.……..……42\n3.7 Висновки до розділу 3.………...………………………….....…………42\nВИСНОВКИ……………………………..………………………………………48\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ……………...……………………..50\nДОДАТОК………………………………………………..……………..………52\nВСТУП", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.123706", "order": 30, "lang": "uk"}}
{"id": "1_docx#31", "doc_id": "1_docx", "text": "У сучасному світі, коли інформаційні потоки стрімко зростають, значна частина даних, які створює людина чи система, має неструктурований характер. Це тексти документів, електронні листи, новини, пости у соціальних мережах, коментарі, відгуки, звіти чи наукові статті. За статистичними оцінками, понад 80 відсотків усієї цифрової інформації становлять саме неструктуровані дані, які не мають чіткої схеми чи фіксованої структури зберігання. Проте саме вони містять найціннішу інформацію для аналізу змісту, виявлення тенденцій, автоматизованого прийняття рішень чи побудови систем штучного інтелекту. Проблема полягає в тому, що традиційні інструменти обробки даних не пристосовані для ефективної роботи з такими джерелами. Методи на кшталт статистичного аналізу або пошуку за ключовими словами не здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’язки між ними.\nАктуальність теми зумовлена необхідністю створення ефективних методів, що дозволяють обробляти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штуч", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.131177", "order": 31, "lang": "uk"}}
{"id": "1_docx#32", "doc_id": "1_docx", "text": "яти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штучного інтелекту, зокрема векторизації текстів і нейромережевих моделей природної мови (NLP), відкриває нові можливості для створення інтелектуальних систем аналізу тексту. Ці системи здатні не лише класифікувати чи шукати інформацію, а й знаходити семантичні зв’язки між фрагментами тексту, узагальнювати їх зміст і будувати рекомендації. Саме тому тема «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» є актуальною і відповідає сучасним тенденціям розвитку інформаційних технологій.\nМетою даної роботи є розробка методу та програмного забезпечення, що забезпечують ефективну обробку неструктурованих текстових даних шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завдання: провести аналіз існуючих методів обробки текстових даних і", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.136596", "order": 32, "lang": "uk"}}
{"id": "1_docx#33", "doc_id": "1_docx", "text": "сучасних підходів до представлення тексту; дослідити технології побудови векторних моделей, зокрема SentenceTransformers та FAISS; спроєктувати архітектуру програмного комплексу для інтеграції різних джерел даних; реалізувати систему UXText Pipeline, яка поєднує модулі обробки, зберігання та пошуку текстів; провести експериментальне тестування для оцінки ефективності запропонованого методу.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.139705", "order": 33, "lang": "uk"}}
{"id": "1_docx#34", "doc_id": "1_docx", "text": "Об’єктом дослідження є процес обробки неструктурованих текстових даних у системах аналітики. Предметом дослідження — методи й алгоритми нормалізації, векторизації та індексації текстів, що дозволяють підвищити якість пошуку, класифікації й аналізу текстової інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.142700", "order": 34, "lang": "uk"}}
{"id": "1_docx#35", "doc_id": "1_docx", "text": "Під час виконання роботи застосовувалися методи аналітичного огляду та синтезу для вивчення літератури й порівняння існуючих підходів; алгоритмічного проєктування — для створення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також статистичні методи оцінювання результатів, що дозволили кількісно виміряти покращення якості обробки текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.145851", "order": 35, "lang": "uk"}}
{"id": "1_docx#36", "doc_id": "1_docx", "text": "Наукова новизна роботи полягає у створенні універсального підходу до обробки неструктурованих даних, який поєднує модулі виділення тексту, нормалізації, семантичного подання та побудови індекса для швидкого пошуку. Запропонований метод дозволяє працювати з різними форматами документів (PDF, DOCX, HTML), об’єднуючи їх у єдину структуру даних без втрати змістової інформації. Крім того, у роботі реалізовано алгоритм побудови семантичного індекса з використанням SentenceTransformers і FAISS, що підвищує ефективність пошуку за змістом, а не лише за словами.\nПрактичне значення роботи полягає в тому, що розроблений програмний комплекс UXText Pipeline може бути використаний як основа для створення аналітичних систем у різних сферах — від бізнес-аналітики та освіти до наукових досліджень і документообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і\nпідвищує точність інформаційного пошуку. Розроблені рішення можуть бути впроваджені в установах, де необхідна швидка обробка текстових звітів, статей або відгуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.151464", "order": 36, "lang": "uk"}}
{"id": "1_docx#37", "doc_id": "1_docx", "text": "гуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» (2024 р.) та при реалізації програмного прототипу, опублікованого у відкритому доступі на GitHub. Під час експериментів підтверджено ефективність запропонованого методу — час обробки документів скоротився, а точність семантичного пошуку зросла порівняно з базовими алгоритмами.\nДипломна робота складається зі вступу, чотирьох розділів, висновків, списку використаних джерел і додатків. Загальний обсяг становить 70 сторінок, включає 25 рисунків, 12 таблиць і 3 додатки. У роботі послідовно викладено теоретичні основи, аналіз існуючих рішень, власну методику обробки текстів, результати експериментів і приклади практичного використання створеного програмного комплексу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.156089", "order": 37, "lang": "uk"}}
{"id": "1_docx#38", "doc_id": "1_docx", "text": "РОЗДІЛ 1.\nАНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.161094", "order": 38, "lang": "uk"}}
{"id": "1_docx#39", "doc_id": "1_docx", "text": "У сучасному цифровому світі обсяг інформації, який щоденно створюється, зростає з небаченою швидкістю. За оцінками аналітичної компанії IDC (International Data Corporation), до 2025 року загальний обсяг глобальних даних перевищить 175 зетабайт, причому понад 80 відсотків із них становитимуть саме неструктуровані дані — тексти, зображення, відео, аудіозаписи, повідомлення в соціальних мережах тощо [1]. Ці дані генеруються користувачами, організаціями, сенсорами та системами штучного інтелекту. Проте, попри їхню кількість і потенційну цінність, лише невелика частина реально використовується для аналітики через відсутність ефективних методів автоматичної обробки.\nПроблема полягає у тому, що більшість традиційних інформаційних систем побудовані для роботи зі структурованими даними — таблицями, числовими показниками, логами, базами SQL. Такі системи ефективно працюють лише тоді, коли дані мають чітко визначену схему зберігання: є поля, типи, формат і зв’язки між ними. Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані з", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.166955", "order": 39, "lang": "uk"}}
{"id": "1_docx#40", "doc_id": "1_docx", "text": "тувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані зображення. Як наслідок, традиційні системи управління даними (DBMS) не здатні безпосередньо інтерпретувати зміст таких файлів, а тому значна частина потенційно корисної інформації залишається поза увагою аналітичних алгоритмів [2].\nЯ, Оліфіренко Кирило, як студент спеціальності «Комп’ютерні науки», у процесі дослідження дійшов висновку, що проблема обробки неструктурованих даних є не лише технічною, а й методологічною. Навіть у сучасних інформаційних середовищах — як у бізнесі, так і в державних установах — зберігаються великі масиви документів у різних форматах: звіти у PDF, службові документи у DOCX, переписки у форматі електронної пошти, повідомлення з месенджерів, а також текстові поля у CRM-системах. Всі ці", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.171391", "order": 40, "lang": "uk"}}
{"id": "1_docx#41", "doc_id": "1_docx", "text": "джерела містять важливу інформацію, яку складно уніфікувати без єдиної системи автоматичної обробки.\nАктуальність цієї проблеми підтверджується низкою об’єктивних чинників:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.173365", "order": 41, "lang": "uk"}}
{"id": "1_docx#42", "doc_id": "1_docx", "text": "Стрімке зростання обсягів текстових даних.\nЩосекунди в інтернеті з’являються мільйони повідомлень — лише у Twitter щодня публікується понад 500 млн твітів, у Facebook — понад 300 млн постів, а також понад 4,5 млрд електронних листів у глобальному масштабі [3]. Така кількість текстів робить ручну обробку неможливою, що потребує створення інтелектуальних алгоритмів аналізу.\nНеоднорідність джерел та форматів.\nТексти можуть бути представлені в різних форматах (PDF, DOCX, HTML, TXT, Markdown), з різною структурою, мовою, стилем і контекстом. У багатьох випадках інформація поєднує текст і візуальні елементи (таблиці, формули, підписи до зображень), що створює додаткові труднощі при обробці.\nЗростання ролі неструктурованих даних у прийнятті рішень.\nУ багатьох сферах — бізнесі, медицині, освіті, науці, державному управлінні — від якісного аналізу текстової інформації залежить точність прогнозів і ефективність управлінських дій.\nУ бізнесі — аналіз відгуків клієнтів дозволяє виявляти рівень задоволеності.\nУ медицині — аналіз медичних карт допомагає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального ", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.180021", "order": 42, "lang": "uk"}}
{"id": "1_docx#43", "doc_id": "1_docx", "text": "агає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального методу обробки.\nІснуючі рішення часто обмежені конкретними форматами або мовами, а результати залежать від якості текстів, наявності помилок, скорочень,\nзмішаних мов. Тому актуальним є завдання створення гнучкої системи, здатної об’єднувати різні етапи обробки — від вилучення тексту до його семантичного аналізу.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.183031", "order": 43, "lang": "uk"}}
{"id": "1_docx#44", "doc_id": "1_docx", "text": "Ключовою метою сучасних досліджень у цій галузі є розробка методів, які поєднують традиційні алгоритми лінгвістичного аналізу з сучасними моделями машинного навчання та глибокого навчання (Deep Learning). Ці методи включають такі етапи, як:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.184907", "order": 44, "lang": "uk"}}
{"id": "1_docx#45", "doc_id": "1_docx", "text": "вилучення тексту з документів різних форматів;\nочищення від шуму (HTML-тегів, непотрібних символів);\nтокенізація й лематизація;\nвизначення мови;\nпобудова векторних подань (векторизація);\nіндексація та семантичний пошук.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.186863", "order": 45, "lang": "uk"}}
{"id": "1_docx#46", "doc_id": "1_docx", "text": "Зокрема, у моїй роботі розглядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семантичного пошуку за змістом). Такий підхід дозволяє створити універсальний пайплайн UXText Pipeline, який автоматизує процес перетворення неструктурованих даних у придатну для аналізу форму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.189538", "order": 46, "lang": "uk"}}
{"id": "1_docx#47", "doc_id": "1_docx", "text": "Разом із тим, залишається низка викликів, які обумовлюють актуальність подальших досліджень:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.190994", "order": 47, "lang": "uk"}}
{"id": "1_docx#48", "doc_id": "1_docx", "text": "неоднорідність форматів і різна якість текстових даних;\nпотреба у великих обчислювальних ресурсах для сучасних моделей (BERT, GPT);\nвідсутність єдиних стандартів інтеграції NLP-моделей у корпоративні системи;\nскладність пояснення результатів глибоких моделей (“чорна скринька”);\nобмежена підтримка української мови у більшості відкритих NLP-бібліотек.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.194337", "order": 48, "lang": "uk"}}
{"id": "1_docx#49", "doc_id": "1_docx", "text": "З огляду на це, можна зробити висновок, що обробка неструктурованих даних є однією з центральних проблем сучасної науки про дані (Data Science). Її ефективне вирішення дозволить перейти від хаотичних текстових потоків до системного аналізу, що підвищить якість рішень у бізнесі, науці й суспільстві.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.196813", "order": 49, "lang": "uk"}}
{"id": "1_docx#50", "doc_id": "1_docx", "text": "Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих текстових даних, який забезпечить:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.199618", "order": 50, "lang": "uk"}}
{"id": "1_docx#51", "doc_id": "1_docx", "text": "універсальність щодо типів і форматів документів;\nавтоматичність та масштабованість;\nточність семантичного пошуку;\nможливість інтеграції з реальними інформаційними системами.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.201423", "order": 51, "lang": "uk"}}
{"id": "1_docx#52", "doc_id": "1_docx", "text": "Саме ця мета визначає подальшу логіку роботи — у наступних розділах буде розроблено архітектуру, реалізацію та апробацію програмного комплексу UXText Pipeline, який поєднує сучасні алгоритми NLP, бібліотеки Python і векторні бази даних для побудови ефективної системи аналізу неструктурованих текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.203739", "order": 52, "lang": "uk"}}
{"id": "1_docx#53", "doc_id": "1_docx", "text": "1.2. Поняття неструктурованих даних, їх роль у сучасних інформаційних системах", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.205307", "order": 53, "lang": "uk"}}
{"id": "1_docx#54", "doc_id": "1_docx", "text": "У сучасних інформаційних системах усі дані, з якими працюють користувачі, аналітики або системи штучного інтелекту, умовно поділяються на три основні типи — структуровані, напівструктуровані та неструктуровані. Така класифікація визначається рівнем організованості інформації, способом її зберігання та можливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управління знаннями.\nСтруктуровані дані — це дані, які зберігаються у чітко визначеній формі та організовані за наперед встановленою схемою. Вони мають фіксовану структуру — таблиці, поля, типи даних, зв’язки між записами. Прикладами таких даних є бази даних SQL, електронні таблиці Excel, реєстри або облікові системи. Основною перевагою структурованих даних є можливість швидкого аналізу за допомогою мов запитів (SQL), статистичних методів чи систем бізнес-аналітики (BI). Проте їхній головний недолік полягає у тому, що вони описують лише формалізовану частину реальності — числа, категорії, дати, тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.212548", "order": 54, "lang": "uk"}}
{"id": "1_docx#55", "doc_id": "1_docx", "text": "тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце між структурованими й неструктурованими. Вони мають певну внутрішню організацію, але не підпорядковуються жорсткій схемі. Такі дані часто представлені у форматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має стандартні поля (“від кого”, “кому”, “дата”), але зміст повідомлення — це вільний текст. Подібні дані активно використовуються у вебтехнологіях, обміні інформацією між сервісами (API) та у NoSQL-базах даних (MongoDB, Cassandra, Firebase). Їхня гнучкість є перевагою, однак аналіз таких даних потребує спеціалізованих методів парсингу, фільтрації й нормалізації [3].\nНеструктуровані дані — це інформація, яка не має фіксованої структури і не може бути представлена у вигляді таблиць або полів. До них належать тексти документів, повідомлення у месенджерах, публікації у соціальних мережах, звіти, аудіо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Осн", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.218042", "order": 55, "lang": "uk"}}
{"id": "1_docx#56", "doc_id": "1_docx", "text": "іо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Основна особливість таких даних полягає в тому, що вони передають значення через зміст, а не через форму. Два документи можуть мати зовсім різну структуру, але описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу\nконтексту, семантики, синтаксису і навіть прихованих зв’язків між словами [4].\nЗа оцінками компанії IBM, близько 80% усієї інформації, створеної людьми, належить до неструктурованих даних [5]. Це означає, що лише невелика частина всієї інформації у світі може бути легко оброблена стандартними базами даних, тоді як решта — тексти, документи, звіти, статті, пости, коментарі — залишаються невикористаними. Саме тому обробка неструктурованих даних стає однією з найважливіших задач сучасної комп’ютерної науки та аналітики.\nУ контексті архітектури інформаційних систем неструктуровані дані зберігаються переважно у файлових системах, хмарних сховищах або об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasti", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.223672", "order": 56, "lang": "uk"}}
{"id": "1_docx#57", "doc_id": "1_docx", "text": "бо об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasticsearch, дозволяють зберігати документи у будь-якому форматі та індексувати їх для пошуку. Проте ключову роль тут відіграють алгоритми попередньої обробки — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [6].\nРоль неструктурованих даних у сучасних ІТ-системах особливо важлива в контексті розвитку штучного інтелекту (AI) та Natural Language Processing (NLP). Саме ці дані є основним джерелом навчання для великих мовних моделей (LLM), таких як GPT, BERT, RoBERTa, LLaMA, T5 тощо. Їх тренування здійснюється на величезних масивах текстів — статтях, наукових працях, форумах, енциклопедіях, книгах [7]. Без таких джерел штучний інтелект не міг би досягти нинішнього рівня розуміння природної мови, контексту, емоційних відтінків і логічних зв’язків.\nЗ точки зору бізнесу, аналіз неструктурованих даних відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцме", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.229982", "order": 57, "lang": "uk"}}
{"id": "1_docx#58", "doc_id": "1_docx", "text": "відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцмережах або результатів опитувань, отримують конкурентну перевагу — вони швидше\nреагують на ринкові зміни, прогнозують поведінку споживачів і вдосконалюють свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині — аналіз клінічних описів допомагає автоматично виявляти симптоми, а в державному секторі — підвищує ефективність управлінських рішень.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.233622", "order": 58, "lang": "uk"}}
{"id": "1_docx#59", "doc_id": "1_docx", "text": "Таблиця 1.1 Порівняльна характеристика структурованих, напівструктурованих та неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.237140", "order": 59, "lang": "uk"}}
{"id": "1_docx#60", "doc_id": "1_docx", "text": "Тип даних Приклади Характеристика Приклади використання Структуровані SQL, Excel, CRM Чітка схема, таблиці, поля Облік, звітність, фінансовий аналіз Напівструктуровані JSON, XML, CSV Часткова організація, відсутність жорсткої схеми Обмін даними між системами, API Неструктуровані Тексти, PDF, аудіо, відео Відсутня структура, потребує інтелектуальної обробки NLP, аналітика, AI, Data Mining", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.239910", "order": 60, "lang": "uk"}}
{"id": "1_docx#61", "doc_id": "1_docx", "text": "Таким чином, неструктуровані дані становлять основну частину сучасного цифрового контенту. Вони є не лише джерелом знань, але й рушієм розвитку інтелектуальних технологій, аналітики та систем прийняття рішень. Їхнє ефективне використання вимагає поєднання методів комп’ютерної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що дозволяють перетворювати неструктуровані тексти у структуровану форму", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.244032", "order": 61, "lang": "uk"}}
{"id": "1_docx#62", "doc_id": "1_docx", "text": "для подальшого аналізу, що є важливим кроком до вдосконалення систем аналізу тексту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.246264", "order": 62, "lang": "uk"}}
{"id": "1_docx#63", "doc_id": "1_docx", "text": "1.3. Методи та підходи до обробки текстових даних у системах штучного інтелекту", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.249049", "order": 63, "lang": "uk"}}
{"id": "1_docx#64", "doc_id": "1_docx", "text": "Обробка текстових даних у системах штучного інтелекту є одним із найскладніших і водночас найдинамічніших напрямів розвитку сучасних інформаційних технологій. Людська мова — це гнучкий і багаторівневий механізм комунікації, який поєднує граматику, контекст, емоції, стиль та приховані смисли. Навчити комп’ютер розуміти текст означає створити модель, здатну не лише розпізнавати слова, а й аналізувати їхні зв’язки, контекст використання та семантичне навантаження [1].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.252647", "order": 64, "lang": "uk"}}
{"id": "1_docx#65", "doc_id": "1_docx", "text": "Залежно від рівня складності та точності, методи обробки текстових даних умовно поділяються на три основні групи:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.254599", "order": 65, "lang": "uk"}}
{"id": "1_docx#66", "doc_id": "1_docx", "text": "статистичні,\nметоди машинного навчання,\nнейромережеві (глибокі).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.256577", "order": 66, "lang": "uk"}}
{"id": "1_docx#67", "doc_id": "1_docx", "text": "Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити закономірності у частоті вживання слів [2]. Найвідомішим став метод Bag of Words (BoW), у якому кожен документ подається як набір слів без урахування порядку. Такий підхід простий у реалізації, але не враховує контекст, тому використовується для класифікації текстів і тематичного аналізу.\nІнший фундаментальний метод — TF-IDF (Term Frequency — Inverse Document Frequency) — оцінює важливість слова у конкретному документі відносно всієї колекції. Наприклад, слово “дані” зустрічається у всіх документах, тому має низьку вагу, тоді як термін “лематизація” зустрічається\nрідше, отже має більшу інформативність. TF-IDF залишається основою у пошукових системах (зокрема Google Search) і є відправною точкою для формування векторних подань текстів [3].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.262295", "order": 67, "lang": "uk"}}
{"id": "1_docx#68", "doc_id": "1_docx", "text": "Методи машинного навчання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.265973", "order": 68, "lang": "uk"}}
{"id": "1_docx#69", "doc_id": "1_docx", "text": "Подальший розвиток комп’ютерних технологій призвів до появи методів машинного навчання (Machine Learning), які дозволили враховувати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йому подати достатньо прикладів [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.268544", "order": 69, "lang": "uk"}}
{"id": "1_docx#70", "doc_id": "1_docx", "text": "Серед найпоширеніших алгоритмів:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.270059", "order": 70, "lang": "uk"}}
{"id": "1_docx#71", "doc_id": "1_docx", "text": "Наївний баєсівський класифікатор (Naive Bayes) — базується на теоремі Байєса і використовується для класифікації документів;\nМетод опорних векторів (SVM) — визначає межі між класами у багатовимірному просторі ознак;\nМетод k-ближчих сусідів (kNN) — класифікує тексти на основі схожості;\nЛогістична регресія — визначає ймовірність належності тексту до певної категорії.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.272591", "order": 71, "lang": "uk"}}
{"id": "1_docx#72", "doc_id": "1_docx", "text": "Такі алгоритми застосовувалися у задачах аналізу тональності тексту (sentiment analysis), тематичної класифікації, виявлення спаму чи автоматичного реферування. Їхнім обмеженням є потреба у ручному створенні ознак (feature engineering) — тобто необхідність перетворювати тексти у числові вектори на основі частотних характеристик (уніграми, біграми, триграми тощо).", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.275723", "order": 72, "lang": "uk"}}
{"id": "1_docx#73", "doc_id": "1_docx", "text": "Нейромережеві методи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.278783", "order": 73, "lang": "uk"}}
{"id": "1_docx#74", "doc_id": "1_docx", "text": "Наступним етапом еволюції стали нейромережеві моделі, які дозволили навчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які зберігають інформацію про значення та контекст.\nПершим проривом стала модель Word2Vec, розроблена в Google Research [5]. Вона навчалася на мільярдах слів і дозволяла розміщувати їх у спільному багатовимірному просторі, де схожі за змістом слова (наприклад, “кіт” і “пес”) знаходяться поруч. Подібні принципи використовують моделі GloVe (Global Vectors for Word Representation) [6] і FastText, розроблений у Meta AI Research, який враховує морфологічну структуру слова.\nГоловний прорив у галузі стався з появою трансформерних архітектур, які суттєво підвищили якість аналізу природної мови. Моделі типу BERT (Bidirectional Encoder Representations from Transformers) [7], GPT (Generative Pre-trained Transformer) [8] і SentenceTransformers використовують механізм Self-Attention (“механізм уваги”), який дозволяє моделі концентруватися на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьом", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.283552", "order": 74, "lang": "uk"}}
{"id": "1_docx#75", "doc_id": "1_docx", "text": "на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем і когнітивних аналітичних інструментів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.285678", "order": 75, "lang": "uk"}}
{"id": "1_docx#76", "doc_id": "1_docx", "text": "Етапи попередньої обробки тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.288233", "order": 76, "lang": "uk"}}
{"id": "1_docx#77", "doc_id": "1_docx", "text": "Перед застосуванням будь-яких алгоритмів штучного інтелекту текст проходить кілька стандартних етапів підготовки [9]:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.289988", "order": 77, "lang": "uk"}}
{"id": "1_docx#78", "doc_id": "1_docx", "text": "Токенізація — розбиття тексту на окремі одиниці (токени): слова, частини слів або символи.\nНормалізація — приведення тексту до єдиного вигляду (зменшення регістру, видалення пунктуації, стоп-слів).\nЛематизація або стемінг — зведення слів до базової форми (наприклад, “працюю”, “працював” → “працювати”).\nPOS-тегування (Part-of-Speech Tagging) — визначення частини мови для кожного слова.\nСинтаксичний аналіз (Parsing) — встановлення граматичних зв’язків між словами у реченні.\nСемантичний аналіз — визначення смислових відношень (“автор написав книгу” → дія, суб’єкт, об’єкт).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.294552", "order": 78, "lang": "uk"}}
{"id": "1_docx#79", "doc_id": "1_docx", "text": "Таблиця 1.2 Еволюція методів обробки текстових даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.296796", "order": 79, "lang": "uk"}}
{"id": "1_docx#80", "doc_id": "1_docx", "text": "Покоління Основні методи Приклади моделей Ключові характеристики Статистичні Bag of Words, TF-IDF – Простота реалізації, не враховує контекст Машинного навчання SVM, Naive Bayes, kNN Scikit-learn Використання вручну створених ознак, середня точність Нейромережеві Word2Vec, FastText, BERT, GPT TensorFlow, PyTorch Висока точність, контекстне розуміння, потреба у великих ресурсах", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.299535", "order": 80, "lang": "uk"}}
{"id": "1_docx#81", "doc_id": "1_docx", "text": "Інтеграція методів у сучасні системи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.300746", "order": 81, "lang": "uk"}}
{"id": "1_docx#82", "doc_id": "1_docx", "text": "Сучасні системи штучного інтелекту об’єднують усі ці методи у єдиний пайплайн обробки тексту — від очищення даних до формування векторного подання та пошуку за змістом [10].\nТиповий приклад:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.302944", "order": 82, "lang": "uk"}}
{"id": "1_docx#83", "doc_id": "1_docx", "text": "вилучення тексту з файлів різних форматів (PDF, DOCX, HTML);\nлематизація й очищення;\nперетворення тексту у вектори (SentenceTransformers);\nпобудова індексу у FAISS;\nздійснення семантичного пошуку.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.304730", "order": 83, "lang": "uk"}}
{"id": "1_docx#84", "doc_id": "1_docx", "text": "Саме цей підхід реалізовано у моєму дипломному проєкті UXText Pipeline, який демонструє практичну інтеграцію моделей NLP, бібліотеки Unstructured та індексаційного механізму FAISS у єдину систему.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.306411", "order": 84, "lang": "uk"}}
{"id": "1_docx#85", "doc_id": "1_docx", "text": "Отже, методи обробки текстових даних пройшли довгий шлях від простих статистичних моделей до глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не лише аналізують тексти, а й здатні взаємодіяти з користувачем природною мовою, пояснювати результати й формувати нові знання на основі отриманої інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.310507", "order": 85, "lang": "uk"}}
{"id": "1_docx#86", "doc_id": "1_docx", "text": "1.4. Алгоритми й моделі обробки природної мови (NLP)", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.312325", "order": 86, "lang": "uk"}}
{"id": "1_docx#87", "doc_id": "1_docx", "text": "Обробка природної мови (Natural Language Processing, скорочено NLP) є однією з ключових галузей штучного інтелекту, що вивчає методи взаємодії комп’ютерів із людською мовою. Основна мета NLP полягає у тому, щоб навчити машини розуміти, інтерпретувати й генерувати тексти так, як це робить людина [1]. Протягом останніх десятиліть у цій сфері було створено велику кількість алгоритмів і моделей, які поступово вдосконалювали здатність систем аналізувати тексти — від простого підрахунку частоти слів до глибокого семантичного розуміння контексту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.315641", "order": 87, "lang": "uk"}}
{"id": "1_docx#88", "doc_id": "1_docx", "text": "Класичні статистичні моделі", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.316919", "order": 88, "lang": "uk"}}
{"id": "1_docx#89", "doc_id": "1_docx", "text": "На початковому етапі розвитку NLP основну роль відігравали статистичні методи, засновані на кількісному аналізі тексту. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього порядку [2]. Наприклад, фрази “кіт спить на дивані” та “на дивані спить кіт” будуть представлені однаково, оскільки містять ті самі слова. Попри простоту реалізації, цей метод ігнорує синтаксичні та семантичні зв’язки, що обмежує його застосування в задачах, де важливий контекст.\nДля підвищення точності аналізу було запропоновано метод TF-IDF (Term Frequency — Inverse Document Frequency) [3], який враховує не лише частоту слова в документі, а й його рідкість у колекції текстів. TF-IDF дозволяє\nнадавати більшої ваги унікальним термінам, які є змістовно значущими для конкретного документа. Цей метод став базовим у побудові пошукових систем, тематичній класифікації та фільтрації спаму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.321278", "order": 89, "lang": "uk"}}
{"id": "1_docx#90", "doc_id": "1_docx", "text": "Векторні моделі представлення текстів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.322869", "order": 90, "lang": "uk"}}
{"id": "1_docx#91", "doc_id": "1_docx", "text": "Подальшим етапом розвитку NLP стали векторні моделі, що дозволили описати слова у вигляді багатовимірних числових векторів. Такі моделі відображають семантичну близькість між словами — чим подібніші значення, тим менша відстань між їхніми векторами [4].\nОднією з перших таких моделей стала Word2Vec, розроблена командою Google Research [5]. Вона дозволяє вловлювати смислові зв’язки між словами: наприклад, векторна операція “король” – “чоловік” + “жінка” ≈ “королева” демонструє, що модель засвоює відношення між поняттями.\nІнша популярна модель — GloVe (Global Vectors for Word Representation), створена в Stanford NLP Group [6], поєднує статистичні принципи з нейронними мережами. Вона формує вектори на основі статистики спільних появ слів у тексті, що підвищує стабільність представлень.\nМодель FastText, розроблена компанією Meta (Facebook) [7], враховує морфологічну структуру слів, розбиваючи їх на підрядки (n-грамні вектори). Завдяки цьому система може розуміти нові або рідкісні слова, що особливо корисно для української мови, де багато словоформ.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.328579", "order": 91, "lang": "uk"}}
{"id": "1_docx#92", "doc_id": "1_docx", "text": "Таблиця 1.3 Порівняння класичних моделей представлення тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.331614", "order": 92, "lang": "uk"}}
{"id": "1_docx#93", "doc_id": "1_docx", "text": "Модель Принцип роботи Переваги Обмеження Bag of Words Підрахунок частоти слів у тексті Простота реалізації, ефективність для малих даних Ігнорує порядок і контекст TF-IDF Врахування частоти і рідкості слів Підвищує точність пошуку, придатний для класифікації Не враховує семантичні зв’язки Word2Vec Навчання векторів слів на великих корпусах Враховує семантику, компактне представлення Не розрізняє контексти одного слова GloVe Аналіз спільних появ слів у тексті Поєднує статистику й нейронний підхід Вимагає великих ресурсів FastText Представлення слова через підрядки Добре працює з новими словами Потребує тонкого налаштування параметрів", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.335428", "order": 93, "lang": "uk"}}
{"id": "1_docx#94", "doc_id": "1_docx", "text": "Контекстні моделі та архітектура трансформерів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.336752", "order": 94, "lang": "uk"}}
{"id": "1_docx#95", "doc_id": "1_docx", "text": "Обмеження класичних моделей полягали в тому, що кожне слово мало одне фіксоване представлення, незалежно від контексту. У реченнях “банк річки” та “кредитний банк” слово “банк” має різні значення, однак старі моделі цього не враховували. Революційним рішенням стало впровадження трансформерних моделей, які враховують контекст кожного слова в межах речення або документа [8].\nМодель BERT (Bidirectional Encoder Representations from Transformers), створена компанією Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT може розуміти, які слова в контексті впливають на значення інших, що забезпечує високу точність у задачах класифікації, пошуку відповідей і машинного перекладу. Модифікації RoBERTa, DistilBERT і ALBERT підвищили швидкість і ефективність обчислень без втрати якості [10].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.341181", "order": 95, "lang": "uk"}}
{"id": "1_docx#96", "doc_id": "1_docx", "text": "Іншу еволюційну гілку представляють генеративні трансформери, серед яких найвідомішою є GPT (Generative Pre-trained Transformer),", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.343307", "order": 96, "lang": "uk"}}
{"id": "1_docx#97", "doc_id": "1_docx", "text": "розроблена OpenAI [11]. GPT здатна не лише аналізувати текст, а й генерувати новий контент — логічно зв’язні речення, відповіді, коди чи описи. Її принцип полягає у передбаченні наступного слова за попереднім контекстом, що робить можливим природне спілкування людини з машиною.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.345950", "order": 97, "lang": "uk"}}
{"id": "1_docx#98", "doc_id": "1_docx", "text": "Сучасні моделі векторизації тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.347104", "order": 98, "lang": "uk"}}
{"id": "1_docx#99", "doc_id": "1_docx", "text": "Для практичних задач класифікації, кластеризації та семантичного пошуку активно застосовується модель SentenceTransformers, яка є модифікацією BERT і оптимізована для створення векторів речень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді.\nТакий принцип лежить в основі семантичного пошуку, де збігаються не слова, а змісти. Наприклад, запит “літак затримався” може знайти документ із фразою “рейс відкладено”, навіть без жодного спільного слова. Саме це робить сучасні пошукові системи, чат-боти та аналітичні інструменти ефективнішими, ніж будь-коли раніше [13].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.350671", "order": 99, "lang": "uk"}}
{"id": "1_docx#100", "doc_id": "1_docx", "text": "Еволюція моделей обробки природної мови пройшла шлях від простих статистичних методів до складних багаторівневих нейромережевих архітектур.\nКласичні моделі, як Bag of Words та TF-IDF, заклали математичну основу для подання текстів, тоді як Word2Vec, GloVe та FastText зробили перші кроки до розуміння семантики.\nНатомість сучасні трансформери (BERT, GPT, SentenceTransformers) дозволили досягти справжнього контекстного розуміння мови, наблизивши штучний інтелект до людського рівня сприйняття.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.354367", "order": 100, "lang": "uk"}}
{"id": "1_docx#101", "doc_id": "1_docx", "text": "У межах цієї дипломної роботи я, Оліфіренко Кирило, досліджую можливості поєднання таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до векторизації та семантичного пошуку.\nРис. 1.1. Схематичне представлення методу обробки неструктурованих текстових даних (UXText Pipeline)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.357146", "order": 101, "lang": "uk"}}
{"id": "1_docx#102", "doc_id": "1_docx", "text": "1.5. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.358975", "order": 102, "lang": "uk"}}
{"id": "1_docx#103", "doc_id": "1_docx", "text": "У розвитку інструментів для обробки природної мови простежується еволюція від класичних бібліотек, орієнтованих на базові лінгвістичні операції, до сучасних фреймворків, що працюють із великими мовними моделями (LLM) та векторними базами даних. Для систематизації екосистеми NLP-технологій доцільно виділити три основні групи:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.361849", "order": 103, "lang": "uk"}}
{"id": "1_docx#104", "doc_id": "1_docx", "text": "традиційні бібліотеки Python для обробки текстів,\nсучасні фреймворки та сервіси глибокого навчання,\nінтеграційні рішення для побудови аналітичних систем.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:47.364212", "order": 104, "lang": "uk"}}
{"id": "1_docx#105", "doc_id": "1_docx", "text": "Ця класифікація дозволяє простежити логічну послідовність розвитку підходів — від токенізації й морфологічного аналізу до комплексного семантичного пошуку та генеративного текстового інтелекту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.366291", "order": 105, "lang": "uk"}}
{"id": "1_docx#106", "doc_id": "1_docx", "text": "1.5.1. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.367679", "order": 106, "lang": "uk"}}
{"id": "1_docx#107", "doc_id": "1_docx", "text": "Першими інструментами, які забезпечили автоматизацію лінгвістичного аналізу, стали бібліотеки NLTK, spaCy та scikit-learn.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.369339", "order": 107, "lang": "uk"}}
{"id": "1_docx#108", "doc_id": "1_docx", "text": "NLTK (Natural Language Toolkit) — одна з найстаріших бібліотек для роботи з природною мовою у Python [1]. Вона містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко використовується в освіті та дослідженнях, оскільки дозволяє швидко створювати експериментальні моделі та аналізувати тексти. Проте через низьку швидкодію і складність масштабування її рідко застосовують у великих проєктах.\nspaCy стала відповіддю на потребу у високопродуктивних NLP-рішеннях. Вона орієнтована на промислове використання та забезпечує обробку текстів у десятки разів швидше, ніж NLTK [2]. SpaCy підтримує понад 60 мов, має модулі для виявлення іменованих сутностей (NER), синтаксичних залежностей і визначення частин мови. Завдяки інтеграції з TensorFlow і PyTorch, spaCy поєднує класичну обробку з глибоким навчанням.\nscikit-learn, хоча й не є суто NLP-бібліотекою, використовується для класифікації текстів, тематичного моделювання та аналізу тональності [3]. Бібліотека надає інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме sci", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.374766", "order": 108, "lang": "uk"}}
{"id": "1_docx#109", "doc_id": "1_docx", "text": "інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.377637", "order": 109, "lang": "uk"}}
{"id": "1_docx#110", "doc_id": "1_docx", "text": "Таблиця 1.4 Порівняння традиційних бібліотек Python для NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.378958", "order": 110, "lang": "uk"}}
{"id": "1_docx#111", "doc_id": "1_docx", "text": "Бібліотека Основні функції Переваги Недоліки та обмеження NLTK Токенізація, лематизація, морфологічний аналіз Простота, освітня цінність Повільна робота, не для продакшну spaCy POS-tagging, NER, синтаксичний аналіз Висока швидкодія, сучасні моделі Обмежена кількість мовних ресурсів scikit-learn Класифікація, TF-IDF, машинне навчання Гнучкість, модульність Не обробляє мову напряму, потребує препроцесингу", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.381651", "order": 111, "lang": "uk"}}
{"id": "1_docx#112", "doc_id": "1_docx", "text": "1.5.1. Сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured)", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.397299", "order": 112, "lang": "ro"}}
{"id": "1_docx#113", "doc_id": "1_docx", "text": "З появою глибоких нейронних мереж і трансформерних архітектур відбувся різкий стрибок у можливостях NLP.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.399849", "order": 113, "lang": "uk"}}
{"id": "1_docx#114", "doc_id": "1_docx", "text": "Фреймворк Transformers від Hugging Face став центральною платформою для роботи з великими мовними моделями [4]. Він надає доступ до сотень попередньо навчених моделей, серед яких BERT, GPT, T5, RoBERTa, DistilBERT, SentenceTransformers тощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних досліджень у NLP.\nІншим ключовим інструментом є OpenAI API, що надає доступ до моделей сімейства GPT (Generative Pre-trained Transformer) [5]. Ці моделі дозволяють не лише аналізувати текст, а й генерувати узагальнення, відповіді,", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.402754", "order": 114, "lang": "uk"}}
{"id": "1_docx#115", "doc_id": "1_docx", "text": "описи чи навіть програмний код, що відкриває новий рівень взаємодії людини та машини.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.404471", "order": 115, "lang": "uk"}}
{"id": "1_docx#116", "doc_id": "1_docx", "text": "Фреймворк LangChain вирішує проблему інтеграції мовних моделей із базами знань, надаючи можливість побудови логічних ланцюгів (chains) між LLM і зовнішніми джерелами [6]. Цей підхід використовується у системах Retrieval-Augmented Generation (RAG), де модель не лише генерує текст, а й отримує релевантну інформацію з баз даних чи документів.\nДля обробки документів різних форматів (PDF, DOCX, HTML, TXT) використовується бібліотека Unstructured.io, яка автоматично виділяє текстові елементи, розпізнає структуру документа і зберігає контекст [7]. Саме вона використовується в моєму проєкті UXText Pipeline для імпорту та нормалізації текстів перед векторизацією.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.407813", "order": 116, "lang": "uk"}}
{"id": "1_docx#117", "doc_id": "1_docx", "text": "Рис. 1.2. Архітектура екосистеми інструментів NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.410184", "order": 117, "lang": "uk"}}
{"id": "1_docx#118", "doc_id": "1_docx", "text": "1.5.3. Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.411632", "order": 118, "lang": "uk"}}
{"id": "1_docx#119", "doc_id": "1_docx", "text": "Завершальним етапом є зберігання, індексація та пошук текстових даних. Для цього застосовуються інтеграційні рішення, які поєднують властивості баз даних та пошукових систем.\nFAISS (Facebook AI Similarity Search) — бібліотека для швидкого пошуку найближчих сусідів у багатовимірних просторах [8]. Вона оптимізована для GPU та дозволяє працювати з мільйонами векторів, що робить її основою систем семантичного пошуку.\nChromaDB — векторна база даних нового покоління, яка інтегрується з мовними моделями через LangChain та інші API [9]. Вона використовується в системах типу RAG для пошуку інформації за змістом. ChromaDB проста у використанні та підтримує локальне й хмарне розгортання.\nElasticSearch — потужна пошукова платформа, яка поєднує індексацію текстів і машинне навчання [10]. Вона підтримує пошук як за ключовими словами, так і за векторними поданнями, завдяки чому часто використовується у корпоративних аналітичних системах.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.415990", "order": 119, "lang": "uk"}}
{"id": "1_docx#120", "doc_id": "1_docx", "text": "Таблиця 1.5 Порівняльна характеристика інтеграційних інструментів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.417343", "order": 120, "lang": "uk"}}
{"id": "1_docx#121", "doc_id": "1_docx", "text": "Інструмент Тип рішення Основні можливості Переваги FAISS Бібліотека векторного пошуку GPU-пошук, кластеризація, масштабованість Висока швидкість, підтримка великих обсягів ChromaDB Векторна база даних Семантичний пошук, інтеграція з LLM Простота, інтеграція з LangChain ElasticSearch Пошукова система Повнотекстовий та семантичний пошук Масштабованість, корпоративний рівень", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.420553", "order": 121, "lang": "uk"}}
{"id": "1_docx#122", "doc_id": "1_docx", "text": "Таким чином, сучасна екосистема обробки текстів складається з трьох взаємодоповнювальних рівнів:\nБазовий рівень — бібліотеки для лінгвістичного аналізу (NLTK, spaCy, scikit-learn).\nАналітичний рівень — фреймворки глибокого навчання (Transformers, LangChain, OpenAI API, Unstructured).\nІнфраструктурний рівень — рішення для зберігання, пошуку та індексації даних (FAISS, ChromaDB, ElasticSearch).\nПоєднання цих технологій створює основу для побудови інтелектуальних систем аналізу неструктурованих даних, здатних здійснювати пошук, класифікацію, узагальнення та інтерпретацію текстів із точністю, наближеною до людського розуміння.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.423887", "order": 122, "lang": "uk"}}
{"id": "1_docx#123", "doc_id": "1_docx", "text": "1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.428521", "order": 123, "lang": "uk"}}
{"id": "1_docx#124", "doc_id": "1_docx", "text": "Проблематика обробки неструктурованих даних залишається однією з найактуальніших тем сучасної комп’ютерної лінгвістики та аналітики даних. За оцінками International Data Corporation (IDC), понад 80 % усієї інформації у світі має неструктурований характер — це тексти, аудіо, відео, зображення, документи різних форматів [1]. Попри суттєвий прогрес у розвитку штучного інтелекту та глибоких нейронних мереж, реальні виклики пов’язані не лише з алгоритмами, а й з їх практичним застосуванням, масштабуванням і пояснюваністю результатів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.431828", "order": 124, "lang": "uk"}}
{"id": "1_docx#125", "doc_id": "1_docx", "text": "Під час розроблення власної дипломної системи UXText Pipeline я, Оліфіренко Кирило, переконався, що найбільші труднощі виникають саме на\nстику технічних, мовних і організаційних аспектів — там, де алгоритми мають взаємодіяти з реальними, часто «брудними» даними користувачів.\nНеоднорідність форматів і структур даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.434306", "order": 125, "lang": "uk"}}
{"id": "1_docx#126", "doc_id": "1_docx", "text": "Однією з найсерйозніших проблем є неоднорідність форматів джерел інформації. Тексти можуть бути представлені у вигляді PDF, DOCX, HTML, TXT, JSON, електронних листів або навіть відсканованих зображень. У кожного з форматів — своя структура, кодування, набір метаданих і правила відображення.\nНаприклад, при вилученні тексту з PDF часто трапляється втрата логічної послідовності слів або ігнорування таблиць, що ускладнює подальшу обробку [2].\nДля уніфікації таких даних застосовуються спеціальні парсери, зокрема бібліотека Unstructured.io, яка розділяє документ на структурні елементи — абзаци, списки, таблиці. Проте навіть вона не гарантує ідеального результату для зображень чи рукописних нотаток.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.437950", "order": 126, "lang": "uk"}}
{"id": "1_docx#127", "doc_id": "1_docx", "text": "Таким чином, перший виклик — створення універсальних конвертерів, здатних коректно вилучати текст незалежно від формату вхідного документа.\nРесурсоємність і масштабування NLP-моделей", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.440557", "order": 127, "lang": "uk"}}
{"id": "1_docx#128", "doc_id": "1_docx", "text": "Сучасні моделі, такі як BERT, GPT чи T5, вимагають надзвичайно великих обчислювальних ресурсів. Для їхнього навчання потрібні потужні графічні процесори (GPU), великі обсяги оперативної пам’яті та тривалі обчислення [3]. Навіть часткове донавчання моделей для конкретного домену може тривати годинами чи днями.\nУ корпоративних і навчальних умовах це створює фінансові та енергетичні обмеження. Саме тому розвивається напрям ефективних моделей\n(efficient AI) — таких, як DistilBERT, TinyLlama чи ALBERT, які споживають менше ресурсів при збереженні високої точності [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.445472", "order": 128, "lang": "uk"}}
{"id": "1_docx#129", "doc_id": "1_docx", "text": "Проблема пояснюваності (Explainability)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.450253", "order": 129, "lang": "uk"}}
{"id": "1_docx#130", "doc_id": "1_docx", "text": "Ще один суттєвий виклик — інтерпретація результатів моделей. Класичні методи (TF-IDF, SVM) дають змогу зрозуміти, які саме ознаки вплинули на результат. Натомість нейронні мережі — це «чорні скриньки», логіка роботи яких не завжди зрозуміла [5].\nУ наукових або юридичних системах це неприйнятно, адже користувач має знати, чому модель зробила певний висновок. Саме тому активно розвивається напрям Explainable AI (XAI), що поєднує точність глибинного навчання з інтерпретованістю статистичних методів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.453655", "order": 130, "lang": "uk"}}
{"id": "1_docx#131", "doc_id": "1_docx", "text": "Прикладом можуть бути візуалізації важливості токенів у реченні чи методи LIME і SHAP, які показують, які слова вплинули на класифікацію.\nМовна неоднорідність і підтримка української мови", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.455381", "order": 131, "lang": "uk"}}
{"id": "1_docx#132", "doc_id": "1_docx", "text": "Суттєвим бар’єром залишається багатомовність і різна якість підтримки мов у бібліотеках. Для англійської мови існують великі корпуси, тоді як для української — вони обмежені. Наприклад, spaCy має часткову підтримку української морфології, але без повноцінного синтаксичного аналізу [6].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.457361", "order": 132, "lang": "uk"}}
{"id": "1_docx#133", "doc_id": "1_docx", "text": "Це ускладнює автоматизацію аналітики в українських організаціях і підкреслює потребу у національних корпусах даних та відкритих моделях, адаптованих до українського контексту.\nІнтеграція NLP із реальними ІТ-системами", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.459748", "order": 133, "lang": "uk"}}
{"id": "1_docx#134", "doc_id": "1_docx", "text": "Нерідко навіть високоточні моделі залишаються ізольованими — їх важко впровадити у реальні бізнес-процеси. CRM-платформи, ERP-системи чи\nдокументообіги використовують різні технологічні стеки (SQL, NoSQL, REST, GraphQL), тому інтеграція NLP-алгоритмів вимагає розробки універсальних API та конекторів [7].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.462523", "order": 134, "lang": "uk"}}
{"id": "1_docx#135", "doc_id": "1_docx", "text": "Це стало передумовою появи таких фреймворків, як LangChain та LlamaIndex, що поєднують мовні моделі з базами даних і зовнішніми джерелами знань.\nЯкість текстових даних і семантична неоднозначність\nЖодна модель не працює добре без якісних вхідних даних.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.464864", "order": 135, "lang": "uk"}}
{"id": "1_docx#136", "doc_id": "1_docx", "text": "Помилки, жаргон, скорочення, неоднозначність (наприклад, слово «банк» — фінансова установа чи берег річки) значно впливають на результати [8]. Для їх усунення використовують етапи очищення, нормалізації, лематизації та векторизації, які дозволяють зменшити шум у даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.467553", "order": 136, "lang": "uk"}}
{"id": "1_docx#137", "doc_id": "1_docx", "text": "Саме на цьому етапі реалізується найбільша частина інтелектуальної роботи у системах обробки неструктурованих текстів.\nБезпека та конфіденційність даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.469322", "order": 137, "lang": "uk"}}
{"id": "1_docx#138", "doc_id": "1_docx", "text": "Оскільки більшість систем NLP працюють з чутливою інформацією, актуальним є питання захисту персональних даних і локальної обробки. Хмарні сервіси (як-от OpenAI API чи Claude AI) не завжди відповідають вимогам корпоративної або державної безпеки.\nТому зростає попит на офлайн-моделі та приватні рішення, що розгортаються у внутрішніх мережах підприємств без передачі даних третім сторонам [9].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.471844", "order": 138, "lang": "uk"}}
{"id": "1_docx#139", "doc_id": "1_docx", "text": "Напрями вдосконалення\nДля подолання наведених проблем сформувалися кілька ключових напрямів розвитку методів обробки неструктурованих даних:\nМультимодальні моделі — поєднання тексту, зображень, аудіо та відео (наприклад, CLIP, Gemini, Kosmos-1), що дозволяє системам аналізувати інформацію комплексно.\nОптимізація ресурсів — створення полегшених архітектур (DistilBERT, TinyLlama, Mistral 7B), які зберігають продуктивність, але працюють на споживацькому обладнанні.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.474698", "order": 139, "lang": "uk"}}
{"id": "1_docx#140", "doc_id": "1_docx", "text": "Пояснюваність моделей (XAI) — розробка інтерпретованих методів аналізу, що дають змогу пояснювати результати користувачам.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.477378", "order": 140, "lang": "uk"}}
{"id": "1_docx#141", "doc_id": "1_docx", "text": "Покращення якості даних — автоматичне виявлення шуму, нормалізація, переклад, фільтрація та сегментація контенту.\nСтандартизація інтеграцій — розробка відкритих API та протоколів взаємодії для універсальної роботи NLP-модулів у різних системах.\nТаблиця 1.6 Основні проблеми та напрями їх вирішення", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.479767", "order": 141, "lang": "uk"}}
{"id": "1_docx#142", "doc_id": "1_docx", "text": "Проблема Причина Можливі напрями вдосконалення Неоднорідність форматів Різні структури документів (PDF, HTML, DOCX) Використання універсальних парсерів (Unstructured.io) Висока ресурсоємність Складні моделі з мільйонами параметрів Використання полегшених архітектур (DistilBERT, TinyLlama) Непояснюваність результатів «Чорна скринька» нейронних мереж Методи XAI, візуалізація уваги, SHAP-аналіз Багатомовність Обмежені корпуси української мови Створення відкритих національних корпусів Складність інтеграції Несумісні технологічні стеки Використання LangChain, REST API, FAISS Якість тексту Наявність шуму, помилок Очищення, нормалізація, векторизація Безпека даних Обробка чутливої інформації Локальні (on-premise) рішення, приватні LLM", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:47.483501", "order": 142, "lang": "uk"}}
{"id": "1_docx#143", "doc_id": "1_docx", "text": "Рис. 1.3. Узагальнена схема проблем і шляхів удосконалення методів обробки неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.486333", "order": 143, "lang": "uk"}}
{"id": "1_docx#144", "doc_id": "1_docx", "text": "1.7. Висновки до розділу 1", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:47.488358", "order": 144, "lang": "uk"}}
{"id": "1_docx#145", "doc_id": "1_docx", "text": "У першому розділі було проведено всебічний теоретичний аналіз проблеми обробки неструктурованих даних, окреслено сучасні тенденції та виклики, а також визначено методологічні основи, на яких базується подальша частина дипломного дослідження. Аналіз показав, що обсяг неструктурованої інформації у світі стрімко зростає, і більшість даних, які сьогодні генеруються людиною чи машинами, мають саме неструктурований характер — це тексти,\nзображення, відео, аудіо, повідомлення у соціальних мережах, електронна пошта, технічна документація тощо. Такі дані не мають чіткої схеми зберігання і потребують спеціальних методів для вилучення, структуризації та аналізу.\nБуло встановлено, що традиційні методи обробки текстової інформації, зокрема статистичні підходи на кшталт Bag of Words або TF-IDF, мають обмежену здатність до розуміння контексту та семантики. Їх перевагою є простота реалізації та зрозумілість результатів, однак у сучасних умовах, коли мова йде про глибоке семантичне розпізнавання, ці підходи стають недостатніми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у ко", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.495081", "order": 145, "lang": "uk"}}
{"id": "1_docx#146", "doc_id": "1_docx", "text": "зі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у контексті, аналізувати зв’язки між фразами та реченнями, а також формувати осмислені векторні представлення тексту. Ці технології створили основу для побудови систем нового покоління — інтелектуальних пошукових механізмів, аналітичних платформ і мовних асистентів.\nУ ході огляду інструментів було виокремлено три рівні екосистеми для роботи з текстовими даними. Перший рівень становлять традиційні бібліотеки Python (NLTK, spaCy, scikit-learn), які забезпечують базову лінгвістичну обробку та підготовку текстів. Другий рівень охоплює сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured), що дозволяють реалізовувати глибокий семантичний аналіз, генерацію текстів і роботу з великими мовними моделями. Третій рівень формують інтеграційні рішення (FAISS, ChromaDB, ElasticSearch), які забезпечують збереження, векторизацію, пошук і зв’язок між компонентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки неструктурованих даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.501161", "order": 146, "lang": "uk"}}
{"id": "1_docx#147", "doc_id": "1_docx", "text": "Разом із тим, проведений аналіз виявив низку ключових проблем, які залишаються невирішеними у сфері NLP та аналітики текстів. Серед них —", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:47.503148", "order": 147, "lang": "uk"}}
{"id": "1_docx#148", "doc_id": "1_docx", "text": "неоднорідність форматів даних, висока ресурсомісткість сучасних моделей, складність інтерпретації результатів, брак універсальних методів для багатомовного аналізу, а також питання інтеграції таких систем у реальні бізнес-процеси. Водночас визначено основні напрями вдосконалення існуючих методів: розвиток мультимодальних моделей, підвищення ефективності обчислень, створення пояснюваних моделей (Explainable AI) та вдосконалення інструментів попередньої обробки тексту.\nПідсумовуючи результати теоретичної частини, можна зробити висновок, що побудова ефективних систем аналізу неструктурованих даних потребує поєднання класичних методів статистичної обробки з сучасними підходами глибокого навчання та трансформерних моделей. Це дозволяє досягнути балансу між точністю, швидкодією та інтерпретованістю результатів.\nСаме тому у подальших розділах дипломної роботи буде розглянуто розробку універсального методу обробки неструктурованих текстових даних, який інтегрує сучасні бібліотеки (Unstructured, Transformers, SentenceTransformers, FAISS) у єдиний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів ін", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.510001", "order": 148, "lang": "uk"}}
{"id": "1_docx#149", "doc_id": "1_docx", "text": "ний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів інформації та створення інструменту, придатного для практичного використання у системах аналітики, досліджень і бізнесу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:47.512221", "order": 149, "lang": "uk"}}
{"id": "1_docx#1", "doc_id": "1_docx", "text": "Державний торговельно-економічний університет\nКафедра комп’ютерних наук та інформаційних систем\nКВАЛІФІКАЦІЙНА РОБОТА\nна тему:\n«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.770238", "order": 1, "lang": "uk"}}
{"id": "1_docx#2", "doc_id": "1_docx", "text": "Студента 2 курсу, 6м групи спеціальності 122 «Комп’ютерні науки» __________ підпис студента Оліфіренко Кирило Андрійович Науковий керівник кандидат фізико-математичних наук, доцент ___________ підпис керівника Томашевська Тетяна Володимирівна Гарант освітньої програми доктор фізико-математичних наук, професор ___________ підпис керівника Пурський Олег Іванович", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:56.772857", "order": 2, "lang": "uk"}}
{"id": "1_docx#3", "doc_id": "1_docx", "text": "Київ 2025\nДержавний торговельно-економічний університет\nФакультет інформаційних технологій\nКафедра комп’ютерних наук та інформаційних систем\nСпеціальність 122 «Комп’ютерні науки»\nОсвітня програма «Комп’ютерні науки»\nЗатверджую\nЗав. кафедри ____________Пурський О.І.\n«» грудня 2025р.\nЗавдання\nна кваліфікаційну роботу студенту\nОліфіренко Кирилу Андрійовичу\n(прізвище, ім’я, по батькові)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.776170", "order": 3, "lang": "uk"}}
{"id": "1_docx#4", "doc_id": "1_docx", "text": "Тема кваліфікаційної роботи", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:56.777867", "order": 4, "lang": "uk"}}
{"id": "1_docx#5", "doc_id": "1_docx", "text": "«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»\nЗатверджена наказом ректора від «» листопада 2025 р. № 4142", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.782904", "order": 5, "lang": "uk"}}
{"id": "1_docx#6", "doc_id": "1_docx", "text": "2. Строк здачі студентом закінченої роботи 15 листопада 2025 року", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.784818", "order": 6, "lang": "uk"}}
{"id": "1_docx#7", "doc_id": "1_docx", "text": "3. Цільова установка та вихідні дані до роботи\nМета роботи: розробка моделі та інформаційної технології оцінки конкурентоспроможності підприємств електронної торгівлі.\nОб’єкт дослідження: автоматизація процесів оцінювання конкурентоспроможності підприємств електронної торгівлі.\nПредмет дослідження: моделі, методи та інформаційні технології оцінювання конкурентоспроможності.\n4. Перелік графічного матеріалу______________________________________ ______________________________________________________________________________________________________________________________________________________________________________________________________\n5. Консультанти по роботі із зазначенням розділів, за якими здійснюється консультування:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.788412", "order": 7, "lang": "uk"}}
{"id": "1_docx#8", "doc_id": "1_docx", "text": "Розділ Консультант (прізвище, ініціали) Підпис, дата Завдання видав Завдання прийняв 1 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 2 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 3 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р.", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:56.792001", "order": 8, "lang": "uk"}}
{"id": "1_docx#9", "doc_id": "1_docx", "text": "6. Зміст кваліфікаційної роботи (перелік питань за кожним розділом)\nВСТУП\nРОЗДІЛ 1. ТЕОРЕТИЧНІ АСПЕКТИ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВА\n1.1. Аналіз проблематики та існуючих методів управління конкурентоспроможністю\n1.2. Особливості оцінки та управління конкурентоспроможністю підприємств електронної комерції\n1.3. Концептуальна модель оцінки та управління конкурентоспроможністю підприємств електронної комерції\nРОЗДІЛ 2. МАТЕМАТИЧНІ МОДЕЛІ ОЦІНКИ ТА УПРАВЛІННЯ КОНКУРЕНТОСПРОМОЖНІСТЮ ПІДПРИЄМСТВА\n2.1. Система показників та модель оцінки конкурентоспроможності підприємств електронної комерції\n2.2. Модель управління конкурентоспроможністю підприємства\n2.3. Моделювання процесу оцінки конкурентоспроможності підприємства\nРОЗДІЛ 3. ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВ ЕЛЕКТРОННОЇ ТОРГІВЛІ\n3.1. Інформаційно-логічна модель системи оцінки конкурентоспроможності підприємств\n3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств\n3.3.Технологія використання інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.798754", "order": 9, "lang": "uk"}}
{"id": "1_docx#10", "doc_id": "1_docx", "text": "інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання роботи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.800744", "order": 10, "lang": "uk"}}
{"id": "1_docx#11", "doc_id": "1_docx", "text": "№ Пор. Назва етапів кваліфікаційної роботи Строк виконання етапів роботи За планом фактично 1 2 3 4 1 Вибір теми кваліфікаційної роботи 01.11.2023 01.11.2023 2 Розробка та затвердження завдання на кваліфікаційну роботу 05.12.2023 05.12.2023 3 Вступ 01.05.2024 01.05.2024 4 РОЗДІЛ 1. Теоретичні аспекти оцінки конкурентоспроможності підприємства 14.06.2024 14.06.2024 5 Підготовка статті у збірник наукових статей магістрів 20.06.2024 20.06.2024 6 РОЗДІЛ 2. Математичні моделі оцінки та управління конкурентоспроможністю підприємства 05.09.2024 05.09.2024 7 РОЗДІЛ 3. Інформаційна технологія оцінки конкурентоспроможності підприємств електронної торгівлі 17.10.2024 17.10.2024 8 Висновки 21.10.2024 21.10.2024 9 Здача кваліфікаційної роботи на кафедру науковому керівнику 23.10.2024 23.10.2024 10 Попередній захист кваліфікаційної роботи 28.10.2024 28.10.2024 11 Виправлення зауважень, зовнішнє рецензування кваліфікаційної роботи 30.10.2024 30.10.2024 12 Представлення готової зшитої кваліфікаційної роботи на кафедру 04.11.2024 04.11.2024 13 Публічний захист кваліфікаційної роботи За розкладом роботи ЕК", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:56.805459", "order": 11, "lang": "uk"}}
{"id": "1_docx#12", "doc_id": "1_docx", "text": "8. Дата видачі завдання «5» грудня 2023 р", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.806956", "order": 12, "lang": "uk"}}
{"id": "1_docx#13", "doc_id": "1_docx", "text": "9. Керівник випускного кваліфікаційної роботи Самойленко Г.Т\n(прізвище, ініціали, підпис)\n10. Гарант освітньої програми Пурський О.І.\n(прізвище, ініціали, підпис)\n11. Завдання прийняв до виконання студент Оліфіренко К. ________\n(прізвище, ініціали, підпис)\n12. Відгук керівника кваліфікаційної роботи\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.811250", "order": 13, "lang": "uk"}}
{"id": "1_docx#14", "doc_id": "1_docx", "text": "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\nКерівник кваліфікаційної роботи\n______________________\n(підпис, дата)\n13. Висновок про кваліфікаційну роботу\nКваліфікаційна робота студента _______ Оліфіренко К.А.\n(прізвище, ініціали)\nможе бути допущена до захисту в екзаменаційній комісії.\nГарант освітньої програми Пурський О.І.\n(підпис, прізвище, ініціали)\nЗавідувач кафедри Пурський О.І.\n(підпис, прізвище, ініціали)\n«_____»_________________2025 р.\nАнотація\nУ кваліфікаційній роботі здійснено комплексну розробку моделей та інформаційної технології моніторингу соціально-економічних показників з", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.815045", "order": 14, "lang": "uk"}}
{"id": "1_docx#15", "doc_id": "1_docx", "text": "метою підвищення ефективності управління регіональним розвитком. Теоретично обґрунтовано основні положення формування і проведення соціально-економічного моніторингу та запропоновано концепцію створення інформаційної системи оціньовання показників соціально-економічного розвитку регіону. Розроблено метод автоматизованого розрахунку комплексної оцінки показників соціально-економічного розвитку. Створено автоматизовану Web-систему оціньовання показників соціально-економічного розвитку регіонів України.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.818565", "order": 15, "lang": "uk"}}
{"id": "1_docx#16", "doc_id": "1_docx", "text": "Ключові слова: соціально-економічний розвиток, математична модель, інтегральні показники, інформаційна технологія.\nAnotation", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.820390", "order": 16, "lang": "uk"}}
{"id": "1_docx#17", "doc_id": "1_docx", "text": "The qualification work is devoted to development of model and information technology of monitoring of social and economic indexes for the purpose of management efficiency increase by regional development The mechanism of the taking into account of differentiation of regional development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of social and economic development is offered and programmed. The Web-system for monitoring indicators of social and economic development of Ukraine regions is created and the technology of its use is developed.\nKeywords: social and economic development, mathematical model, integrated indicators, information technology.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.823946", "order": 17, "lang": "en"}}
{"id": "1_docx#18", "doc_id": "1_docx", "text": "ЗМІСТ\nВСТУП…………………………………………………..…………………………9\nРОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ…………….……..………….12\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних……………………………………………………..……………….….12", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.828775", "order": 18, "lang": "uk"}}
{"id": "1_docx#19", "doc_id": "1_docx", "text": "1.2 Поняття неструктурованих даних, їх роль у сучасних інформаційних системах………………………………………………...………….………..16", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.830565", "order": 19, "lang": "uk"}}
{"id": "1_docx#20", "doc_id": "1_docx", "text": "1.3 Методи та підходи до обробки текстових даних у системах штучного інтелекту ………………………………………………..…………………...20", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.833441", "order": 20, "lang": "uk"}}
{"id": "1_docx#21", "doc_id": "1_docx", "text": "1.4 Алгоритми й моделі обробки природної мови (NLP).……...………...20\n1.5 Огляд інструментів і бібліотек для роботи з текстовими даними………………………………………………...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.835368", "order": 21, "lang": "uk"}}
{"id": "1_docx#22", "doc_id": "1_docx", "text": "1.5.1 Традиційні бібліотеки Python для NLP (NLTK, spaCy, scikit-learn)……………………..…..……………...…………………………….....23", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.837611", "order": 22, "lang": "uk"}}
{"id": "1_docx#23", "doc_id": "1_docx", "text": "1.5.2 Сучасні фреймворки, сервіси (Transformers, OpenAI API, LangChain, Unstructured)……………………..…..…………………….…...23\n1.5.3 Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)….……………………………………...23\n1.6 Проблематика та напрями вдосконалення методів обробки неструктурованих даних………………………...…...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.840018", "order": 23, "lang": "uk"}}
{"id": "1_docx#24", "doc_id": "1_docx", "text": "1.7 Висновки до розділу 1……………………………..…………………...20\nРОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………………………………………………….…..26\n2.1 Постановка задачі, вимоги та функціональні можливості системи…26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.842731", "order": 24, "lang": "uk"}}
{"id": "1_docx#25", "doc_id": "1_docx", "text": "2.2 Архітектура методу обробки неструктурованих текстових даних…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.848020", "order": 25, "lang": "uk"}}
{"id": "1_docx#26", "doc_id": "1_docx", "text": "2.3. Розробка алгоритму розбиття документів на семантичні фрагменти (чанки)………………………………………………………………….……33\n2.4 Методи векторизації тексту та побудова семантичного індексу…...…………………….………………………………….…………26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.850489", "order": 26, "lang": "uk"}}
{"id": "1_docx#27", "doc_id": "1_docx", "text": "2.5 Реалізація бази знань та пошукового механізму ……..…………..…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.851706", "order": 27, "lang": "uk"}}
{"id": "1_docx#28", "doc_id": "1_docx", "text": "2.6 Висновки до розділу 2…………………………...……..…………..…..30\nРОЗДІЛ 3. ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ІНТЕРФЕЙС СИСТЕМИ………………………………………………………….....…………36\n3.1 Загальна структура програмного комплексу UXText Pipeline.………36", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.858864", "order": 28, "lang": "uk"}}
{"id": "1_docx#29", "doc_id": "1_docx", "text": "3.2 Інтеграція з бібліотекою Unstructured.io та FAISS ………….………..38\n3.3 Реалізація API-сервісу (FastAPI) для обробки запитів.………....……42\n3.4 Розробка користувацького інтерфейсу (Streamlit)…….…….………..38", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.860912", "order": 29, "lang": "uk"}}
{"id": "1_docx#30", "doc_id": "1_docx", "text": "3.5 Візуалізація результатів аналізу текстів .………...………...…………42\n3.6 Тестування та оцінювання ефективності роботи системи.……..……42\n3.7 Висновки до розділу 3.………...………………………….....…………42\nВИСНОВКИ……………………………..………………………………………48\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ……………...……………………..50\nДОДАТОК………………………………………………..……………..………52\nВСТУП", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.863262", "order": 30, "lang": "uk"}}
{"id": "1_docx#31", "doc_id": "1_docx", "text": "У сучасному світі, коли інформаційні потоки стрімко зростають, значна частина даних, які створює людина чи система, має неструктурований характер. Це тексти документів, електронні листи, новини, пости у соціальних мережах, коментарі, відгуки, звіти чи наукові статті. За статистичними оцінками, понад 80 відсотків усієї цифрової інформації становлять саме неструктуровані дані, які не мають чіткої схеми чи фіксованої структури зберігання. Проте саме вони містять найціннішу інформацію для аналізу змісту, виявлення тенденцій, автоматизованого прийняття рішень чи побудови систем штучного інтелекту. Проблема полягає в тому, що традиційні інструменти обробки даних не пристосовані для ефективної роботи з такими джерелами. Методи на кшталт статистичного аналізу або пошуку за ключовими словами не здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’язки між ними.\nАктуальність теми зумовлена необхідністю створення ефективних методів, що дозволяють обробляти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штуч", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.869491", "order": 31, "lang": "uk"}}
{"id": "1_docx#32", "doc_id": "1_docx", "text": "яти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штучного інтелекту, зокрема векторизації текстів і нейромережевих моделей природної мови (NLP), відкриває нові можливості для створення інтелектуальних систем аналізу тексту. Ці системи здатні не лише класифікувати чи шукати інформацію, а й знаходити семантичні зв’язки між фрагментами тексту, узагальнювати їх зміст і будувати рекомендації. Саме тому тема «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» є актуальною і відповідає сучасним тенденціям розвитку інформаційних технологій.\nМетою даної роботи є розробка методу та програмного забезпечення, що забезпечують ефективну обробку неструктурованих текстових даних шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завдання: провести аналіз існуючих методів обробки текстових даних і", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.874880", "order": 32, "lang": "uk"}}
{"id": "1_docx#33", "doc_id": "1_docx", "text": "сучасних підходів до представлення тексту; дослідити технології побудови векторних моделей, зокрема SentenceTransformers та FAISS; спроєктувати архітектуру програмного комплексу для інтеграції різних джерел даних; реалізувати систему UXText Pipeline, яка поєднує модулі обробки, зберігання та пошуку текстів; провести експериментальне тестування для оцінки ефективності запропонованого методу.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.878978", "order": 33, "lang": "uk"}}
{"id": "1_docx#34", "doc_id": "1_docx", "text": "Об’єктом дослідження є процес обробки неструктурованих текстових даних у системах аналітики. Предметом дослідження — методи й алгоритми нормалізації, векторизації та індексації текстів, що дозволяють підвищити якість пошуку, класифікації й аналізу текстової інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.881570", "order": 34, "lang": "uk"}}
{"id": "1_docx#35", "doc_id": "1_docx", "text": "Під час виконання роботи застосовувалися методи аналітичного огляду та синтезу для вивчення літератури й порівняння існуючих підходів; алгоритмічного проєктування — для створення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також статистичні методи оцінювання результатів, що дозволили кількісно виміряти покращення якості обробки текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.884442", "order": 35, "lang": "uk"}}
{"id": "1_docx#36", "doc_id": "1_docx", "text": "Наукова новизна роботи полягає у створенні універсального підходу до обробки неструктурованих даних, який поєднує модулі виділення тексту, нормалізації, семантичного подання та побудови індекса для швидкого пошуку. Запропонований метод дозволяє працювати з різними форматами документів (PDF, DOCX, HTML), об’єднуючи їх у єдину структуру даних без втрати змістової інформації. Крім того, у роботі реалізовано алгоритм побудови семантичного індекса з використанням SentenceTransformers і FAISS, що підвищує ефективність пошуку за змістом, а не лише за словами.\nПрактичне значення роботи полягає в тому, що розроблений програмний комплекс UXText Pipeline може бути використаний як основа для створення аналітичних систем у різних сферах — від бізнес-аналітики та освіти до наукових досліджень і документообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і\nпідвищує точність інформаційного пошуку. Розроблені рішення можуть бути впроваджені в установах, де необхідна швидка обробка текстових звітів, статей або відгуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.890257", "order": 36, "lang": "uk"}}
{"id": "1_docx#37", "doc_id": "1_docx", "text": "гуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» (2024 р.) та при реалізації програмного прототипу, опублікованого у відкритому доступі на GitHub. Під час експериментів підтверджено ефективність запропонованого методу — час обробки документів скоротився, а точність семантичного пошуку зросла порівняно з базовими алгоритмами.\nДипломна робота складається зі вступу, чотирьох розділів, висновків, списку використаних джерел і додатків. Загальний обсяг становить 70 сторінок, включає 25 рисунків, 12 таблиць і 3 додатки. У роботі послідовно викладено теоретичні основи, аналіз існуючих рішень, власну методику обробки текстів, результати експериментів і приклади практичного використання створеного програмного комплексу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.896646", "order": 37, "lang": "uk"}}
{"id": "1_docx#38", "doc_id": "1_docx", "text": "РОЗДІЛ 1.\nАНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:56.900985", "order": 38, "lang": "uk"}}
{"id": "1_docx#39", "doc_id": "1_docx", "text": "У сучасному цифровому світі обсяг інформації, який щоденно створюється, зростає з небаченою швидкістю. За оцінками аналітичної компанії IDC (International Data Corporation), до 2025 року загальний обсяг глобальних даних перевищить 175 зетабайт, причому понад 80 відсотків із них становитимуть саме неструктуровані дані — тексти, зображення, відео, аудіозаписи, повідомлення в соціальних мережах тощо [1]. Ці дані генеруються користувачами, організаціями, сенсорами та системами штучного інтелекту. Проте, попри їхню кількість і потенційну цінність, лише невелика частина реально використовується для аналітики через відсутність ефективних методів автоматичної обробки.\nПроблема полягає у тому, що більшість традиційних інформаційних систем побудовані для роботи зі структурованими даними — таблицями, числовими показниками, логами, базами SQL. Такі системи ефективно працюють лише тоді, коли дані мають чітко визначену схему зберігання: є поля, типи, формат і зв’язки між ними. Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані з", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.906463", "order": 39, "lang": "uk"}}
{"id": "1_docx#40", "doc_id": "1_docx", "text": "тувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані зображення. Як наслідок, традиційні системи управління даними (DBMS) не здатні безпосередньо інтерпретувати зміст таких файлів, а тому значна частина потенційно корисної інформації залишається поза увагою аналітичних алгоритмів [2].\nЯ, Оліфіренко Кирило, як студент спеціальності «Комп’ютерні науки», у процесі дослідження дійшов висновку, що проблема обробки неструктурованих даних є не лише технічною, а й методологічною. Навіть у сучасних інформаційних середовищах — як у бізнесі, так і в державних установах — зберігаються великі масиви документів у різних форматах: звіти у PDF, службові документи у DOCX, переписки у форматі електронної пошти, повідомлення з месенджерів, а також текстові поля у CRM-системах. Всі ці", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.912442", "order": 40, "lang": "uk"}}
{"id": "1_docx#41", "doc_id": "1_docx", "text": "джерела містять важливу інформацію, яку складно уніфікувати без єдиної системи автоматичної обробки.\nАктуальність цієї проблеми підтверджується низкою об’єктивних чинників:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.914572", "order": 41, "lang": "uk"}}
{"id": "1_docx#42", "doc_id": "1_docx", "text": "Стрімке зростання обсягів текстових даних.\nЩосекунди в інтернеті з’являються мільйони повідомлень — лише у Twitter щодня публікується понад 500 млн твітів, у Facebook — понад 300 млн постів, а також понад 4,5 млрд електронних листів у глобальному масштабі [3]. Така кількість текстів робить ручну обробку неможливою, що потребує створення інтелектуальних алгоритмів аналізу.\nНеоднорідність джерел та форматів.\nТексти можуть бути представлені в різних форматах (PDF, DOCX, HTML, TXT, Markdown), з різною структурою, мовою, стилем і контекстом. У багатьох випадках інформація поєднує текст і візуальні елементи (таблиці, формули, підписи до зображень), що створює додаткові труднощі при обробці.\nЗростання ролі неструктурованих даних у прийнятті рішень.\nУ багатьох сферах — бізнесі, медицині, освіті, науці, державному управлінні — від якісного аналізу текстової інформації залежить точність прогнозів і ефективність управлінських дій.\nУ бізнесі — аналіз відгуків клієнтів дозволяє виявляти рівень задоволеності.\nУ медицині — аналіз медичних карт допомагає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального ", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:56.920006", "order": 42, "lang": "uk"}}
{"id": "1_docx#43", "doc_id": "1_docx", "text": "агає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального методу обробки.\nІснуючі рішення часто обмежені конкретними форматами або мовами, а результати залежать від якості текстів, наявності помилок, скорочень,\nзмішаних мов. Тому актуальним є завдання створення гнучкої системи, здатної об’єднувати різні етапи обробки — від вилучення тексту до його семантичного аналізу.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:56.922861", "order": 43, "lang": "uk"}}
{"id": "1_docx#44", "doc_id": "1_docx", "text": "Ключовою метою сучасних досліджень у цій галузі є розробка методів, які поєднують традиційні алгоритми лінгвістичного аналізу з сучасними моделями машинного навчання та глибокого навчання (Deep Learning). Ці методи включають такі етапи, як:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.925089", "order": 44, "lang": "uk"}}
{"id": "1_docx#45", "doc_id": "1_docx", "text": "вилучення тексту з документів різних форматів;\nочищення від шуму (HTML-тегів, непотрібних символів);\nтокенізація й лематизація;\nвизначення мови;\nпобудова векторних подань (векторизація);\nіндексація та семантичний пошук.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:56.927977", "order": 45, "lang": "uk"}}
{"id": "1_docx#46", "doc_id": "1_docx", "text": "Зокрема, у моїй роботі розглядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семантичного пошуку за змістом). Такий підхід дозволяє створити універсальний пайплайн UXText Pipeline, який автоматизує процес перетворення неструктурованих даних у придатну для аналізу форму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.930840", "order": 46, "lang": "uk"}}
{"id": "1_docx#47", "doc_id": "1_docx", "text": "Разом із тим, залишається низка викликів, які обумовлюють актуальність подальших досліджень:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.932285", "order": 47, "lang": "uk"}}
{"id": "1_docx#48", "doc_id": "1_docx", "text": "неоднорідність форматів і різна якість текстових даних;\nпотреба у великих обчислювальних ресурсах для сучасних моделей (BERT, GPT);\nвідсутність єдиних стандартів інтеграції NLP-моделей у корпоративні системи;\nскладність пояснення результатів глибоких моделей (“чорна скринька”);\nобмежена підтримка української мови у більшості відкритих NLP-бібліотек.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:56.934673", "order": 48, "lang": "uk"}}
{"id": "1_docx#49", "doc_id": "1_docx", "text": "З огляду на це, можна зробити висновок, що обробка неструктурованих даних є однією з центральних проблем сучасної науки про дані (Data Science). Її ефективне вирішення дозволить перейти від хаотичних текстових потоків до системного аналізу, що підвищить якість рішень у бізнесі, науці й суспільстві.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.936898", "order": 49, "lang": "uk"}}
{"id": "1_docx#50", "doc_id": "1_docx", "text": "Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих текстових даних, який забезпечить:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.939831", "order": 50, "lang": "uk"}}
{"id": "1_docx#51", "doc_id": "1_docx", "text": "універсальність щодо типів і форматів документів;\nавтоматичність та масштабованість;\nточність семантичного пошуку;\nможливість інтеграції з реальними інформаційними системами.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:56.942208", "order": 51, "lang": "uk"}}
{"id": "1_docx#52", "doc_id": "1_docx", "text": "Саме ця мета визначає подальшу логіку роботи — у наступних розділах буде розроблено архітектуру, реалізацію та апробацію програмного комплексу UXText Pipeline, який поєднує сучасні алгоритми NLP, бібліотеки Python і векторні бази даних для побудови ефективної системи аналізу неструктурованих текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.946669", "order": 52, "lang": "uk"}}
{"id": "1_docx#53", "doc_id": "1_docx", "text": "1.2. Поняття неструктурованих даних, їх роль у сучасних інформаційних системах", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:56.948621", "order": 53, "lang": "uk"}}
{"id": "1_docx#54", "doc_id": "1_docx", "text": "У сучасних інформаційних системах усі дані, з якими працюють користувачі, аналітики або системи штучного інтелекту, умовно поділяються на три основні типи — структуровані, напівструктуровані та неструктуровані. Така класифікація визначається рівнем організованості інформації, способом її зберігання та можливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управління знаннями.\nСтруктуровані дані — це дані, які зберігаються у чітко визначеній формі та організовані за наперед встановленою схемою. Вони мають фіксовану структуру — таблиці, поля, типи даних, зв’язки між записами. Прикладами таких даних є бази даних SQL, електронні таблиці Excel, реєстри або облікові системи. Основною перевагою структурованих даних є можливість швидкого аналізу за допомогою мов запитів (SQL), статистичних методів чи систем бізнес-аналітики (BI). Проте їхній головний недолік полягає у тому, що вони описують лише формалізовану частину реальності — числа, категорії, дати, тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.954713", "order": 54, "lang": "uk"}}
{"id": "1_docx#55", "doc_id": "1_docx", "text": "тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце між структурованими й неструктурованими. Вони мають певну внутрішню організацію, але не підпорядковуються жорсткій схемі. Такі дані часто представлені у форматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має стандартні поля (“від кого”, “кому”, “дата”), але зміст повідомлення — це вільний текст. Подібні дані активно використовуються у вебтехнологіях, обміні інформацією між сервісами (API) та у NoSQL-базах даних (MongoDB, Cassandra, Firebase). Їхня гнучкість є перевагою, однак аналіз таких даних потребує спеціалізованих методів парсингу, фільтрації й нормалізації [3].\nНеструктуровані дані — це інформація, яка не має фіксованої структури і не може бути представлена у вигляді таблиць або полів. До них належать тексти документів, повідомлення у месенджерах, публікації у соціальних мережах, звіти, аудіо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Осн", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.961340", "order": 55, "lang": "uk"}}
{"id": "1_docx#56", "doc_id": "1_docx", "text": "іо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Основна особливість таких даних полягає в тому, що вони передають значення через зміст, а не через форму. Два документи можуть мати зовсім різну структуру, але описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу\nконтексту, семантики, синтаксису і навіть прихованих зв’язків між словами [4].\nЗа оцінками компанії IBM, близько 80% усієї інформації, створеної людьми, належить до неструктурованих даних [5]. Це означає, що лише невелика частина всієї інформації у світі може бути легко оброблена стандартними базами даних, тоді як решта — тексти, документи, звіти, статті, пости, коментарі — залишаються невикористаними. Саме тому обробка неструктурованих даних стає однією з найважливіших задач сучасної комп’ютерної науки та аналітики.\nУ контексті архітектури інформаційних систем неструктуровані дані зберігаються переважно у файлових системах, хмарних сховищах або об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasti", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.967134", "order": 56, "lang": "uk"}}
{"id": "1_docx#57", "doc_id": "1_docx", "text": "бо об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasticsearch, дозволяють зберігати документи у будь-якому форматі та індексувати їх для пошуку. Проте ключову роль тут відіграють алгоритми попередньої обробки — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [6].\nРоль неструктурованих даних у сучасних ІТ-системах особливо важлива в контексті розвитку штучного інтелекту (AI) та Natural Language Processing (NLP). Саме ці дані є основним джерелом навчання для великих мовних моделей (LLM), таких як GPT, BERT, RoBERTa, LLaMA, T5 тощо. Їх тренування здійснюється на величезних масивах текстів — статтях, наукових працях, форумах, енциклопедіях, книгах [7]. Без таких джерел штучний інтелект не міг би досягти нинішнього рівня розуміння природної мови, контексту, емоційних відтінків і логічних зв’язків.\nЗ точки зору бізнесу, аналіз неструктурованих даних відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцме", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.972325", "order": 57, "lang": "uk"}}
{"id": "1_docx#58", "doc_id": "1_docx", "text": "відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцмережах або результатів опитувань, отримують конкурентну перевагу — вони швидше\nреагують на ринкові зміни, прогнозують поведінку споживачів і вдосконалюють свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині — аналіз клінічних описів допомагає автоматично виявляти симптоми, а в державному секторі — підвищує ефективність управлінських рішень.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.976549", "order": 58, "lang": "uk"}}
{"id": "1_docx#59", "doc_id": "1_docx", "text": "Таблиця 1.1 Порівняльна характеристика структурованих, напівструктурованих та неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.980697", "order": 59, "lang": "uk"}}
{"id": "1_docx#60", "doc_id": "1_docx", "text": "Тип даних Приклади Характеристика Приклади використання Структуровані SQL, Excel, CRM Чітка схема, таблиці, поля Облік, звітність, фінансовий аналіз Напівструктуровані JSON, XML, CSV Часткова організація, відсутність жорсткої схеми Обмін даними між системами, API Неструктуровані Тексти, PDF, аудіо, відео Відсутня структура, потребує інтелектуальної обробки NLP, аналітика, AI, Data Mining", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:56.983546", "order": 60, "lang": "uk"}}
{"id": "1_docx#61", "doc_id": "1_docx", "text": "Таким чином, неструктуровані дані становлять основну частину сучасного цифрового контенту. Вони є не лише джерелом знань, але й рушієм розвитку інтелектуальних технологій, аналітики та систем прийняття рішень. Їхнє ефективне використання вимагає поєднання методів комп’ютерної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що дозволяють перетворювати неструктуровані тексти у структуровану форму", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.986967", "order": 61, "lang": "uk"}}
{"id": "1_docx#62", "doc_id": "1_docx", "text": "для подальшого аналізу, що є важливим кроком до вдосконалення систем аналізу тексту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.989027", "order": 62, "lang": "uk"}}
{"id": "1_docx#63", "doc_id": "1_docx", "text": "1.3. Методи та підходи до обробки текстових даних у системах штучного інтелекту", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:56.992093", "order": 63, "lang": "uk"}}
{"id": "1_docx#64", "doc_id": "1_docx", "text": "Обробка текстових даних у системах штучного інтелекту є одним із найскладніших і водночас найдинамічніших напрямів розвитку сучасних інформаційних технологій. Людська мова — це гнучкий і багаторівневий механізм комунікації, який поєднує граматику, контекст, емоції, стиль та приховані смисли. Навчити комп’ютер розуміти текст означає створити модель, здатну не лише розпізнавати слова, а й аналізувати їхні зв’язки, контекст використання та семантичне навантаження [1].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:56.996320", "order": 64, "lang": "uk"}}
{"id": "1_docx#65", "doc_id": "1_docx", "text": "Залежно від рівня складності та точності, методи обробки текстових даних умовно поділяються на три основні групи:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:56.998379", "order": 65, "lang": "uk"}}
{"id": "1_docx#66", "doc_id": "1_docx", "text": "статистичні,\nметоди машинного навчання,\nнейромережеві (глибокі).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:57.000408", "order": 66, "lang": "uk"}}
{"id": "1_docx#67", "doc_id": "1_docx", "text": "Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити закономірності у частоті вживання слів [2]. Найвідомішим став метод Bag of Words (BoW), у якому кожен документ подається як набір слів без урахування порядку. Такий підхід простий у реалізації, але не враховує контекст, тому використовується для класифікації текстів і тематичного аналізу.\nІнший фундаментальний метод — TF-IDF (Term Frequency — Inverse Document Frequency) — оцінює важливість слова у конкретному документі відносно всієї колекції. Наприклад, слово “дані” зустрічається у всіх документах, тому має низьку вагу, тоді як термін “лематизація” зустрічається\nрідше, отже має більшу інформативність. TF-IDF залишається основою у пошукових системах (зокрема Google Search) і є відправною точкою для формування векторних подань текстів [3].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.004621", "order": 67, "lang": "uk"}}
{"id": "1_docx#68", "doc_id": "1_docx", "text": "Методи машинного навчання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.008503", "order": 68, "lang": "uk"}}
{"id": "1_docx#69", "doc_id": "1_docx", "text": "Подальший розвиток комп’ютерних технологій призвів до появи методів машинного навчання (Machine Learning), які дозволили враховувати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йому подати достатньо прикладів [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.011767", "order": 69, "lang": "uk"}}
{"id": "1_docx#70", "doc_id": "1_docx", "text": "Серед найпоширеніших алгоритмів:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.013337", "order": 70, "lang": "uk"}}
{"id": "1_docx#71", "doc_id": "1_docx", "text": "Наївний баєсівський класифікатор (Naive Bayes) — базується на теоремі Байєса і використовується для класифікації документів;\nМетод опорних векторів (SVM) — визначає межі між класами у багатовимірному просторі ознак;\nМетод k-ближчих сусідів (kNN) — класифікує тексти на основі схожості;\nЛогістична регресія — визначає ймовірність належності тексту до певної категорії.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:57.015882", "order": 71, "lang": "uk"}}
{"id": "1_docx#72", "doc_id": "1_docx", "text": "Такі алгоритми застосовувалися у задачах аналізу тональності тексту (sentiment analysis), тематичної класифікації, виявлення спаму чи автоматичного реферування. Їхнім обмеженням є потреба у ручному створенні ознак (feature engineering) — тобто необхідність перетворювати тексти у числові вектори на основі частотних характеристик (уніграми, біграми, триграми тощо).", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.018481", "order": 72, "lang": "uk"}}
{"id": "1_docx#73", "doc_id": "1_docx", "text": "Нейромережеві методи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.020954", "order": 73, "lang": "uk"}}
{"id": "1_docx#74", "doc_id": "1_docx", "text": "Наступним етапом еволюції стали нейромережеві моделі, які дозволили навчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які зберігають інформацію про значення та контекст.\nПершим проривом стала модель Word2Vec, розроблена в Google Research [5]. Вона навчалася на мільярдах слів і дозволяла розміщувати їх у спільному багатовимірному просторі, де схожі за змістом слова (наприклад, “кіт” і “пес”) знаходяться поруч. Подібні принципи використовують моделі GloVe (Global Vectors for Word Representation) [6] і FastText, розроблений у Meta AI Research, який враховує морфологічну структуру слова.\nГоловний прорив у галузі стався з появою трансформерних архітектур, які суттєво підвищили якість аналізу природної мови. Моделі типу BERT (Bidirectional Encoder Representations from Transformers) [7], GPT (Generative Pre-trained Transformer) [8] і SentenceTransformers використовують механізм Self-Attention (“механізм уваги”), який дозволяє моделі концентруватися на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьом", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.026432", "order": 74, "lang": "uk"}}
{"id": "1_docx#75", "doc_id": "1_docx", "text": "на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем і когнітивних аналітичних інструментів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.028842", "order": 75, "lang": "uk"}}
{"id": "1_docx#76", "doc_id": "1_docx", "text": "Етапи попередньої обробки тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.031501", "order": 76, "lang": "uk"}}
{"id": "1_docx#77", "doc_id": "1_docx", "text": "Перед застосуванням будь-яких алгоритмів штучного інтелекту текст проходить кілька стандартних етапів підготовки [9]:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.033345", "order": 77, "lang": "uk"}}
{"id": "1_docx#78", "doc_id": "1_docx", "text": "Токенізація — розбиття тексту на окремі одиниці (токени): слова, частини слів або символи.\nНормалізація — приведення тексту до єдиного вигляду (зменшення регістру, видалення пунктуації, стоп-слів).\nЛематизація або стемінг — зведення слів до базової форми (наприклад, “працюю”, “працював” → “працювати”).\nPOS-тегування (Part-of-Speech Tagging) — визначення частини мови для кожного слова.\nСинтаксичний аналіз (Parsing) — встановлення граматичних зв’язків між словами у реченні.\nСемантичний аналіз — визначення смислових відношень (“автор написав книгу” → дія, суб’єкт, об’єкт).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:57.036893", "order": 78, "lang": "uk"}}
{"id": "1_docx#79", "doc_id": "1_docx", "text": "Таблиця 1.2 Еволюція методів обробки текстових даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.039006", "order": 79, "lang": "uk"}}
{"id": "1_docx#80", "doc_id": "1_docx", "text": "Покоління Основні методи Приклади моделей Ключові характеристики Статистичні Bag of Words, TF-IDF – Простота реалізації, не враховує контекст Машинного навчання SVM, Naive Bayes, kNN Scikit-learn Використання вручну створених ознак, середня точність Нейромережеві Word2Vec, FastText, BERT, GPT TensorFlow, PyTorch Висока точність, контекстне розуміння, потреба у великих ресурсах", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:57.043415", "order": 80, "lang": "uk"}}
{"id": "1_docx#81", "doc_id": "1_docx", "text": "Інтеграція методів у сучасні системи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.045036", "order": 81, "lang": "uk"}}
{"id": "1_docx#82", "doc_id": "1_docx", "text": "Сучасні системи штучного інтелекту об’єднують усі ці методи у єдиний пайплайн обробки тексту — від очищення даних до формування векторного подання та пошуку за змістом [10].\nТиповий приклад:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.047329", "order": 82, "lang": "uk"}}
{"id": "1_docx#83", "doc_id": "1_docx", "text": "вилучення тексту з файлів різних форматів (PDF, DOCX, HTML);\nлематизація й очищення;\nперетворення тексту у вектори (SentenceTransformers);\nпобудова індексу у FAISS;\nздійснення семантичного пошуку.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:57.049144", "order": 83, "lang": "uk"}}
{"id": "1_docx#84", "doc_id": "1_docx", "text": "Саме цей підхід реалізовано у моєму дипломному проєкті UXText Pipeline, який демонструє практичну інтеграцію моделей NLP, бібліотеки Unstructured та індексаційного механізму FAISS у єдину систему.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.050846", "order": 84, "lang": "uk"}}
{"id": "1_docx#85", "doc_id": "1_docx", "text": "Отже, методи обробки текстових даних пройшли довгий шлях від простих статистичних моделей до глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не лише аналізують тексти, а й здатні взаємодіяти з користувачем природною мовою, пояснювати результати й формувати нові знання на основі отриманої інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.054168", "order": 85, "lang": "uk"}}
{"id": "1_docx#86", "doc_id": "1_docx", "text": "1.4. Алгоритми й моделі обробки природної мови (NLP)", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.055796", "order": 86, "lang": "uk"}}
{"id": "1_docx#87", "doc_id": "1_docx", "text": "Обробка природної мови (Natural Language Processing, скорочено NLP) є однією з ключових галузей штучного інтелекту, що вивчає методи взаємодії комп’ютерів із людською мовою. Основна мета NLP полягає у тому, щоб навчити машини розуміти, інтерпретувати й генерувати тексти так, як це робить людина [1]. Протягом останніх десятиліть у цій сфері було створено велику кількість алгоритмів і моделей, які поступово вдосконалювали здатність систем аналізувати тексти — від простого підрахунку частоти слів до глибокого семантичного розуміння контексту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.059931", "order": 87, "lang": "uk"}}
{"id": "1_docx#88", "doc_id": "1_docx", "text": "Класичні статистичні моделі", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.061372", "order": 88, "lang": "uk"}}
{"id": "1_docx#89", "doc_id": "1_docx", "text": "На початковому етапі розвитку NLP основну роль відігравали статистичні методи, засновані на кількісному аналізі тексту. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього порядку [2]. Наприклад, фрази “кіт спить на дивані” та “на дивані спить кіт” будуть представлені однаково, оскільки містять ті самі слова. Попри простоту реалізації, цей метод ігнорує синтаксичні та семантичні зв’язки, що обмежує його застосування в задачах, де важливий контекст.\nДля підвищення точності аналізу було запропоновано метод TF-IDF (Term Frequency — Inverse Document Frequency) [3], який враховує не лише частоту слова в документі, а й його рідкість у колекції текстів. TF-IDF дозволяє\nнадавати більшої ваги унікальним термінам, які є змістовно значущими для конкретного документа. Цей метод став базовим у побудові пошукових систем, тематичній класифікації та фільтрації спаму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.066069", "order": 89, "lang": "uk"}}
{"id": "1_docx#90", "doc_id": "1_docx", "text": "Векторні моделі представлення текстів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.067737", "order": 90, "lang": "uk"}}
{"id": "1_docx#91", "doc_id": "1_docx", "text": "Подальшим етапом розвитку NLP стали векторні моделі, що дозволили описати слова у вигляді багатовимірних числових векторів. Такі моделі відображають семантичну близькість між словами — чим подібніші значення, тим менша відстань між їхніми векторами [4].\nОднією з перших таких моделей стала Word2Vec, розроблена командою Google Research [5]. Вона дозволяє вловлювати смислові зв’язки між словами: наприклад, векторна операція “король” – “чоловік” + “жінка” ≈ “королева” демонструє, що модель засвоює відношення між поняттями.\nІнша популярна модель — GloVe (Global Vectors for Word Representation), створена в Stanford NLP Group [6], поєднує статистичні принципи з нейронними мережами. Вона формує вектори на основі статистики спільних появ слів у тексті, що підвищує стабільність представлень.\nМодель FastText, розроблена компанією Meta (Facebook) [7], враховує морфологічну структуру слів, розбиваючи їх на підрядки (n-грамні вектори). Завдяки цьому система може розуміти нові або рідкісні слова, що особливо корисно для української мови, де багато словоформ.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.072563", "order": 91, "lang": "uk"}}
{"id": "1_docx#92", "doc_id": "1_docx", "text": "Таблиця 1.3 Порівняння класичних моделей представлення тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.076151", "order": 92, "lang": "uk"}}
{"id": "1_docx#93", "doc_id": "1_docx", "text": "Модель Принцип роботи Переваги Обмеження Bag of Words Підрахунок частоти слів у тексті Простота реалізації, ефективність для малих даних Ігнорує порядок і контекст TF-IDF Врахування частоти і рідкості слів Підвищує точність пошуку, придатний для класифікації Не враховує семантичні зв’язки Word2Vec Навчання векторів слів на великих корпусах Враховує семантику, компактне представлення Не розрізняє контексти одного слова GloVe Аналіз спільних появ слів у тексті Поєднує статистику й нейронний підхід Вимагає великих ресурсів FastText Представлення слова через підрядки Добре працює з новими словами Потребує тонкого налаштування параметрів", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:57.081142", "order": 93, "lang": "uk"}}
{"id": "1_docx#94", "doc_id": "1_docx", "text": "Контекстні моделі та архітектура трансформерів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.082607", "order": 94, "lang": "uk"}}
{"id": "1_docx#95", "doc_id": "1_docx", "text": "Обмеження класичних моделей полягали в тому, що кожне слово мало одне фіксоване представлення, незалежно від контексту. У реченнях “банк річки” та “кредитний банк” слово “банк” має різні значення, однак старі моделі цього не враховували. Революційним рішенням стало впровадження трансформерних моделей, які враховують контекст кожного слова в межах речення або документа [8].\nМодель BERT (Bidirectional Encoder Representations from Transformers), створена компанією Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT може розуміти, які слова в контексті впливають на значення інших, що забезпечує високу точність у задачах класифікації, пошуку відповідей і машинного перекладу. Модифікації RoBERTa, DistilBERT і ALBERT підвищили швидкість і ефективність обчислень без втрати якості [10].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.087130", "order": 95, "lang": "uk"}}
{"id": "1_docx#96", "doc_id": "1_docx", "text": "Іншу еволюційну гілку представляють генеративні трансформери, серед яких найвідомішою є GPT (Generative Pre-trained Transformer),", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.088505", "order": 96, "lang": "uk"}}
{"id": "1_docx#97", "doc_id": "1_docx", "text": "розроблена OpenAI [11]. GPT здатна не лише аналізувати текст, а й генерувати новий контент — логічно зв’язні речення, відповіді, коди чи описи. Її принцип полягає у передбаченні наступного слова за попереднім контекстом, що робить можливим природне спілкування людини з машиною.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.090879", "order": 97, "lang": "uk"}}
{"id": "1_docx#98", "doc_id": "1_docx", "text": "Сучасні моделі векторизації тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.092862", "order": 98, "lang": "uk"}}
{"id": "1_docx#99", "doc_id": "1_docx", "text": "Для практичних задач класифікації, кластеризації та семантичного пошуку активно застосовується модель SentenceTransformers, яка є модифікацією BERT і оптимізована для створення векторів речень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді.\nТакий принцип лежить в основі семантичного пошуку, де збігаються не слова, а змісти. Наприклад, запит “літак затримався” може знайти документ із фразою “рейс відкладено”, навіть без жодного спільного слова. Саме це робить сучасні пошукові системи, чат-боти та аналітичні інструменти ефективнішими, ніж будь-коли раніше [13].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.096873", "order": 99, "lang": "uk"}}
{"id": "1_docx#100", "doc_id": "1_docx", "text": "Еволюція моделей обробки природної мови пройшла шлях від простих статистичних методів до складних багаторівневих нейромережевих архітектур.\nКласичні моделі, як Bag of Words та TF-IDF, заклали математичну основу для подання текстів, тоді як Word2Vec, GloVe та FastText зробили перші кроки до розуміння семантики.\nНатомість сучасні трансформери (BERT, GPT, SentenceTransformers) дозволили досягти справжнього контекстного розуміння мови, наблизивши штучний інтелект до людського рівня сприйняття.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.100635", "order": 100, "lang": "uk"}}
{"id": "1_docx#101", "doc_id": "1_docx", "text": "У межах цієї дипломної роботи я, Оліфіренко Кирило, досліджую можливості поєднання таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до векторизації та семантичного пошуку.\nРис. 1.1. Схематичне представлення методу обробки неструктурованих текстових даних (UXText Pipeline)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.103488", "order": 101, "lang": "uk"}}
{"id": "1_docx#102", "doc_id": "1_docx", "text": "1.5. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.104884", "order": 102, "lang": "uk"}}
{"id": "1_docx#103", "doc_id": "1_docx", "text": "У розвитку інструментів для обробки природної мови простежується еволюція від класичних бібліотек, орієнтованих на базові лінгвістичні операції, до сучасних фреймворків, що працюють із великими мовними моделями (LLM) та векторними базами даних. Для систематизації екосистеми NLP-технологій доцільно виділити три основні групи:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.107257", "order": 103, "lang": "uk"}}
{"id": "1_docx#104", "doc_id": "1_docx", "text": "традиційні бібліотеки Python для обробки текстів,\nсучасні фреймворки та сервіси глибокого навчання,\nінтеграційні рішення для побудови аналітичних систем.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:47:57.110343", "order": 104, "lang": "uk"}}
{"id": "1_docx#105", "doc_id": "1_docx", "text": "Ця класифікація дозволяє простежити логічну послідовність розвитку підходів — від токенізації й морфологічного аналізу до комплексного семантичного пошуку та генеративного текстового інтелекту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.112844", "order": 105, "lang": "uk"}}
{"id": "1_docx#106", "doc_id": "1_docx", "text": "1.5.1. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.114396", "order": 106, "lang": "uk"}}
{"id": "1_docx#107", "doc_id": "1_docx", "text": "Першими інструментами, які забезпечили автоматизацію лінгвістичного аналізу, стали бібліотеки NLTK, spaCy та scikit-learn.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.116099", "order": 107, "lang": "uk"}}
{"id": "1_docx#108", "doc_id": "1_docx", "text": "NLTK (Natural Language Toolkit) — одна з найстаріших бібліотек для роботи з природною мовою у Python [1]. Вона містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко використовується в освіті та дослідженнях, оскільки дозволяє швидко створювати експериментальні моделі та аналізувати тексти. Проте через низьку швидкодію і складність масштабування її рідко застосовують у великих проєктах.\nspaCy стала відповіддю на потребу у високопродуктивних NLP-рішеннях. Вона орієнтована на промислове використання та забезпечує обробку текстів у десятки разів швидше, ніж NLTK [2]. SpaCy підтримує понад 60 мов, має модулі для виявлення іменованих сутностей (NER), синтаксичних залежностей і визначення частин мови. Завдяки інтеграції з TensorFlow і PyTorch, spaCy поєднує класичну обробку з глибоким навчанням.\nscikit-learn, хоча й не є суто NLP-бібліотекою, використовується для класифікації текстів, тематичного моделювання та аналізу тональності [3]. Бібліотека надає інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме sci", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.121351", "order": 108, "lang": "uk"}}
{"id": "1_docx#109", "doc_id": "1_docx", "text": "інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.123424", "order": 109, "lang": "uk"}}
{"id": "1_docx#110", "doc_id": "1_docx", "text": "Таблиця 1.4 Порівняння традиційних бібліотек Python для NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.125063", "order": 110, "lang": "uk"}}
{"id": "1_docx#111", "doc_id": "1_docx", "text": "Бібліотека Основні функції Переваги Недоліки та обмеження NLTK Токенізація, лематизація, морфологічний аналіз Простота, освітня цінність Повільна робота, не для продакшну spaCy POS-tagging, NER, синтаксичний аналіз Висока швидкодія, сучасні моделі Обмежена кількість мовних ресурсів scikit-learn Класифікація, TF-IDF, машинне навчання Гнучкість, модульність Не обробляє мову напряму, потребує препроцесингу", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:57.128667", "order": 111, "lang": "uk"}}
{"id": "1_docx#112", "doc_id": "1_docx", "text": "1.5.1. Сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured)", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.145942", "order": 112, "lang": "ro"}}
{"id": "1_docx#113", "doc_id": "1_docx", "text": "З появою глибоких нейронних мереж і трансформерних архітектур відбувся різкий стрибок у можливостях NLP.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.148613", "order": 113, "lang": "uk"}}
{"id": "1_docx#114", "doc_id": "1_docx", "text": "Фреймворк Transformers від Hugging Face став центральною платформою для роботи з великими мовними моделями [4]. Він надає доступ до сотень попередньо навчених моделей, серед яких BERT, GPT, T5, RoBERTa, DistilBERT, SentenceTransformers тощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних досліджень у NLP.\nІншим ключовим інструментом є OpenAI API, що надає доступ до моделей сімейства GPT (Generative Pre-trained Transformer) [5]. Ці моделі дозволяють не лише аналізувати текст, а й генерувати узагальнення, відповіді,", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.151655", "order": 114, "lang": "uk"}}
{"id": "1_docx#115", "doc_id": "1_docx", "text": "описи чи навіть програмний код, що відкриває новий рівень взаємодії людини та машини.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.153437", "order": 115, "lang": "uk"}}
{"id": "1_docx#116", "doc_id": "1_docx", "text": "Фреймворк LangChain вирішує проблему інтеграції мовних моделей із базами знань, надаючи можливість побудови логічних ланцюгів (chains) між LLM і зовнішніми джерелами [6]. Цей підхід використовується у системах Retrieval-Augmented Generation (RAG), де модель не лише генерує текст, а й отримує релевантну інформацію з баз даних чи документів.\nДля обробки документів різних форматів (PDF, DOCX, HTML, TXT) використовується бібліотека Unstructured.io, яка автоматично виділяє текстові елементи, розпізнає структуру документа і зберігає контекст [7]. Саме вона використовується в моєму проєкті UXText Pipeline для імпорту та нормалізації текстів перед векторизацією.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.156912", "order": 116, "lang": "uk"}}
{"id": "1_docx#117", "doc_id": "1_docx", "text": "Рис. 1.2. Архітектура екосистеми інструментів NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.158895", "order": 117, "lang": "uk"}}
{"id": "1_docx#118", "doc_id": "1_docx", "text": "1.5.3. Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.160564", "order": 118, "lang": "uk"}}
{"id": "1_docx#119", "doc_id": "1_docx", "text": "Завершальним етапом є зберігання, індексація та пошук текстових даних. Для цього застосовуються інтеграційні рішення, які поєднують властивості баз даних та пошукових систем.\nFAISS (Facebook AI Similarity Search) — бібліотека для швидкого пошуку найближчих сусідів у багатовимірних просторах [8]. Вона оптимізована для GPU та дозволяє працювати з мільйонами векторів, що робить її основою систем семантичного пошуку.\nChromaDB — векторна база даних нового покоління, яка інтегрується з мовними моделями через LangChain та інші API [9]. Вона використовується в системах типу RAG для пошуку інформації за змістом. ChromaDB проста у використанні та підтримує локальне й хмарне розгортання.\nElasticSearch — потужна пошукова платформа, яка поєднує індексацію текстів і машинне навчання [10]. Вона підтримує пошук як за ключовими словами, так і за векторними поданнями, завдяки чому часто використовується у корпоративних аналітичних системах.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.164910", "order": 119, "lang": "uk"}}
{"id": "1_docx#120", "doc_id": "1_docx", "text": "Таблиця 1.5 Порівняльна характеристика інтеграційних інструментів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.166232", "order": 120, "lang": "uk"}}
{"id": "1_docx#121", "doc_id": "1_docx", "text": "Інструмент Тип рішення Основні можливості Переваги FAISS Бібліотека векторного пошуку GPU-пошук, кластеризація, масштабованість Висока швидкість, підтримка великих обсягів ChromaDB Векторна база даних Семантичний пошук, інтеграція з LLM Простота, інтеграція з LangChain ElasticSearch Пошукова система Повнотекстовий та семантичний пошук Масштабованість, корпоративний рівень", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:57.169393", "order": 121, "lang": "uk"}}
{"id": "1_docx#122", "doc_id": "1_docx", "text": "Таким чином, сучасна екосистема обробки текстів складається з трьох взаємодоповнювальних рівнів:\nБазовий рівень — бібліотеки для лінгвістичного аналізу (NLTK, spaCy, scikit-learn).\nАналітичний рівень — фреймворки глибокого навчання (Transformers, LangChain, OpenAI API, Unstructured).\nІнфраструктурний рівень — рішення для зберігання, пошуку та індексації даних (FAISS, ChromaDB, ElasticSearch).\nПоєднання цих технологій створює основу для побудови інтелектуальних систем аналізу неструктурованих даних, здатних здійснювати пошук, класифікацію, узагальнення та інтерпретацію текстів із точністю, наближеною до людського розуміння.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.172667", "order": 122, "lang": "uk"}}
{"id": "1_docx#123", "doc_id": "1_docx", "text": "1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.177708", "order": 123, "lang": "uk"}}
{"id": "1_docx#124", "doc_id": "1_docx", "text": "Проблематика обробки неструктурованих даних залишається однією з найактуальніших тем сучасної комп’ютерної лінгвістики та аналітики даних. За оцінками International Data Corporation (IDC), понад 80 % усієї інформації у світі має неструктурований характер — це тексти, аудіо, відео, зображення, документи різних форматів [1]. Попри суттєвий прогрес у розвитку штучного інтелекту та глибоких нейронних мереж, реальні виклики пов’язані не лише з алгоритмами, а й з їх практичним застосуванням, масштабуванням і пояснюваністю результатів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.180901", "order": 124, "lang": "uk"}}
{"id": "1_docx#125", "doc_id": "1_docx", "text": "Під час розроблення власної дипломної системи UXText Pipeline я, Оліфіренко Кирило, переконався, що найбільші труднощі виникають саме на\nстику технічних, мовних і організаційних аспектів — там, де алгоритми мають взаємодіяти з реальними, часто «брудними» даними користувачів.\nНеоднорідність форматів і структур даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.183520", "order": 125, "lang": "uk"}}
{"id": "1_docx#126", "doc_id": "1_docx", "text": "Однією з найсерйозніших проблем є неоднорідність форматів джерел інформації. Тексти можуть бути представлені у вигляді PDF, DOCX, HTML, TXT, JSON, електронних листів або навіть відсканованих зображень. У кожного з форматів — своя структура, кодування, набір метаданих і правила відображення.\nНаприклад, при вилученні тексту з PDF часто трапляється втрата логічної послідовності слів або ігнорування таблиць, що ускладнює подальшу обробку [2].\nДля уніфікації таких даних застосовуються спеціальні парсери, зокрема бібліотека Unstructured.io, яка розділяє документ на структурні елементи — абзаци, списки, таблиці. Проте навіть вона не гарантує ідеального результату для зображень чи рукописних нотаток.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.187266", "order": 126, "lang": "uk"}}
{"id": "1_docx#127", "doc_id": "1_docx", "text": "Таким чином, перший виклик — створення універсальних конвертерів, здатних коректно вилучати текст незалежно від формату вхідного документа.\nРесурсоємність і масштабування NLP-моделей", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.189728", "order": 127, "lang": "uk"}}
{"id": "1_docx#128", "doc_id": "1_docx", "text": "Сучасні моделі, такі як BERT, GPT чи T5, вимагають надзвичайно великих обчислювальних ресурсів. Для їхнього навчання потрібні потужні графічні процесори (GPU), великі обсяги оперативної пам’яті та тривалі обчислення [3]. Навіть часткове донавчання моделей для конкретного домену може тривати годинами чи днями.\nУ корпоративних і навчальних умовах це створює фінансові та енергетичні обмеження. Саме тому розвивається напрям ефективних моделей\n(efficient AI) — таких, як DistilBERT, TinyLlama чи ALBERT, які споживають менше ресурсів при збереженні високої точності [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.194417", "order": 128, "lang": "uk"}}
{"id": "1_docx#129", "doc_id": "1_docx", "text": "Проблема пояснюваності (Explainability)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.199077", "order": 129, "lang": "uk"}}
{"id": "1_docx#130", "doc_id": "1_docx", "text": "Ще один суттєвий виклик — інтерпретація результатів моделей. Класичні методи (TF-IDF, SVM) дають змогу зрозуміти, які саме ознаки вплинули на результат. Натомість нейронні мережі — це «чорні скриньки», логіка роботи яких не завжди зрозуміла [5].\nУ наукових або юридичних системах це неприйнятно, адже користувач має знати, чому модель зробила певний висновок. Саме тому активно розвивається напрям Explainable AI (XAI), що поєднує точність глибинного навчання з інтерпретованістю статистичних методів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.202369", "order": 130, "lang": "uk"}}
{"id": "1_docx#131", "doc_id": "1_docx", "text": "Прикладом можуть бути візуалізації важливості токенів у реченні чи методи LIME і SHAP, які показують, які слова вплинули на класифікацію.\nМовна неоднорідність і підтримка української мови", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.204050", "order": 131, "lang": "uk"}}
{"id": "1_docx#132", "doc_id": "1_docx", "text": "Суттєвим бар’єром залишається багатомовність і різна якість підтримки мов у бібліотеках. Для англійської мови існують великі корпуси, тоді як для української — вони обмежені. Наприклад, spaCy має часткову підтримку української морфології, але без повноцінного синтаксичного аналізу [6].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.206034", "order": 132, "lang": "uk"}}
{"id": "1_docx#133", "doc_id": "1_docx", "text": "Це ускладнює автоматизацію аналітики в українських організаціях і підкреслює потребу у національних корпусах даних та відкритих моделях, адаптованих до українського контексту.\nІнтеграція NLP із реальними ІТ-системами", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.207804", "order": 133, "lang": "uk"}}
{"id": "1_docx#134", "doc_id": "1_docx", "text": "Нерідко навіть високоточні моделі залишаються ізольованими — їх важко впровадити у реальні бізнес-процеси. CRM-платформи, ERP-системи чи\nдокументообіги використовують різні технологічні стеки (SQL, NoSQL, REST, GraphQL), тому інтеграція NLP-алгоритмів вимагає розробки універсальних API та конекторів [7].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.211235", "order": 134, "lang": "uk"}}
{"id": "1_docx#135", "doc_id": "1_docx", "text": "Це стало передумовою появи таких фреймворків, як LangChain та LlamaIndex, що поєднують мовні моделі з базами даних і зовнішніми джерелами знань.\nЯкість текстових даних і семантична неоднозначність\nЖодна модель не працює добре без якісних вхідних даних.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.213881", "order": 135, "lang": "uk"}}
{"id": "1_docx#136", "doc_id": "1_docx", "text": "Помилки, жаргон, скорочення, неоднозначність (наприклад, слово «банк» — фінансова установа чи берег річки) значно впливають на результати [8]. Для їх усунення використовують етапи очищення, нормалізації, лематизації та векторизації, які дозволяють зменшити шум у даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.216593", "order": 136, "lang": "uk"}}
{"id": "1_docx#137", "doc_id": "1_docx", "text": "Саме на цьому етапі реалізується найбільша частина інтелектуальної роботи у системах обробки неструктурованих текстів.\nБезпека та конфіденційність даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.218378", "order": 137, "lang": "uk"}}
{"id": "1_docx#138", "doc_id": "1_docx", "text": "Оскільки більшість систем NLP працюють з чутливою інформацією, актуальним є питання захисту персональних даних і локальної обробки. Хмарні сервіси (як-от OpenAI API чи Claude AI) не завжди відповідають вимогам корпоративної або державної безпеки.\nТому зростає попит на офлайн-моделі та приватні рішення, що розгортаються у внутрішніх мережах підприємств без передачі даних третім сторонам [9].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.220916", "order": 138, "lang": "uk"}}
{"id": "1_docx#139", "doc_id": "1_docx", "text": "Напрями вдосконалення\nДля подолання наведених проблем сформувалися кілька ключових напрямів розвитку методів обробки неструктурованих даних:\nМультимодальні моделі — поєднання тексту, зображень, аудіо та відео (наприклад, CLIP, Gemini, Kosmos-1), що дозволяє системам аналізувати інформацію комплексно.\nОптимізація ресурсів — створення полегшених архітектур (DistilBERT, TinyLlama, Mistral 7B), які зберігають продуктивність, але працюють на споживацькому обладнанні.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.223627", "order": 139, "lang": "uk"}}
{"id": "1_docx#140", "doc_id": "1_docx", "text": "Пояснюваність моделей (XAI) — розробка інтерпретованих методів аналізу, що дають змогу пояснювати результати користувачам.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.226579", "order": 140, "lang": "uk"}}
{"id": "1_docx#141", "doc_id": "1_docx", "text": "Покращення якості даних — автоматичне виявлення шуму, нормалізація, переклад, фільтрація та сегментація контенту.\nСтандартизація інтеграцій — розробка відкритих API та протоколів взаємодії для універсальної роботи NLP-модулів у різних системах.\nТаблиця 1.6 Основні проблеми та напрями їх вирішення", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.229591", "order": 141, "lang": "uk"}}
{"id": "1_docx#142", "doc_id": "1_docx", "text": "Проблема Причина Можливі напрями вдосконалення Неоднорідність форматів Різні структури документів (PDF, HTML, DOCX) Використання універсальних парсерів (Unstructured.io) Висока ресурсоємність Складні моделі з мільйонами параметрів Використання полегшених архітектур (DistilBERT, TinyLlama) Непояснюваність результатів «Чорна скринька» нейронних мереж Методи XAI, візуалізація уваги, SHAP-аналіз Багатомовність Обмежені корпуси української мови Створення відкритих національних корпусів Складність інтеграції Несумісні технологічні стеки Використання LangChain, REST API, FAISS Якість тексту Наявність шуму, помилок Очищення, нормалізація, векторизація Безпека даних Обробка чутливої інформації Локальні (on-premise) рішення, приватні LLM", "type": "Table", "meta": {"ingested_at": "2025-11-13T18:47:57.234031", "order": 142, "lang": "uk"}}
{"id": "1_docx#143", "doc_id": "1_docx", "text": "Рис. 1.3. Узагальнена схема проблем і шляхів удосконалення методів обробки неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.236976", "order": 143, "lang": "uk"}}
{"id": "1_docx#144", "doc_id": "1_docx", "text": "1.7. Висновки до розділу 1", "type": "Title", "meta": {"ingested_at": "2025-11-13T18:47:57.239074", "order": 144, "lang": "uk"}}
{"id": "1_docx#145", "doc_id": "1_docx", "text": "У першому розділі було проведено всебічний теоретичний аналіз проблеми обробки неструктурованих даних, окреслено сучасні тенденції та виклики, а також визначено методологічні основи, на яких базується подальша частина дипломного дослідження. Аналіз показав, що обсяг неструктурованої інформації у світі стрімко зростає, і більшість даних, які сьогодні генеруються людиною чи машинами, мають саме неструктурований характер — це тексти,\nзображення, відео, аудіо, повідомлення у соціальних мережах, електронна пошта, технічна документація тощо. Такі дані не мають чіткої схеми зберігання і потребують спеціальних методів для вилучення, структуризації та аналізу.\nБуло встановлено, що традиційні методи обробки текстової інформації, зокрема статистичні підходи на кшталт Bag of Words або TF-IDF, мають обмежену здатність до розуміння контексту та семантики. Їх перевагою є простота реалізації та зрозумілість результатів, однак у сучасних умовах, коли мова йде про глибоке семантичне розпізнавання, ці підходи стають недостатніми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у ко", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.248315", "order": 145, "lang": "uk"}}
{"id": "1_docx#146", "doc_id": "1_docx", "text": "зі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у контексті, аналізувати зв’язки між фразами та реченнями, а також формувати осмислені векторні представлення тексту. Ці технології створили основу для побудови систем нового покоління — інтелектуальних пошукових механізмів, аналітичних платформ і мовних асистентів.\nУ ході огляду інструментів було виокремлено три рівні екосистеми для роботи з текстовими даними. Перший рівень становлять традиційні бібліотеки Python (NLTK, spaCy, scikit-learn), які забезпечують базову лінгвістичну обробку та підготовку текстів. Другий рівень охоплює сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured), що дозволяють реалізовувати глибокий семантичний аналіз, генерацію текстів і роботу з великими мовними моделями. Третій рівень формують інтеграційні рішення (FAISS, ChromaDB, ElasticSearch), які забезпечують збереження, векторизацію, пошук і зв’язок між компонентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки неструктурованих даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.257140", "order": 146, "lang": "uk"}}
{"id": "1_docx#147", "doc_id": "1_docx", "text": "Разом із тим, проведений аналіз виявив низку ключових проблем, які залишаються невирішеними у сфері NLP та аналітики текстів. Серед них —", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:47:57.262127", "order": 147, "lang": "uk"}}
{"id": "1_docx#148", "doc_id": "1_docx", "text": "неоднорідність форматів даних, висока ресурсомісткість сучасних моделей, складність інтерпретації результатів, брак універсальних методів для багатомовного аналізу, а також питання інтеграції таких систем у реальні бізнес-процеси. Водночас визначено основні напрями вдосконалення існуючих методів: розвиток мультимодальних моделей, підвищення ефективності обчислень, створення пояснюваних моделей (Explainable AI) та вдосконалення інструментів попередньої обробки тексту.\nПідсумовуючи результати теоретичної частини, можна зробити висновок, що побудова ефективних систем аналізу неструктурованих даних потребує поєднання класичних методів статистичної обробки з сучасними підходами глибокого навчання та трансформерних моделей. Це дозволяє досягнути балансу між точністю, швидкодією та інтерпретованістю результатів.\nСаме тому у подальших розділах дипломної роботи буде розглянуто розробку універсального методу обробки неструктурованих текстових даних, який інтегрує сучасні бібліотеки (Unstructured, Transformers, SentenceTransformers, FAISS) у єдиний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів ін", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.272694", "order": 148, "lang": "uk"}}
{"id": "1_docx#149", "doc_id": "1_docx", "text": "ний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів інформації та створення інструменту, придатного для практичного використання у системах аналітики, досліджень і бізнесу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:47:57.276982", "order": 149, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#1", "doc_id": "1_Jira_____________________403_docx", "text": "Міністерство освіти та науки України\nДержавний вищий навчальний заклад “Київський національний економічний університет\nімені Вадима Гетьмана”\nКафедра інформаційних систем\nв економіці\nДисципліна “Проектування інформаційних систем”\nЗВІТ", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.627527", "order": 1, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#2", "doc_id": "1_Jira_____________________403_docx", "text": "з лабораторної роботи № 1", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.629721", "order": 2, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#3", "doc_id": "1_Jira_____________________403_docx", "text": "“Керування життєвим циклом ІС з використанням гнучких методологій”\nПідготував:\nстудент 4 курсу групи ІН-403\nОліфіренко Кирило Андрійович\nспеціальності “Комп’ютерні науки”\nПрийняла:\nас. Сквордякова О.О.\nКиїв - 2023", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.632737", "order": 3, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#4", "doc_id": "1_Jira_____________________403_docx", "text": "Лабораторна робота № 1", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.661641", "order": 4, "lang": "mk"}}
{"id": "1_Jira_____________________403_docx#5", "doc_id": "1_Jira_____________________403_docx", "text": "“Керування життєвим циклом ІС з використанням гнучких методологій”\nТема проекту: Проектування інформаційної системи для адміністрування готелю.\nХід роботи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.664339", "order": 5, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#6", "doc_id": "1_Jira_____________________403_docx", "text": "Створення проекту в Jira", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T18:50:53.669400", "order": 6, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#7", "doc_id": "1_Jira_____________________403_docx", "text": "На етапі початку моєї роботи в системі Jira, я вирішив створити проектну сторінку для зручності управління всією інформацією, якою користуюсь під час виконання мого курсового проекту та лабораторних робіт з проектування інформаційних систем. Це дозволить мені систематизувати дані та спростить процес планування завдань. Я обрала гнучку методологію SCRUM для свого проекту.\nЦя сторінка стане центральним місцем, де я буду зберігати всі необхідні дані та документацію. SCRUM дозволяє мені ефективно керувати робочим процесом та швидко реагувати на зміни в ході виконання проекту. Я можу створювати backlog завдань, планувати спринти та визначати пріоритети завдань з урахуванням актуальних потреб проекту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.674945", "order": 7, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#8", "doc_id": "1_Jira_____________________403_docx", "text": "Додатково, я можу використовувати Jira для створення та відстеження задач, призначення їх відповідальним особам та моніторингу прогресу.\nРис. 1 – Вибір гнучкої методології як SCRUM\nРис. 2 – Реєстрація у Jira та створений проект та дошка\nРис. 3 – Заповнюємо деталі нашого проекту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.678281", "order": 8, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#9", "doc_id": "1_Jira_____________________403_docx", "text": "При створенні мого проекту я відразу обрав шаблон, спеціально адаптований для використання методології SCRUM. Це дозволило автоматично створити необхідні сторінки, включаючи \"Дошку\", на якій розміщуються всі завдання поточного спрінта.\nЦя \"Дошка\" стала центральним елементом для відстеження та організації задач. Тут знаходяться усі необхідні завдання для поточного етапу", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.681510", "order": 9, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#10", "doc_id": "1_Jira_____________________403_docx", "text": "проекту, де я можу визначити їх статус, відмітити прогрес та відстежувати виконання.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.683334", "order": 10, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#11", "doc_id": "1_Jira_____________________403_docx", "text": "Щодо теми мого проекту, \"Інформаційна система для адміністрування готелю\", ця методологія та інструмент Jira є ідеальними для ефективного впорядкування завдань, пов'язаних з розробкою системи. Я можу визначати та пріоритизувати функціональні вимоги, створювати ітерації розробки та відстежувати прогрес впродовж усього проекту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.686448", "order": 11, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#12", "doc_id": "1_Jira_____________________403_docx", "text": "Рис. 4-5 – Створюємо спринт “Прототипування та технічне завдання”\n“Беклог” – де розміщуються всі поточні задачі нашого проекту:\nРис. 5 – Запускаємо перший спринт “Прототипування та технічне завдання”\nКожна заявка виглядає так:\nРис. 6-7 – Виконана задача “Прототипування”\nПідєднуємо Figma:\nРис. 8-9 – Підєднуємо Figma до Jira\nЗробимо підзадачі:\nРис. 10 – Зробили під задачі для технічного завдання\nТакож під’єдннуємо репозиторій Git:\nРис. 11-12 – підєднуємо Git для Jira\nСтворюмо сторінку в Сonfluence і розміщую тут лабораторні роботи, також можна буде додати сюди вже готові звіти з виконаними лабораторними роботами протягом семестру:\nРис. 13-14 – Сторінка в Сonfluence з готовими звітами лабораторних робіт\nРис. 15 – Запускаємо спринт з встановленими статусами задач\nРис. 16-17 – Дошка з нашим спринтом\nІсторія виконаних задач", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.691936", "order": 12, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#13", "doc_id": "1_Jira_____________________403_docx", "text": "Під час семестру я виконував лабораторні роботи та деякі роботи по проекту. На цій вкладці відображається історія всіх виконаних мною завдань.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.694206", "order": 13, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#14", "doc_id": "1_Jira_____________________403_docx", "text": "Рис. 18-20 – Історія всіх виконаних завдань\nВисновки:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.696236", "order": 14, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#15", "doc_id": "1_Jira_____________________403_docx", "text": "У цій лабораторній роботі я взаємодіяв з системою управління життєвим циклом ІС – Jira. Мною була створена власна сторінка для проекту, де я можу легко створювати нові завдання та моніторити їх статуси. Також є можливість включати завдання в спрінти та виконувати їх в рамках цих ітерацій. Це істотно поліпшує продуктивність під час виконання робіт. Також, я ознайомився як підключати доп ресурси, програми до Jira, ми підключили клікабельний прототип Figma та особистий репозиторій Git.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T18:50:53.699760", "order": 15, "lang": "uk"}}
{"id": "1_Jira_____________________403_docx#16", "doc_id": "1_Jira_____________________403_docx", "text": "Власний досвід підтверджує, що використання систем управління життєвим циклом значно спрощує відстеження внесених змін та їх синхронізацію під час колективної роботи.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T18:50:53.701662", "order": 16, "lang": "uk"}}
{"id": "Microsoft_Word__2__docx#1", "doc_id": "Microsoft_Word__2__docx", "text": "Мене звати Кирило Оліфіренко, я аналітик із фокусом на SQL-розробку, адміністрування баз даних, оптимізацію продуктивності запитів і аналіз даних. Українська — моя рідна мова, якою користуюся щодня вдома, в колі близьких та в професійному середовищі Києва. Англійську я почав вивчати ще в школі, а згодом значно покращив її під час навчання у галузі комп’ютерних наук. Робота з англомовною технічною документацією та спілкування у міжнародних проєктах допомогли мені стати впевненим користувачем цієї мови. Моя освіта включає диплом молодшого спеціаліста за спеціальністю \"Інженерія програмного забезпечення\", а також диплом бакалавра з комп’ютерних наук Київського національного економічного університету. Наразі я здобуваю ступінь магістра з комп’ютерних наук у Київському національному торговельно-економічному університеті, де очікую завершити навчання у 2025 році. Професійно я працював SQL-розробником у ТОВ \"Український центр безпеки\" (УЦБ) у Києві, де займався управлінням великими масивами даних і створенням аналітичних звітів для підтримки прийняття бізнес-рішень. Мої технічні навички охоплюють роботу з різними СУБД — MS SQL Server, PostgreSQL, MySQL, MongoDB, Oracle, Neo4j. Також, я ма", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:03:09.949592", "order": 1, "lang": "uk"}}
{"id": "Microsoft_Word__2__docx#2", "doc_id": "Microsoft_Word__2__docx", "text": " прийняття бізнес-рішень. Мої технічні навички охоплюють роботу з різними СУБД — MS SQL Server, PostgreSQL, MySQL, MongoDB, Oracle, Neo4j. Також, я маю досвід у використанні інструментів бізнес-аналітики, зокрема Power BI та Tableau, що дозволяє візуалізувати дані та ефективно комунікувати результати. Окрім цього, я працював з ETL-процесами, моделюванням даних і застосовував Python для поглибленого аналізу та автоматизації рутинних задач. Я володію стандартною українською мовою, характерною для Києва, що допомагає мені легко знаходити спільну мову з колегами та партнерами. Маю розвинуте аналітичне мислення і системний підхід до вирішення складних задач, завжди прагну покращувати свої навички та впроваджувати інновації у роботу. Я хочу працювати на цій платформі, тому що мені цікаво застосовувати свої аналітичні навички та знання у реальних проєктах, які мають практичну цінність. Тут я бачу можливість постійно розвиватися, навчатися новому і долучатися до різноманітних завдань — від аналізу текстів до роботи з даними й штучним інтелектом. Мені подобається, що платформа пропонує гнучкий графік і можливість працювати з дому, що дає змогу поєднувати роботу з навчанням. Я ціную можливіс", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:03:09.955925", "order": 2, "lang": "uk"}}
{"id": "Microsoft_Word__2__docx#3", "doc_id": "Microsoft_Word__2__docx", "text": "ом. Мені подобається, що платформа пропонує гнучкий графік і можливість працювати з дому, що дає змогу поєднувати роботу з навчанням. Я ціную можливість отримувати чесну оплату за свою працю та бути частиною глобальної спільноти професіоналів. Також, для мене важливо працювати там, де можна реально бачити результати своєї роботи і розуміти, що вона корисна для інших. Я мотивований старанно виконувати завдання і готовий докладати максимум зусиль для досягнення високої якості.\nMy name is Kyrylo Olifirenko, and I am an analyst specializing in SQL development, database administration, query performance optimization, and data analysis. Ukrainian is my native language, which I use daily at home, among close friends and family, and in my professional environment in Kyiv. I started learning English back in school and later significantly improved my skills during my studies in computer science. Working with English technical documentation and communicating in international projects helped me become a confident user of the language.\nMy education includes an associate degree in Software Engineering as well as a bachelor’s degree in Computer Science from Kyiv National Economic University. Curr", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:03:09.963385", "order": 3, "lang": "en"}}
{"id": "Microsoft_Word__2__docx#4", "doc_id": "Microsoft_Word__2__docx", "text": "n includes an associate degree in Software Engineering as well as a bachelor’s degree in Computer Science from Kyiv National Economic University. Currently, I am pursuing a Master of Science degree in Computer Science at Kyiv National University of Trade and Economics, with an expected graduation date in 2025.\nProfessionally, I have worked as an SQL developer at LLC \"Ukrainian Security Center\" (USC) in Kyiv, where I managed large datasets and created analytical reports to support business decision-making. My technical skills cover working with various database management systems, including MS SQL Server, PostgreSQL, MySQL, MongoDB, Oracle, and Neo4j. I also have experience using business analytics tools such as Power BI and Tableau, which allow me to visualize data and communicate results effectively. Additionally, I have worked with ETL processes, data modeling, and have applied Python for in-depth analysis and automation of routine tasks.\nI speak standard Ukrainian, typical for the Kyiv region, which helps me easily find common ground with colleagues and partners. I have a well-developed analytical mindset and a systematic approach to solving complex problems. I am always eager t", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:03:09.967981", "order": 4, "lang": "en"}}
{"id": "Microsoft_Word__2__docx#5", "doc_id": "Microsoft_Word__2__docx", "text": "nd with colleagues and partners. I have a well-developed analytical mindset and a systematic approach to solving complex problems. I am always eager to enhance my skills and implement innovative solutions in my work.\nI want to work on this platform because I am interested in applying my analytical skills and knowledge to real projects that have practical value. Here, I see an opportunity to continuously grow, learn new things, and participate in diverse tasks — from text analysis to working with data and artificial intelligence.\nI appreciate that the platform offers a flexible schedule and the possibility to work from home, which allows me to combine work with my studies. I value the opportunity to receive fair compensation for my work and to be part of a global community of professionals.\nAlso, it is important for me to work in an environment where I can see the results of my efforts and understand that my work is useful to others. I am motivated to perform tasks diligently and am ready to put in maximum effort to achieve high quality.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:03:09.973329", "order": 5, "lang": "en"}}
{"id": "1_docx#1", "doc_id": "1_docx", "text": "Державний торговельно-економічний університет\nКафедра комп’ютерних наук та інформаційних систем\nКВАЛІФІКАЦІЙНА РОБОТА\nна тему:\n«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:36.974522", "order": 1, "lang": "uk"}}
{"id": "1_docx#2", "doc_id": "1_docx", "text": "Студента 2 курсу, 6м групи спеціальності 122 «Комп’ютерні науки» __________ підпис студента Оліфіренко Кирило Андрійович Науковий керівник кандидат фізико-математичних наук, доцент ___________ підпис керівника Томашевська Тетяна Володимирівна Гарант освітньої програми доктор фізико-математичних наук, професор ___________ підпис керівника Пурський Олег Іванович", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:36.977266", "order": 2, "lang": "uk"}}
{"id": "1_docx#3", "doc_id": "1_docx", "text": "Київ 2025\nДержавний торговельно-економічний університет\nФакультет інформаційних технологій\nКафедра комп’ютерних наук та інформаційних систем\nСпеціальність 122 «Комп’ютерні науки»\nОсвітня програма «Комп’ютерні науки»\nЗатверджую\nЗав. кафедри ____________Пурський О.І.\n«» грудня 2025р.\nЗавдання\nна кваліфікаційну роботу студенту\nОліфіренко Кирилу Андрійовичу\n(прізвище, ім’я, по батькові)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:36.980095", "order": 3, "lang": "uk"}}
{"id": "1_docx#4", "doc_id": "1_docx", "text": "Тема кваліфікаційної роботи", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:36.981342", "order": 4, "lang": "uk"}}
{"id": "1_docx#5", "doc_id": "1_docx", "text": "«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»\nЗатверджена наказом ректора від «» листопада 2025 р. № 4142", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:36.986675", "order": 5, "lang": "uk"}}
{"id": "1_docx#6", "doc_id": "1_docx", "text": "2. Строк здачі студентом закінченої роботи 15 листопада 2025 року", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:36.988864", "order": 6, "lang": "uk"}}
{"id": "1_docx#7", "doc_id": "1_docx", "text": "3. Цільова установка та вихідні дані до роботи\nМета роботи: розробка моделі та інформаційної технології оцінки конкурентоспроможності підприємств електронної торгівлі.\nОб’єкт дослідження: автоматизація процесів оцінювання конкурентоспроможності підприємств електронної торгівлі.\nПредмет дослідження: моделі, методи та інформаційні технології оцінювання конкурентоспроможності.\n4. Перелік графічного матеріалу______________________________________ ______________________________________________________________________________________________________________________________________________________________________________________________________\n5. Консультанти по роботі із зазначенням розділів, за якими здійснюється консультування:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:36.992251", "order": 7, "lang": "uk"}}
{"id": "1_docx#8", "doc_id": "1_docx", "text": "Розділ Консультант (прізвище, ініціали) Підпис, дата Завдання видав Завдання прийняв 1 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 2 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 3 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р.", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:36.994888", "order": 8, "lang": "uk"}}
{"id": "1_docx#9", "doc_id": "1_docx", "text": "6. Зміст кваліфікаційної роботи (перелік питань за кожним розділом)\nВСТУП\nРОЗДІЛ 1. ТЕОРЕТИЧНІ АСПЕКТИ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВА\n1.1. Аналіз проблематики та існуючих методів управління конкурентоспроможністю\n1.2. Особливості оцінки та управління конкурентоспроможністю підприємств електронної комерції\n1.3. Концептуальна модель оцінки та управління конкурентоспроможністю підприємств електронної комерції\nРОЗДІЛ 2. МАТЕМАТИЧНІ МОДЕЛІ ОЦІНКИ ТА УПРАВЛІННЯ КОНКУРЕНТОСПРОМОЖНІСТЮ ПІДПРИЄМСТВА\n2.1. Система показників та модель оцінки конкурентоспроможності підприємств електронної комерції\n2.2. Модель управління конкурентоспроможністю підприємства\n2.3. Моделювання процесу оцінки конкурентоспроможності підприємства\nРОЗДІЛ 3. ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВ ЕЛЕКТРОННОЇ ТОРГІВЛІ\n3.1. Інформаційно-логічна модель системи оцінки конкурентоспроможності підприємств\n3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств\n3.3.Технологія використання інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.000446", "order": 9, "lang": "uk"}}
{"id": "1_docx#10", "doc_id": "1_docx", "text": "інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання роботи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.003184", "order": 10, "lang": "uk"}}
{"id": "1_docx#11", "doc_id": "1_docx", "text": "№ Пор. Назва етапів кваліфікаційної роботи Строк виконання етапів роботи За планом фактично 1 2 3 4 1 Вибір теми кваліфікаційної роботи 01.11.2023 01.11.2023 2 Розробка та затвердження завдання на кваліфікаційну роботу 05.12.2023 05.12.2023 3 Вступ 01.05.2024 01.05.2024 4 РОЗДІЛ 1. Теоретичні аспекти оцінки конкурентоспроможності підприємства 14.06.2024 14.06.2024 5 Підготовка статті у збірник наукових статей магістрів 20.06.2024 20.06.2024 6 РОЗДІЛ 2. Математичні моделі оцінки та управління конкурентоспроможністю підприємства 05.09.2024 05.09.2024 7 РОЗДІЛ 3. Інформаційна технологія оцінки конкурентоспроможності підприємств електронної торгівлі 17.10.2024 17.10.2024 8 Висновки 21.10.2024 21.10.2024 9 Здача кваліфікаційної роботи на кафедру науковому керівнику 23.10.2024 23.10.2024 10 Попередній захист кваліфікаційної роботи 28.10.2024 28.10.2024 11 Виправлення зауважень, зовнішнє рецензування кваліфікаційної роботи 30.10.2024 30.10.2024 12 Представлення готової зшитої кваліфікаційної роботи на кафедру 04.11.2024 04.11.2024 13 Публічний захист кваліфікаційної роботи За розкладом роботи ЕК", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.008247", "order": 11, "lang": "uk"}}
{"id": "1_docx#12", "doc_id": "1_docx", "text": "8. Дата видачі завдання «5» грудня 2023 р", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.009830", "order": 12, "lang": "uk"}}
{"id": "1_docx#13", "doc_id": "1_docx", "text": "9. Керівник випускного кваліфікаційної роботи Самойленко Г.Т\n(прізвище, ініціали, підпис)\n10. Гарант освітньої програми Пурський О.І.\n(прізвище, ініціали, підпис)\n11. Завдання прийняв до виконання студент Оліфіренко К. ________\n(прізвище, ініціали, підпис)\n12. Відгук керівника кваліфікаційної роботи\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.013205", "order": 13, "lang": "uk"}}
{"id": "1_docx#14", "doc_id": "1_docx", "text": "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\nКерівник кваліфікаційної роботи\n______________________\n(підпис, дата)\n13. Висновок про кваліфікаційну роботу\nКваліфікаційна робота студента _______ Оліфіренко К.А.\n(прізвище, ініціали)\nможе бути допущена до захисту в екзаменаційній комісії.\nГарант освітньої програми Пурський О.І.\n(підпис, прізвище, ініціали)\nЗавідувач кафедри Пурський О.І.\n(підпис, прізвище, ініціали)\n«_____»_________________2025 р.\nАнотація\nУ кваліфікаційній роботі здійснено комплексну розробку моделей та інформаційної технології моніторингу соціально-економічних показників з", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.016629", "order": 14, "lang": "uk"}}
{"id": "1_docx#15", "doc_id": "1_docx", "text": "метою підвищення ефективності управління регіональним розвитком. Теоретично обґрунтовано основні положення формування і проведення соціально-економічного моніторингу та запропоновано концепцію створення інформаційної системи оціньовання показників соціально-економічного розвитку регіону. Розроблено метод автоматизованого розрахунку комплексної оцінки показників соціально-економічного розвитку. Створено автоматизовану Web-систему оціньовання показників соціально-економічного розвитку регіонів України.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.020674", "order": 15, "lang": "uk"}}
{"id": "1_docx#16", "doc_id": "1_docx", "text": "Ключові слова: соціально-економічний розвиток, математична модель, інтегральні показники, інформаційна технологія.\nAnotation", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.022740", "order": 16, "lang": "uk"}}
{"id": "1_docx#17", "doc_id": "1_docx", "text": "The qualification work is devoted to development of model and information technology of monitoring of social and economic indexes for the purpose of management efficiency increase by regional development The mechanism of the taking into account of differentiation of regional development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of social and economic development is offered and programmed. The Web-system for monitoring indicators of social and economic development of Ukraine regions is created and the technology of its use is developed.\nKeywords: social and economic development, mathematical model, integrated indicators, information technology.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.026620", "order": 17, "lang": "en"}}
{"id": "1_docx#18", "doc_id": "1_docx", "text": "ЗМІСТ\nВСТУП…………………………………………………..…………………………9\nРОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ…………….……..………….12\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних……………………………………………………..……………….….12", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.030841", "order": 18, "lang": "uk"}}
{"id": "1_docx#19", "doc_id": "1_docx", "text": "1.2 Поняття неструктурованих даних, їх роль у сучасних інформаційних системах………………………………………………...………….………..16", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.032579", "order": 19, "lang": "uk"}}
{"id": "1_docx#20", "doc_id": "1_docx", "text": "1.3 Методи та підходи до обробки текстових даних у системах штучного інтелекту ………………………………………………..…………………...20", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.036108", "order": 20, "lang": "uk"}}
{"id": "1_docx#21", "doc_id": "1_docx", "text": "1.4 Алгоритми й моделі обробки природної мови (NLP).……...………...20\n1.5 Огляд інструментів і бібліотек для роботи з текстовими даними………………………………………………...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.038418", "order": 21, "lang": "uk"}}
{"id": "1_docx#22", "doc_id": "1_docx", "text": "1.5.1 Традиційні бібліотеки Python для NLP (NLTK, spaCy, scikit-learn)……………………..…..……………...…………………………….....23", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.040741", "order": 22, "lang": "uk"}}
{"id": "1_docx#23", "doc_id": "1_docx", "text": "1.5.2 Сучасні фреймворки, сервіси (Transformers, OpenAI API, LangChain, Unstructured)……………………..…..…………………….…...23\n1.5.3 Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)….……………………………………...23\n1.6 Проблематика та напрями вдосконалення методів обробки неструктурованих даних………………………...…...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.043211", "order": 23, "lang": "uk"}}
{"id": "1_docx#24", "doc_id": "1_docx", "text": "1.7 Висновки до розділу 1……………………………..…………………...20\nРОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………………………………………………….…..26\n2.1 Постановка задачі, вимоги та функціональні можливості системи…26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.045305", "order": 24, "lang": "uk"}}
{"id": "1_docx#25", "doc_id": "1_docx", "text": "2.2 Архітектура методу обробки неструктурованих текстових даних…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.050274", "order": 25, "lang": "uk"}}
{"id": "1_docx#26", "doc_id": "1_docx", "text": "2.3. Розробка алгоритму розбиття документів на семантичні фрагменти (чанки)………………………………………………………………….……33\n2.4 Методи векторизації тексту та побудова семантичного індексу…...…………………….………………………………….…………26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.053565", "order": 26, "lang": "uk"}}
{"id": "1_docx#27", "doc_id": "1_docx", "text": "2.5 Реалізація бази знань та пошукового механізму ……..…………..…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.055027", "order": 27, "lang": "uk"}}
{"id": "1_docx#28", "doc_id": "1_docx", "text": "2.6 Висновки до розділу 2…………………………...……..…………..…..30\nРОЗДІЛ 3. ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ІНТЕРФЕЙС СИСТЕМИ………………………………………………………….....…………36\n3.1 Загальна структура програмного комплексу UXText Pipeline.………36", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.061833", "order": 28, "lang": "uk"}}
{"id": "1_docx#29", "doc_id": "1_docx", "text": "3.2 Інтеграція з бібліотекою Unstructured.io та FAISS ………….………..38\n3.3 Реалізація API-сервісу (FastAPI) для обробки запитів.………....……42\n3.4 Розробка користувацького інтерфейсу (Streamlit)…….…….………..38", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.063490", "order": 29, "lang": "uk"}}
{"id": "1_docx#30", "doc_id": "1_docx", "text": "3.5 Візуалізація результатів аналізу текстів .………...………...…………42\n3.6 Тестування та оцінювання ефективності роботи системи.……..……42\n3.7 Висновки до розділу 3.………...………………………….....…………42\nВИСНОВКИ……………………………..………………………………………48\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ……………...……………………..50\nДОДАТОК………………………………………………..……………..………52\nВСТУП", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.065811", "order": 30, "lang": "uk"}}
{"id": "1_docx#31", "doc_id": "1_docx", "text": "У сучасному світі, коли інформаційні потоки стрімко зростають, значна частина даних, які створює людина чи система, має неструктурований характер. Це тексти документів, електронні листи, новини, пости у соціальних мережах, коментарі, відгуки, звіти чи наукові статті. За статистичними оцінками, понад 80 відсотків усієї цифрової інформації становлять саме неструктуровані дані, які не мають чіткої схеми чи фіксованої структури зберігання. Проте саме вони містять найціннішу інформацію для аналізу змісту, виявлення тенденцій, автоматизованого прийняття рішень чи побудови систем штучного інтелекту. Проблема полягає в тому, що традиційні інструменти обробки даних не пристосовані для ефективної роботи з такими джерелами. Методи на кшталт статистичного аналізу або пошуку за ключовими словами не здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’язки між ними.\nАктуальність теми зумовлена необхідністю створення ефективних методів, що дозволяють обробляти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штуч", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.072816", "order": 31, "lang": "uk"}}
{"id": "1_docx#32", "doc_id": "1_docx", "text": "яти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штучного інтелекту, зокрема векторизації текстів і нейромережевих моделей природної мови (NLP), відкриває нові можливості для створення інтелектуальних систем аналізу тексту. Ці системи здатні не лише класифікувати чи шукати інформацію, а й знаходити семантичні зв’язки між фрагментами тексту, узагальнювати їх зміст і будувати рекомендації. Саме тому тема «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» є актуальною і відповідає сучасним тенденціям розвитку інформаційних технологій.\nМетою даної роботи є розробка методу та програмного забезпечення, що забезпечують ефективну обробку неструктурованих текстових даних шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завдання: провести аналіз існуючих методів обробки текстових даних і", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.078246", "order": 32, "lang": "uk"}}
{"id": "1_docx#33", "doc_id": "1_docx", "text": "сучасних підходів до представлення тексту; дослідити технології побудови векторних моделей, зокрема SentenceTransformers та FAISS; спроєктувати архітектуру програмного комплексу для інтеграції різних джерел даних; реалізувати систему UXText Pipeline, яка поєднує модулі обробки, зберігання та пошуку текстів; провести експериментальне тестування для оцінки ефективності запропонованого методу.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.081360", "order": 33, "lang": "uk"}}
{"id": "1_docx#34", "doc_id": "1_docx", "text": "Об’єктом дослідження є процес обробки неструктурованих текстових даних у системах аналітики. Предметом дослідження — методи й алгоритми нормалізації, векторизації та індексації текстів, що дозволяють підвищити якість пошуку, класифікації й аналізу текстової інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.083861", "order": 34, "lang": "uk"}}
{"id": "1_docx#35", "doc_id": "1_docx", "text": "Під час виконання роботи застосовувалися методи аналітичного огляду та синтезу для вивчення літератури й порівняння існуючих підходів; алгоритмічного проєктування — для створення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також статистичні методи оцінювання результатів, що дозволили кількісно виміряти покращення якості обробки текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.088528", "order": 35, "lang": "uk"}}
{"id": "1_docx#36", "doc_id": "1_docx", "text": "Наукова новизна роботи полягає у створенні універсального підходу до обробки неструктурованих даних, який поєднує модулі виділення тексту, нормалізації, семантичного подання та побудови індекса для швидкого пошуку. Запропонований метод дозволяє працювати з різними форматами документів (PDF, DOCX, HTML), об’єднуючи їх у єдину структуру даних без втрати змістової інформації. Крім того, у роботі реалізовано алгоритм побудови семантичного індекса з використанням SentenceTransformers і FAISS, що підвищує ефективність пошуку за змістом, а не лише за словами.\nПрактичне значення роботи полягає в тому, що розроблений програмний комплекс UXText Pipeline може бути використаний як основа для створення аналітичних систем у різних сферах — від бізнес-аналітики та освіти до наукових досліджень і документообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і\nпідвищує точність інформаційного пошуку. Розроблені рішення можуть бути впроваджені в установах, де необхідна швидка обробка текстових звітів, статей або відгуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.094536", "order": 36, "lang": "uk"}}
{"id": "1_docx#37", "doc_id": "1_docx", "text": "гуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» (2024 р.) та при реалізації програмного прототипу, опублікованого у відкритому доступі на GitHub. Під час експериментів підтверджено ефективність запропонованого методу — час обробки документів скоротився, а точність семантичного пошуку зросла порівняно з базовими алгоритмами.\nДипломна робота складається зі вступу, чотирьох розділів, висновків, списку використаних джерел і додатків. Загальний обсяг становить 70 сторінок, включає 25 рисунків, 12 таблиць і 3 додатки. У роботі послідовно викладено теоретичні основи, аналіз існуючих рішень, власну методику обробки текстів, результати експериментів і приклади практичного використання створеного програмного комплексу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.099218", "order": 37, "lang": "uk"}}
{"id": "1_docx#38", "doc_id": "1_docx", "text": "РОЗДІЛ 1.\nАНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.104441", "order": 38, "lang": "uk"}}
{"id": "1_docx#39", "doc_id": "1_docx", "text": "У сучасному цифровому світі обсяг інформації, який щоденно створюється, зростає з небаченою швидкістю. За оцінками аналітичної компанії IDC (International Data Corporation), до 2025 року загальний обсяг глобальних даних перевищить 175 зетабайт, причому понад 80 відсотків із них становитимуть саме неструктуровані дані — тексти, зображення, відео, аудіозаписи, повідомлення в соціальних мережах тощо [1]. Ці дані генеруються користувачами, організаціями, сенсорами та системами штучного інтелекту. Проте, попри їхню кількість і потенційну цінність, лише невелика частина реально використовується для аналітики через відсутність ефективних методів автоматичної обробки.\nПроблема полягає у тому, що більшість традиційних інформаційних систем побудовані для роботи зі структурованими даними — таблицями, числовими показниками, логами, базами SQL. Такі системи ефективно працюють лише тоді, коли дані мають чітко визначену схему зберігання: є поля, типи, формат і зв’язки між ними. Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані з", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.110488", "order": 39, "lang": "uk"}}
{"id": "1_docx#40", "doc_id": "1_docx", "text": "тувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані зображення. Як наслідок, традиційні системи управління даними (DBMS) не здатні безпосередньо інтерпретувати зміст таких файлів, а тому значна частина потенційно корисної інформації залишається поза увагою аналітичних алгоритмів [2].\nЯ, Оліфіренко Кирило, як студент спеціальності «Комп’ютерні науки», у процесі дослідження дійшов висновку, що проблема обробки неструктурованих даних є не лише технічною, а й методологічною. Навіть у сучасних інформаційних середовищах — як у бізнесі, так і в державних установах — зберігаються великі масиви документів у різних форматах: звіти у PDF, службові документи у DOCX, переписки у форматі електронної пошти, повідомлення з месенджерів, а також текстові поля у CRM-системах. Всі ці", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.115111", "order": 40, "lang": "uk"}}
{"id": "1_docx#41", "doc_id": "1_docx", "text": "джерела містять важливу інформацію, яку складно уніфікувати без єдиної системи автоматичної обробки.\nАктуальність цієї проблеми підтверджується низкою об’єктивних чинників:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.117178", "order": 41, "lang": "uk"}}
{"id": "1_docx#42", "doc_id": "1_docx", "text": "Стрімке зростання обсягів текстових даних.\nЩосекунди в інтернеті з’являються мільйони повідомлень — лише у Twitter щодня публікується понад 500 млн твітів, у Facebook — понад 300 млн постів, а також понад 4,5 млрд електронних листів у глобальному масштабі [3]. Така кількість текстів робить ручну обробку неможливою, що потребує створення інтелектуальних алгоритмів аналізу.\nНеоднорідність джерел та форматів.\nТексти можуть бути представлені в різних форматах (PDF, DOCX, HTML, TXT, Markdown), з різною структурою, мовою, стилем і контекстом. У багатьох випадках інформація поєднує текст і візуальні елементи (таблиці, формули, підписи до зображень), що створює додаткові труднощі при обробці.\nЗростання ролі неструктурованих даних у прийнятті рішень.\nУ багатьох сферах — бізнесі, медицині, освіті, науці, державному управлінні — від якісного аналізу текстової інформації залежить точність прогнозів і ефективність управлінських дій.\nУ бізнесі — аналіз відгуків клієнтів дозволяє виявляти рівень задоволеності.\nУ медицині — аналіз медичних карт допомагає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального ", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.123950", "order": 42, "lang": "uk"}}
{"id": "1_docx#43", "doc_id": "1_docx", "text": "агає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального методу обробки.\nІснуючі рішення часто обмежені конкретними форматами або мовами, а результати залежать від якості текстів, наявності помилок, скорочень,\nзмішаних мов. Тому актуальним є завдання створення гнучкої системи, здатної об’єднувати різні етапи обробки — від вилучення тексту до його семантичного аналізу.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.127058", "order": 43, "lang": "uk"}}
{"id": "1_docx#44", "doc_id": "1_docx", "text": "Ключовою метою сучасних досліджень у цій галузі є розробка методів, які поєднують традиційні алгоритми лінгвістичного аналізу з сучасними моделями машинного навчання та глибокого навчання (Deep Learning). Ці методи включають такі етапи, як:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.129029", "order": 44, "lang": "uk"}}
{"id": "1_docx#45", "doc_id": "1_docx", "text": "вилучення тексту з документів різних форматів;\nочищення від шуму (HTML-тегів, непотрібних символів);\nтокенізація й лематизація;\nвизначення мови;\nпобудова векторних подань (векторизація);\nіндексація та семантичний пошук.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.131082", "order": 45, "lang": "uk"}}
{"id": "1_docx#46", "doc_id": "1_docx", "text": "Зокрема, у моїй роботі розглядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семантичного пошуку за змістом). Такий підхід дозволяє створити універсальний пайплайн UXText Pipeline, який автоматизує процес перетворення неструктурованих даних у придатну для аналізу форму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.133843", "order": 46, "lang": "uk"}}
{"id": "1_docx#47", "doc_id": "1_docx", "text": "Разом із тим, залишається низка викликів, які обумовлюють актуальність подальших досліджень:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.135809", "order": 47, "lang": "uk"}}
{"id": "1_docx#48", "doc_id": "1_docx", "text": "неоднорідність форматів і різна якість текстових даних;\nпотреба у великих обчислювальних ресурсах для сучасних моделей (BERT, GPT);\nвідсутність єдиних стандартів інтеграції NLP-моделей у корпоративні системи;\nскладність пояснення результатів глибоких моделей (“чорна скринька”);\nобмежена підтримка української мови у більшості відкритих NLP-бібліотек.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.139407", "order": 48, "lang": "uk"}}
{"id": "1_docx#49", "doc_id": "1_docx", "text": "З огляду на це, можна зробити висновок, що обробка неструктурованих даних є однією з центральних проблем сучасної науки про дані (Data Science). Її ефективне вирішення дозволить перейти від хаотичних текстових потоків до системного аналізу, що підвищить якість рішень у бізнесі, науці й суспільстві.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.142002", "order": 49, "lang": "uk"}}
{"id": "1_docx#50", "doc_id": "1_docx", "text": "Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих текстових даних, який забезпечить:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.144937", "order": 50, "lang": "uk"}}
{"id": "1_docx#51", "doc_id": "1_docx", "text": "універсальність щодо типів і форматів документів;\nавтоматичність та масштабованість;\nточність семантичного пошуку;\nможливість інтеграції з реальними інформаційними системами.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.146921", "order": 51, "lang": "uk"}}
{"id": "1_docx#52", "doc_id": "1_docx", "text": "Саме ця мета визначає подальшу логіку роботи — у наступних розділах буде розроблено архітектуру, реалізацію та апробацію програмного комплексу UXText Pipeline, який поєднує сучасні алгоритми NLP, бібліотеки Python і векторні бази даних для побудови ефективної системи аналізу неструктурованих текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.149377", "order": 52, "lang": "uk"}}
{"id": "1_docx#53", "doc_id": "1_docx", "text": "1.2. Поняття неструктурованих даних, їх роль у сучасних інформаційних системах", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.151212", "order": 53, "lang": "uk"}}
{"id": "1_docx#54", "doc_id": "1_docx", "text": "У сучасних інформаційних системах усі дані, з якими працюють користувачі, аналітики або системи штучного інтелекту, умовно поділяються на три основні типи — структуровані, напівструктуровані та неструктуровані. Така класифікація визначається рівнем організованості інформації, способом її зберігання та можливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управління знаннями.\nСтруктуровані дані — це дані, які зберігаються у чітко визначеній формі та організовані за наперед встановленою схемою. Вони мають фіксовану структуру — таблиці, поля, типи даних, зв’язки між записами. Прикладами таких даних є бази даних SQL, електронні таблиці Excel, реєстри або облікові системи. Основною перевагою структурованих даних є можливість швидкого аналізу за допомогою мов запитів (SQL), статистичних методів чи систем бізнес-аналітики (BI). Проте їхній головний недолік полягає у тому, що вони описують лише формалізовану частину реальності — числа, категорії, дати, тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.158147", "order": 54, "lang": "uk"}}
{"id": "1_docx#55", "doc_id": "1_docx", "text": "тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце між структурованими й неструктурованими. Вони мають певну внутрішню організацію, але не підпорядковуються жорсткій схемі. Такі дані часто представлені у форматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має стандартні поля (“від кого”, “кому”, “дата”), але зміст повідомлення — це вільний текст. Подібні дані активно використовуються у вебтехнологіях, обміні інформацією між сервісами (API) та у NoSQL-базах даних (MongoDB, Cassandra, Firebase). Їхня гнучкість є перевагою, однак аналіз таких даних потребує спеціалізованих методів парсингу, фільтрації й нормалізації [3].\nНеструктуровані дані — це інформація, яка не має фіксованої структури і не може бути представлена у вигляді таблиць або полів. До них належать тексти документів, повідомлення у месенджерах, публікації у соціальних мережах, звіти, аудіо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Осн", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.163624", "order": 55, "lang": "uk"}}
{"id": "1_docx#56", "doc_id": "1_docx", "text": "іо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Основна особливість таких даних полягає в тому, що вони передають значення через зміст, а не через форму. Два документи можуть мати зовсім різну структуру, але описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу\nконтексту, семантики, синтаксису і навіть прихованих зв’язків між словами [4].\nЗа оцінками компанії IBM, близько 80% усієї інформації, створеної людьми, належить до неструктурованих даних [5]. Це означає, що лише невелика частина всієї інформації у світі може бути легко оброблена стандартними базами даних, тоді як решта — тексти, документи, звіти, статті, пости, коментарі — залишаються невикористаними. Саме тому обробка неструктурованих даних стає однією з найважливіших задач сучасної комп’ютерної науки та аналітики.\nУ контексті архітектури інформаційних систем неструктуровані дані зберігаються переважно у файлових системах, хмарних сховищах або об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasti", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.169970", "order": 56, "lang": "uk"}}
{"id": "1_docx#57", "doc_id": "1_docx", "text": "бо об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasticsearch, дозволяють зберігати документи у будь-якому форматі та індексувати їх для пошуку. Проте ключову роль тут відіграють алгоритми попередньої обробки — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [6].\nРоль неструктурованих даних у сучасних ІТ-системах особливо важлива в контексті розвитку штучного інтелекту (AI) та Natural Language Processing (NLP). Саме ці дані є основним джерелом навчання для великих мовних моделей (LLM), таких як GPT, BERT, RoBERTa, LLaMA, T5 тощо. Їх тренування здійснюється на величезних масивах текстів — статтях, наукових працях, форумах, енциклопедіях, книгах [7]. Без таких джерел штучний інтелект не міг би досягти нинішнього рівня розуміння природної мови, контексту, емоційних відтінків і логічних зв’язків.\nЗ точки зору бізнесу, аналіз неструктурованих даних відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцме", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.175708", "order": 57, "lang": "uk"}}
{"id": "1_docx#58", "doc_id": "1_docx", "text": "відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцмережах або результатів опитувань, отримують конкурентну перевагу — вони швидше\nреагують на ринкові зміни, прогнозують поведінку споживачів і вдосконалюють свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині — аналіз клінічних описів допомагає автоматично виявляти симптоми, а в державному секторі — підвищує ефективність управлінських рішень.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.179292", "order": 58, "lang": "uk"}}
{"id": "1_docx#59", "doc_id": "1_docx", "text": "Таблиця 1.1 Порівняльна характеристика структурованих, напівструктурованих та неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.182702", "order": 59, "lang": "uk"}}
{"id": "1_docx#60", "doc_id": "1_docx", "text": "Тип даних Приклади Характеристика Приклади використання Структуровані SQL, Excel, CRM Чітка схема, таблиці, поля Облік, звітність, фінансовий аналіз Напівструктуровані JSON, XML, CSV Часткова організація, відсутність жорсткої схеми Обмін даними між системами, API Неструктуровані Тексти, PDF, аудіо, відео Відсутня структура, потребує інтелектуальної обробки NLP, аналітика, AI, Data Mining", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.186081", "order": 60, "lang": "uk"}}
{"id": "1_docx#61", "doc_id": "1_docx", "text": "Таким чином, неструктуровані дані становлять основну частину сучасного цифрового контенту. Вони є не лише джерелом знань, але й рушієм розвитку інтелектуальних технологій, аналітики та систем прийняття рішень. Їхнє ефективне використання вимагає поєднання методів комп’ютерної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що дозволяють перетворювати неструктуровані тексти у структуровану форму", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.190672", "order": 61, "lang": "uk"}}
{"id": "1_docx#62", "doc_id": "1_docx", "text": "для подальшого аналізу, що є важливим кроком до вдосконалення систем аналізу тексту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.192944", "order": 62, "lang": "uk"}}
{"id": "1_docx#63", "doc_id": "1_docx", "text": "1.3. Методи та підходи до обробки текстових даних у системах штучного інтелекту", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.195902", "order": 63, "lang": "uk"}}
{"id": "1_docx#64", "doc_id": "1_docx", "text": "Обробка текстових даних у системах штучного інтелекту є одним із найскладніших і водночас найдинамічніших напрямів розвитку сучасних інформаційних технологій. Людська мова — це гнучкий і багаторівневий механізм комунікації, який поєднує граматику, контекст, емоції, стиль та приховані смисли. Навчити комп’ютер розуміти текст означає створити модель, здатну не лише розпізнавати слова, а й аналізувати їхні зв’язки, контекст використання та семантичне навантаження [1].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.199684", "order": 64, "lang": "uk"}}
{"id": "1_docx#65", "doc_id": "1_docx", "text": "Залежно від рівня складності та точності, методи обробки текстових даних умовно поділяються на три основні групи:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.202408", "order": 65, "lang": "uk"}}
{"id": "1_docx#66", "doc_id": "1_docx", "text": "статистичні,\nметоди машинного навчання,\nнейромережеві (глибокі).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.205270", "order": 66, "lang": "uk"}}
{"id": "1_docx#67", "doc_id": "1_docx", "text": "Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити закономірності у частоті вживання слів [2]. Найвідомішим став метод Bag of Words (BoW), у якому кожен документ подається як набір слів без урахування порядку. Такий підхід простий у реалізації, але не враховує контекст, тому використовується для класифікації текстів і тематичного аналізу.\nІнший фундаментальний метод — TF-IDF (Term Frequency — Inverse Document Frequency) — оцінює важливість слова у конкретному документі відносно всієї колекції. Наприклад, слово “дані” зустрічається у всіх документах, тому має низьку вагу, тоді як термін “лематизація” зустрічається\nрідше, отже має більшу інформативність. TF-IDF залишається основою у пошукових системах (зокрема Google Search) і є відправною точкою для формування векторних подань текстів [3].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.209797", "order": 67, "lang": "uk"}}
{"id": "1_docx#68", "doc_id": "1_docx", "text": "Методи машинного навчання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.213628", "order": 68, "lang": "uk"}}
{"id": "1_docx#69", "doc_id": "1_docx", "text": "Подальший розвиток комп’ютерних технологій призвів до появи методів машинного навчання (Machine Learning), які дозволили враховувати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йому подати достатньо прикладів [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.216446", "order": 69, "lang": "uk"}}
{"id": "1_docx#70", "doc_id": "1_docx", "text": "Серед найпоширеніших алгоритмів:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.218141", "order": 70, "lang": "uk"}}
{"id": "1_docx#71", "doc_id": "1_docx", "text": "Наївний баєсівський класифікатор (Naive Bayes) — базується на теоремі Байєса і використовується для класифікації документів;\nМетод опорних векторів (SVM) — визначає межі між класами у багатовимірному просторі ознак;\nМетод k-ближчих сусідів (kNN) — класифікує тексти на основі схожості;\nЛогістична регресія — визначає ймовірність належності тексту до певної категорії.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.221527", "order": 71, "lang": "uk"}}
{"id": "1_docx#72", "doc_id": "1_docx", "text": "Такі алгоритми застосовувалися у задачах аналізу тональності тексту (sentiment analysis), тематичної класифікації, виявлення спаму чи автоматичного реферування. Їхнім обмеженням є потреба у ручному створенні ознак (feature engineering) — тобто необхідність перетворювати тексти у числові вектори на основі частотних характеристик (уніграми, біграми, триграми тощо).", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.224515", "order": 72, "lang": "uk"}}
{"id": "1_docx#73", "doc_id": "1_docx", "text": "Нейромережеві методи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.227255", "order": 73, "lang": "uk"}}
{"id": "1_docx#74", "doc_id": "1_docx", "text": "Наступним етапом еволюції стали нейромережеві моделі, які дозволили навчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які зберігають інформацію про значення та контекст.\nПершим проривом стала модель Word2Vec, розроблена в Google Research [5]. Вона навчалася на мільярдах слів і дозволяла розміщувати їх у спільному багатовимірному просторі, де схожі за змістом слова (наприклад, “кіт” і “пес”) знаходяться поруч. Подібні принципи використовують моделі GloVe (Global Vectors for Word Representation) [6] і FastText, розроблений у Meta AI Research, який враховує морфологічну структуру слова.\nГоловний прорив у галузі стався з появою трансформерних архітектур, які суттєво підвищили якість аналізу природної мови. Моделі типу BERT (Bidirectional Encoder Representations from Transformers) [7], GPT (Generative Pre-trained Transformer) [8] і SentenceTransformers використовують механізм Self-Attention (“механізм уваги”), який дозволяє моделі концентруватися на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьом", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.232253", "order": 74, "lang": "uk"}}
{"id": "1_docx#75", "doc_id": "1_docx", "text": "на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем і когнітивних аналітичних інструментів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.234630", "order": 75, "lang": "uk"}}
{"id": "1_docx#76", "doc_id": "1_docx", "text": "Етапи попередньої обробки тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.238806", "order": 76, "lang": "uk"}}
{"id": "1_docx#77", "doc_id": "1_docx", "text": "Перед застосуванням будь-яких алгоритмів штучного інтелекту текст проходить кілька стандартних етапів підготовки [9]:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.241437", "order": 77, "lang": "uk"}}
{"id": "1_docx#78", "doc_id": "1_docx", "text": "Токенізація — розбиття тексту на окремі одиниці (токени): слова, частини слів або символи.\nНормалізація — приведення тексту до єдиного вигляду (зменшення регістру, видалення пунктуації, стоп-слів).\nЛематизація або стемінг — зведення слів до базової форми (наприклад, “працюю”, “працював” → “працювати”).\nPOS-тегування (Part-of-Speech Tagging) — визначення частини мови для кожного слова.\nСинтаксичний аналіз (Parsing) — встановлення граматичних зв’язків між словами у реченні.\nСемантичний аналіз — визначення смислових відношень (“автор написав книгу” → дія, суб’єкт, об’єкт).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.245909", "order": 78, "lang": "uk"}}
{"id": "1_docx#79", "doc_id": "1_docx", "text": "Таблиця 1.2 Еволюція методів обробки текстових даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.248259", "order": 79, "lang": "uk"}}
{"id": "1_docx#80", "doc_id": "1_docx", "text": "Покоління Основні методи Приклади моделей Ключові характеристики Статистичні Bag of Words, TF-IDF – Простота реалізації, не враховує контекст Машинного навчання SVM, Naive Bayes, kNN Scikit-learn Використання вручну створених ознак, середня точність Нейромережеві Word2Vec, FastText, BERT, GPT TensorFlow, PyTorch Висока точність, контекстне розуміння, потреба у великих ресурсах", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.251308", "order": 80, "lang": "uk"}}
{"id": "1_docx#81", "doc_id": "1_docx", "text": "Інтеграція методів у сучасні системи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.253596", "order": 81, "lang": "uk"}}
{"id": "1_docx#82", "doc_id": "1_docx", "text": "Сучасні системи штучного інтелекту об’єднують усі ці методи у єдиний пайплайн обробки тексту — від очищення даних до формування векторного подання та пошуку за змістом [10].\nТиповий приклад:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.256617", "order": 82, "lang": "uk"}}
{"id": "1_docx#83", "doc_id": "1_docx", "text": "вилучення тексту з файлів різних форматів (PDF, DOCX, HTML);\nлематизація й очищення;\nперетворення тексту у вектори (SentenceTransformers);\nпобудова індексу у FAISS;\nздійснення семантичного пошуку.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.258754", "order": 83, "lang": "uk"}}
{"id": "1_docx#84", "doc_id": "1_docx", "text": "Саме цей підхід реалізовано у моєму дипломному проєкті UXText Pipeline, який демонструє практичну інтеграцію моделей NLP, бібліотеки Unstructured та індексаційного механізму FAISS у єдину систему.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.260694", "order": 84, "lang": "uk"}}
{"id": "1_docx#85", "doc_id": "1_docx", "text": "Отже, методи обробки текстових даних пройшли довгий шлях від простих статистичних моделей до глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не лише аналізують тексти, а й здатні взаємодіяти з користувачем природною мовою, пояснювати результати й формувати нові знання на основі отриманої інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.264763", "order": 85, "lang": "uk"}}
{"id": "1_docx#86", "doc_id": "1_docx", "text": "1.4. Алгоритми й моделі обробки природної мови (NLP)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.266603", "order": 86, "lang": "uk"}}
{"id": "1_docx#87", "doc_id": "1_docx", "text": "Обробка природної мови (Natural Language Processing, скорочено NLP) є однією з ключових галузей штучного інтелекту, що вивчає методи взаємодії комп’ютерів із людською мовою. Основна мета NLP полягає у тому, щоб навчити машини розуміти, інтерпретувати й генерувати тексти так, як це робить людина [1]. Протягом останніх десятиліть у цій сфері було створено велику кількість алгоритмів і моделей, які поступово вдосконалювали здатність систем аналізувати тексти — від простого підрахунку частоти слів до глибокого семантичного розуміння контексту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.271048", "order": 87, "lang": "uk"}}
{"id": "1_docx#88", "doc_id": "1_docx", "text": "Класичні статистичні моделі", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.272875", "order": 88, "lang": "uk"}}
{"id": "1_docx#89", "doc_id": "1_docx", "text": "На початковому етапі розвитку NLP основну роль відігравали статистичні методи, засновані на кількісному аналізі тексту. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього порядку [2]. Наприклад, фрази “кіт спить на дивані” та “на дивані спить кіт” будуть представлені однаково, оскільки містять ті самі слова. Попри простоту реалізації, цей метод ігнорує синтаксичні та семантичні зв’язки, що обмежує його застосування в задачах, де важливий контекст.\nДля підвищення точності аналізу було запропоновано метод TF-IDF (Term Frequency — Inverse Document Frequency) [3], який враховує не лише частоту слова в документі, а й його рідкість у колекції текстів. TF-IDF дозволяє\nнадавати більшої ваги унікальним термінам, які є змістовно значущими для конкретного документа. Цей метод став базовим у побудові пошукових систем, тематичній класифікації та фільтрації спаму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.277694", "order": 89, "lang": "uk"}}
{"id": "1_docx#90", "doc_id": "1_docx", "text": "Векторні моделі представлення текстів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.279499", "order": 90, "lang": "uk"}}
{"id": "1_docx#91", "doc_id": "1_docx", "text": "Подальшим етапом розвитку NLP стали векторні моделі, що дозволили описати слова у вигляді багатовимірних числових векторів. Такі моделі відображають семантичну близькість між словами — чим подібніші значення, тим менша відстань між їхніми векторами [4].\nОднією з перших таких моделей стала Word2Vec, розроблена командою Google Research [5]. Вона дозволяє вловлювати смислові зв’язки між словами: наприклад, векторна операція “король” – “чоловік” + “жінка” ≈ “королева” демонструє, що модель засвоює відношення між поняттями.\nІнша популярна модель — GloVe (Global Vectors for Word Representation), створена в Stanford NLP Group [6], поєднує статистичні принципи з нейронними мережами. Вона формує вектори на основі статистики спільних появ слів у тексті, що підвищує стабільність представлень.\nМодель FastText, розроблена компанією Meta (Facebook) [7], враховує морфологічну структуру слів, розбиваючи їх на підрядки (n-грамні вектори). Завдяки цьому система може розуміти нові або рідкісні слова, що особливо корисно для української мови, де багато словоформ.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.284756", "order": 91, "lang": "uk"}}
{"id": "1_docx#92", "doc_id": "1_docx", "text": "Таблиця 1.3 Порівняння класичних моделей представлення тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.288807", "order": 92, "lang": "uk"}}
{"id": "1_docx#93", "doc_id": "1_docx", "text": "Модель Принцип роботи Переваги Обмеження Bag of Words Підрахунок частоти слів у тексті Простота реалізації, ефективність для малих даних Ігнорує порядок і контекст TF-IDF Врахування частоти і рідкості слів Підвищує точність пошуку, придатний для класифікації Не враховує семантичні зв’язки Word2Vec Навчання векторів слів на великих корпусах Враховує семантику, компактне представлення Не розрізняє контексти одного слова GloVe Аналіз спільних появ слів у тексті Поєднує статистику й нейронний підхід Вимагає великих ресурсів FastText Представлення слова через підрядки Добре працює з новими словами Потребує тонкого налаштування параметрів", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.293148", "order": 93, "lang": "uk"}}
{"id": "1_docx#94", "doc_id": "1_docx", "text": "Контекстні моделі та архітектура трансформерів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.294683", "order": 94, "lang": "uk"}}
{"id": "1_docx#95", "doc_id": "1_docx", "text": "Обмеження класичних моделей полягали в тому, що кожне слово мало одне фіксоване представлення, незалежно від контексту. У реченнях “банк річки” та “кредитний банк” слово “банк” має різні значення, однак старі моделі цього не враховували. Революційним рішенням стало впровадження трансформерних моделей, які враховують контекст кожного слова в межах речення або документа [8].\nМодель BERT (Bidirectional Encoder Representations from Transformers), створена компанією Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT може розуміти, які слова в контексті впливають на значення інших, що забезпечує високу точність у задачах класифікації, пошуку відповідей і машинного перекладу. Модифікації RoBERTa, DistilBERT і ALBERT підвищили швидкість і ефективність обчислень без втрати якості [10].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.299567", "order": 95, "lang": "uk"}}
{"id": "1_docx#96", "doc_id": "1_docx", "text": "Іншу еволюційну гілку представляють генеративні трансформери, серед яких найвідомішою є GPT (Generative Pre-trained Transformer),", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.301176", "order": 96, "lang": "uk"}}
{"id": "1_docx#97", "doc_id": "1_docx", "text": "розроблена OpenAI [11]. GPT здатна не лише аналізувати текст, а й генерувати новий контент — логічно зв’язні речення, відповіді, коди чи описи. Її принцип полягає у передбаченні наступного слова за попереднім контекстом, що робить можливим природне спілкування людини з машиною.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.304685", "order": 97, "lang": "uk"}}
{"id": "1_docx#98", "doc_id": "1_docx", "text": "Сучасні моделі векторизації тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.306270", "order": 98, "lang": "uk"}}
{"id": "1_docx#99", "doc_id": "1_docx", "text": "Для практичних задач класифікації, кластеризації та семантичного пошуку активно застосовується модель SentenceTransformers, яка є модифікацією BERT і оптимізована для створення векторів речень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді.\nТакий принцип лежить в основі семантичного пошуку, де збігаються не слова, а змісти. Наприклад, запит “літак затримався” може знайти документ із фразою “рейс відкладено”, навіть без жодного спільного слова. Саме це робить сучасні пошукові системи, чат-боти та аналітичні інструменти ефективнішими, ніж будь-коли раніше [13].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.310395", "order": 99, "lang": "uk"}}
{"id": "1_docx#100", "doc_id": "1_docx", "text": "Еволюція моделей обробки природної мови пройшла шлях від простих статистичних методів до складних багаторівневих нейромережевих архітектур.\nКласичні моделі, як Bag of Words та TF-IDF, заклали математичну основу для подання текстів, тоді як Word2Vec, GloVe та FastText зробили перші кроки до розуміння семантики.\nНатомість сучасні трансформери (BERT, GPT, SentenceTransformers) дозволили досягти справжнього контекстного розуміння мови, наблизивши штучний інтелект до людського рівня сприйняття.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.314376", "order": 100, "lang": "uk"}}
{"id": "1_docx#101", "doc_id": "1_docx", "text": "У межах цієї дипломної роботи я, Оліфіренко Кирило, досліджую можливості поєднання таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до векторизації та семантичного пошуку.\nРис. 1.1. Схематичне представлення методу обробки неструктурованих текстових даних (UXText Pipeline)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.317461", "order": 101, "lang": "uk"}}
{"id": "1_docx#102", "doc_id": "1_docx", "text": "1.5. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.319777", "order": 102, "lang": "uk"}}
{"id": "1_docx#103", "doc_id": "1_docx", "text": "У розвитку інструментів для обробки природної мови простежується еволюція від класичних бібліотек, орієнтованих на базові лінгвістичні операції, до сучасних фреймворків, що працюють із великими мовними моделями (LLM) та векторними базами даних. Для систематизації екосистеми NLP-технологій доцільно виділити три основні групи:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.322646", "order": 103, "lang": "uk"}}
{"id": "1_docx#104", "doc_id": "1_docx", "text": "традиційні бібліотеки Python для обробки текстів,\nсучасні фреймворки та сервіси глибокого навчання,\nінтеграційні рішення для побудови аналітичних систем.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:04:37.325192", "order": 104, "lang": "uk"}}
{"id": "1_docx#105", "doc_id": "1_docx", "text": "Ця класифікація дозволяє простежити логічну послідовність розвитку підходів — від токенізації й морфологічного аналізу до комплексного семантичного пошуку та генеративного текстового інтелекту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.327434", "order": 105, "lang": "uk"}}
{"id": "1_docx#106", "doc_id": "1_docx", "text": "1.5.1. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.328971", "order": 106, "lang": "uk"}}
{"id": "1_docx#107", "doc_id": "1_docx", "text": "Першими інструментами, які забезпечили автоматизацію лінгвістичного аналізу, стали бібліотеки NLTK, spaCy та scikit-learn.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.330786", "order": 107, "lang": "uk"}}
{"id": "1_docx#108", "doc_id": "1_docx", "text": "NLTK (Natural Language Toolkit) — одна з найстаріших бібліотек для роботи з природною мовою у Python [1]. Вона містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко використовується в освіті та дослідженнях, оскільки дозволяє швидко створювати експериментальні моделі та аналізувати тексти. Проте через низьку швидкодію і складність масштабування її рідко застосовують у великих проєктах.\nspaCy стала відповіддю на потребу у високопродуктивних NLP-рішеннях. Вона орієнтована на промислове використання та забезпечує обробку текстів у десятки разів швидше, ніж NLTK [2]. SpaCy підтримує понад 60 мов, має модулі для виявлення іменованих сутностей (NER), синтаксичних залежностей і визначення частин мови. Завдяки інтеграції з TensorFlow і PyTorch, spaCy поєднує класичну обробку з глибоким навчанням.\nscikit-learn, хоча й не є суто NLP-бібліотекою, використовується для класифікації текстів, тематичного моделювання та аналізу тональності [3]. Бібліотека надає інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме sci", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.337674", "order": 108, "lang": "uk"}}
{"id": "1_docx#109", "doc_id": "1_docx", "text": "інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.340269", "order": 109, "lang": "uk"}}
{"id": "1_docx#110", "doc_id": "1_docx", "text": "Таблиця 1.4 Порівняння традиційних бібліотек Python для NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.341700", "order": 110, "lang": "uk"}}
{"id": "1_docx#111", "doc_id": "1_docx", "text": "Бібліотека Основні функції Переваги Недоліки та обмеження NLTK Токенізація, лематизація, морфологічний аналіз Простота, освітня цінність Повільна робота, не для продакшну spaCy POS-tagging, NER, синтаксичний аналіз Висока швидкодія, сучасні моделі Обмежена кількість мовних ресурсів scikit-learn Класифікація, TF-IDF, машинне навчання Гнучкість, модульність Не обробляє мову напряму, потребує препроцесингу", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.344585", "order": 111, "lang": "uk"}}
{"id": "1_docx#112", "doc_id": "1_docx", "text": "1.5.1. Сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.361431", "order": 112, "lang": "ro"}}
{"id": "1_docx#113", "doc_id": "1_docx", "text": "З появою глибоких нейронних мереж і трансформерних архітектур відбувся різкий стрибок у можливостях NLP.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.364265", "order": 113, "lang": "uk"}}
{"id": "1_docx#114", "doc_id": "1_docx", "text": "Фреймворк Transformers від Hugging Face став центральною платформою для роботи з великими мовними моделями [4]. Він надає доступ до сотень попередньо навчених моделей, серед яких BERT, GPT, T5, RoBERTa, DistilBERT, SentenceTransformers тощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних досліджень у NLP.\nІншим ключовим інструментом є OpenAI API, що надає доступ до моделей сімейства GPT (Generative Pre-trained Transformer) [5]. Ці моделі дозволяють не лише аналізувати текст, а й генерувати узагальнення, відповіді,", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.367430", "order": 114, "lang": "uk"}}
{"id": "1_docx#115", "doc_id": "1_docx", "text": "описи чи навіть програмний код, що відкриває новий рівень взаємодії людини та машини.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.369819", "order": 115, "lang": "uk"}}
{"id": "1_docx#116", "doc_id": "1_docx", "text": "Фреймворк LangChain вирішує проблему інтеграції мовних моделей із базами знань, надаючи можливість побудови логічних ланцюгів (chains) між LLM і зовнішніми джерелами [6]. Цей підхід використовується у системах Retrieval-Augmented Generation (RAG), де модель не лише генерує текст, а й отримує релевантну інформацію з баз даних чи документів.\nДля обробки документів різних форматів (PDF, DOCX, HTML, TXT) використовується бібліотека Unstructured.io, яка автоматично виділяє текстові елементи, розпізнає структуру документа і зберігає контекст [7]. Саме вона використовується в моєму проєкті UXText Pipeline для імпорту та нормалізації текстів перед векторизацією.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.373536", "order": 116, "lang": "uk"}}
{"id": "1_docx#117", "doc_id": "1_docx", "text": "Рис. 1.2. Архітектура екосистеми інструментів NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.375153", "order": 117, "lang": "uk"}}
{"id": "1_docx#118", "doc_id": "1_docx", "text": "1.5.3. Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.376574", "order": 118, "lang": "uk"}}
{"id": "1_docx#119", "doc_id": "1_docx", "text": "Завершальним етапом є зберігання, індексація та пошук текстових даних. Для цього застосовуються інтеграційні рішення, які поєднують властивості баз даних та пошукових систем.\nFAISS (Facebook AI Similarity Search) — бібліотека для швидкого пошуку найближчих сусідів у багатовимірних просторах [8]. Вона оптимізована для GPU та дозволяє працювати з мільйонами векторів, що робить її основою систем семантичного пошуку.\nChromaDB — векторна база даних нового покоління, яка інтегрується з мовними моделями через LangChain та інші API [9]. Вона використовується в системах типу RAG для пошуку інформації за змістом. ChromaDB проста у використанні та підтримує локальне й хмарне розгортання.\nElasticSearch — потужна пошукова платформа, яка поєднує індексацію текстів і машинне навчання [10]. Вона підтримує пошук як за ключовими словами, так і за векторними поданнями, завдяки чому часто використовується у корпоративних аналітичних системах.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.381023", "order": 119, "lang": "uk"}}
{"id": "1_docx#120", "doc_id": "1_docx", "text": "Таблиця 1.5 Порівняльна характеристика інтеграційних інструментів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.382467", "order": 120, "lang": "uk"}}
{"id": "1_docx#121", "doc_id": "1_docx", "text": "Інструмент Тип рішення Основні можливості Переваги FAISS Бібліотека векторного пошуку GPU-пошук, кластеризація, масштабованість Висока швидкість, підтримка великих обсягів ChromaDB Векторна база даних Семантичний пошук, інтеграція з LLM Простота, інтеграція з LangChain ElasticSearch Пошукова система Повнотекстовий та семантичний пошук Масштабованість, корпоративний рівень", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.386303", "order": 121, "lang": "uk"}}
{"id": "1_docx#122", "doc_id": "1_docx", "text": "Таким чином, сучасна екосистема обробки текстів складається з трьох взаємодоповнювальних рівнів:\nБазовий рівень — бібліотеки для лінгвістичного аналізу (NLTK, spaCy, scikit-learn).\nАналітичний рівень — фреймворки глибокого навчання (Transformers, LangChain, OpenAI API, Unstructured).\nІнфраструктурний рівень — рішення для зберігання, пошуку та індексації даних (FAISS, ChromaDB, ElasticSearch).\nПоєднання цих технологій створює основу для побудови інтелектуальних систем аналізу неструктурованих даних, здатних здійснювати пошук, класифікацію, узагальнення та інтерпретацію текстів із точністю, наближеною до людського розуміння.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.390112", "order": 122, "lang": "uk"}}
{"id": "1_docx#123", "doc_id": "1_docx", "text": "1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.394162", "order": 123, "lang": "uk"}}
{"id": "1_docx#124", "doc_id": "1_docx", "text": "Проблематика обробки неструктурованих даних залишається однією з найактуальніших тем сучасної комп’ютерної лінгвістики та аналітики даних. За оцінками International Data Corporation (IDC), понад 80 % усієї інформації у світі має неструктурований характер — це тексти, аудіо, відео, зображення, документи різних форматів [1]. Попри суттєвий прогрес у розвитку штучного інтелекту та глибоких нейронних мереж, реальні виклики пов’язані не лише з алгоритмами, а й з їх практичним застосуванням, масштабуванням і пояснюваністю результатів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.397323", "order": 124, "lang": "uk"}}
{"id": "1_docx#125", "doc_id": "1_docx", "text": "Під час розроблення власної дипломної системи UXText Pipeline я, Оліфіренко Кирило, переконався, що найбільші труднощі виникають саме на\nстику технічних, мовних і організаційних аспектів — там, де алгоритми мають взаємодіяти з реальними, часто «брудними» даними користувачів.\nНеоднорідність форматів і структур даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.399870", "order": 125, "lang": "uk"}}
{"id": "1_docx#126", "doc_id": "1_docx", "text": "Однією з найсерйозніших проблем є неоднорідність форматів джерел інформації. Тексти можуть бути представлені у вигляді PDF, DOCX, HTML, TXT, JSON, електронних листів або навіть відсканованих зображень. У кожного з форматів — своя структура, кодування, набір метаданих і правила відображення.\nНаприклад, при вилученні тексту з PDF часто трапляється втрата логічної послідовності слів або ігнорування таблиць, що ускладнює подальшу обробку [2].\nДля уніфікації таких даних застосовуються спеціальні парсери, зокрема бібліотека Unstructured.io, яка розділяє документ на структурні елементи — абзаци, списки, таблиці. Проте навіть вона не гарантує ідеального результату для зображень чи рукописних нотаток.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.404671", "order": 126, "lang": "uk"}}
{"id": "1_docx#127", "doc_id": "1_docx", "text": "Таким чином, перший виклик — створення універсальних конвертерів, здатних коректно вилучати текст незалежно від формату вхідного документа.\nРесурсоємність і масштабування NLP-моделей", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.407371", "order": 127, "lang": "uk"}}
{"id": "1_docx#128", "doc_id": "1_docx", "text": "Сучасні моделі, такі як BERT, GPT чи T5, вимагають надзвичайно великих обчислювальних ресурсів. Для їхнього навчання потрібні потужні графічні процесори (GPU), великі обсяги оперативної пам’яті та тривалі обчислення [3]. Навіть часткове донавчання моделей для конкретного домену може тривати годинами чи днями.\nУ корпоративних і навчальних умовах це створює фінансові та енергетичні обмеження. Саме тому розвивається напрям ефективних моделей\n(efficient AI) — таких, як DistilBERT, TinyLlama чи ALBERT, які споживають менше ресурсів при збереженні високої точності [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.411028", "order": 128, "lang": "uk"}}
{"id": "1_docx#129", "doc_id": "1_docx", "text": "Проблема пояснюваності (Explainability)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.415795", "order": 129, "lang": "uk"}}
{"id": "1_docx#130", "doc_id": "1_docx", "text": "Ще один суттєвий виклик — інтерпретація результатів моделей. Класичні методи (TF-IDF, SVM) дають змогу зрозуміти, які саме ознаки вплинули на результат. Натомість нейронні мережі — це «чорні скриньки», логіка роботи яких не завжди зрозуміла [5].\nУ наукових або юридичних системах це неприйнятно, адже користувач має знати, чому модель зробила певний висновок. Саме тому активно розвивається напрям Explainable AI (XAI), що поєднує точність глибинного навчання з інтерпретованістю статистичних методів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.420211", "order": 130, "lang": "uk"}}
{"id": "1_docx#131", "doc_id": "1_docx", "text": "Прикладом можуть бути візуалізації важливості токенів у реченні чи методи LIME і SHAP, які показують, які слова вплинули на класифікацію.\nМовна неоднорідність і підтримка української мови", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.422220", "order": 131, "lang": "uk"}}
{"id": "1_docx#132", "doc_id": "1_docx", "text": "Суттєвим бар’єром залишається багатомовність і різна якість підтримки мов у бібліотеках. Для англійської мови існують великі корпуси, тоді як для української — вони обмежені. Наприклад, spaCy має часткову підтримку української морфології, але без повноцінного синтаксичного аналізу [6].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.424385", "order": 132, "lang": "uk"}}
{"id": "1_docx#133", "doc_id": "1_docx", "text": "Це ускладнює автоматизацію аналітики в українських організаціях і підкреслює потребу у національних корпусах даних та відкритих моделях, адаптованих до українського контексту.\nІнтеграція NLP із реальними ІТ-системами", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.426301", "order": 133, "lang": "uk"}}
{"id": "1_docx#134", "doc_id": "1_docx", "text": "Нерідко навіть високоточні моделі залишаються ізольованими — їх важко впровадити у реальні бізнес-процеси. CRM-платформи, ERP-системи чи\nдокументообіги використовують різні технологічні стеки (SQL, NoSQL, REST, GraphQL), тому інтеграція NLP-алгоритмів вимагає розробки універсальних API та конекторів [7].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.428956", "order": 134, "lang": "uk"}}
{"id": "1_docx#135", "doc_id": "1_docx", "text": "Це стало передумовою появи таких фреймворків, як LangChain та LlamaIndex, що поєднують мовні моделі з базами даних і зовнішніми джерелами знань.\nЯкість текстових даних і семантична неоднозначність\nЖодна модель не працює добре без якісних вхідних даних.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.431413", "order": 135, "lang": "uk"}}
{"id": "1_docx#136", "doc_id": "1_docx", "text": "Помилки, жаргон, скорочення, неоднозначність (наприклад, слово «банк» — фінансова установа чи берег річки) значно впливають на результати [8]. Для їх усунення використовують етапи очищення, нормалізації, лематизації та векторизації, які дозволяють зменшити шум у даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.434243", "order": 136, "lang": "uk"}}
{"id": "1_docx#137", "doc_id": "1_docx", "text": "Саме на цьому етапі реалізується найбільша частина інтелектуальної роботи у системах обробки неструктурованих текстів.\nБезпека та конфіденційність даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.436925", "order": 137, "lang": "uk"}}
{"id": "1_docx#138", "doc_id": "1_docx", "text": "Оскільки більшість систем NLP працюють з чутливою інформацією, актуальним є питання захисту персональних даних і локальної обробки. Хмарні сервіси (як-от OpenAI API чи Claude AI) не завжди відповідають вимогам корпоративної або державної безпеки.\nТому зростає попит на офлайн-моделі та приватні рішення, що розгортаються у внутрішніх мережах підприємств без передачі даних третім сторонам [9].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.439856", "order": 138, "lang": "uk"}}
{"id": "1_docx#139", "doc_id": "1_docx", "text": "Напрями вдосконалення\nДля подолання наведених проблем сформувалися кілька ключових напрямів розвитку методів обробки неструктурованих даних:\nМультимодальні моделі — поєднання тексту, зображень, аудіо та відео (наприклад, CLIP, Gemini, Kosmos-1), що дозволяє системам аналізувати інформацію комплексно.\nОптимізація ресурсів — створення полегшених архітектур (DistilBERT, TinyLlama, Mistral 7B), які зберігають продуктивність, але працюють на споживацькому обладнанні.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.442800", "order": 139, "lang": "uk"}}
{"id": "1_docx#140", "doc_id": "1_docx", "text": "Пояснюваність моделей (XAI) — розробка інтерпретованих методів аналізу, що дають змогу пояснювати результати користувачам.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.444816", "order": 140, "lang": "uk"}}
{"id": "1_docx#141", "doc_id": "1_docx", "text": "Покращення якості даних — автоматичне виявлення шуму, нормалізація, переклад, фільтрація та сегментація контенту.\nСтандартизація інтеграцій — розробка відкритих API та протоколів взаємодії для універсальної роботи NLP-модулів у різних системах.\nТаблиця 1.6 Основні проблеми та напрями їх вирішення", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.447111", "order": 141, "lang": "uk"}}
{"id": "1_docx#142", "doc_id": "1_docx", "text": "Проблема Причина Можливі напрями вдосконалення Неоднорідність форматів Різні структури документів (PDF, HTML, DOCX) Використання універсальних парсерів (Unstructured.io) Висока ресурсоємність Складні моделі з мільйонами параметрів Використання полегшених архітектур (DistilBERT, TinyLlama) Непояснюваність результатів «Чорна скринька» нейронних мереж Методи XAI, візуалізація уваги, SHAP-аналіз Багатомовність Обмежені корпуси української мови Створення відкритих національних корпусів Складність інтеграції Несумісні технологічні стеки Використання LangChain, REST API, FAISS Якість тексту Наявність шуму, помилок Очищення, нормалізація, векторизація Безпека даних Обробка чутливої інформації Локальні (on-premise) рішення, приватні LLM", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:04:37.450795", "order": 142, "lang": "uk"}}
{"id": "1_docx#143", "doc_id": "1_docx", "text": "Рис. 1.3. Узагальнена схема проблем і шляхів удосконалення методів обробки неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.454524", "order": 143, "lang": "uk"}}
{"id": "1_docx#144", "doc_id": "1_docx", "text": "1.7. Висновки до розділу 1", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:04:37.456724", "order": 144, "lang": "uk"}}
{"id": "1_docx#145", "doc_id": "1_docx", "text": "У першому розділі було проведено всебічний теоретичний аналіз проблеми обробки неструктурованих даних, окреслено сучасні тенденції та виклики, а також визначено методологічні основи, на яких базується подальша частина дипломного дослідження. Аналіз показав, що обсяг неструктурованої інформації у світі стрімко зростає, і більшість даних, які сьогодні генеруються людиною чи машинами, мають саме неструктурований характер — це тексти,\nзображення, відео, аудіо, повідомлення у соціальних мережах, електронна пошта, технічна документація тощо. Такі дані не мають чіткої схеми зберігання і потребують спеціальних методів для вилучення, структуризації та аналізу.\nБуло встановлено, що традиційні методи обробки текстової інформації, зокрема статистичні підходи на кшталт Bag of Words або TF-IDF, мають обмежену здатність до розуміння контексту та семантики. Їх перевагою є простота реалізації та зрозумілість результатів, однак у сучасних умовах, коли мова йде про глибоке семантичне розпізнавання, ці підходи стають недостатніми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у ко", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.462521", "order": 145, "lang": "uk"}}
{"id": "1_docx#146", "doc_id": "1_docx", "text": "зі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у контексті, аналізувати зв’язки між фразами та реченнями, а також формувати осмислені векторні представлення тексту. Ці технології створили основу для побудови систем нового покоління — інтелектуальних пошукових механізмів, аналітичних платформ і мовних асистентів.\nУ ході огляду інструментів було виокремлено три рівні екосистеми для роботи з текстовими даними. Перший рівень становлять традиційні бібліотеки Python (NLTK, spaCy, scikit-learn), які забезпечують базову лінгвістичну обробку та підготовку текстів. Другий рівень охоплює сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured), що дозволяють реалізовувати глибокий семантичний аналіз, генерацію текстів і роботу з великими мовними моделями. Третій рівень формують інтеграційні рішення (FAISS, ChromaDB, ElasticSearch), які забезпечують збереження, векторизацію, пошук і зв’язок між компонентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки неструктурованих даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.467954", "order": 146, "lang": "uk"}}
{"id": "1_docx#147", "doc_id": "1_docx", "text": "Разом із тим, проведений аналіз виявив низку ключових проблем, які залишаються невирішеними у сфері NLP та аналітики текстів. Серед них —", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:04:37.470683", "order": 147, "lang": "uk"}}
{"id": "1_docx#148", "doc_id": "1_docx", "text": "неоднорідність форматів даних, висока ресурсомісткість сучасних моделей, складність інтерпретації результатів, брак універсальних методів для багатомовного аналізу, а також питання інтеграції таких систем у реальні бізнес-процеси. Водночас визначено основні напрями вдосконалення існуючих методів: розвиток мультимодальних моделей, підвищення ефективності обчислень, створення пояснюваних моделей (Explainable AI) та вдосконалення інструментів попередньої обробки тексту.\nПідсумовуючи результати теоретичної частини, можна зробити висновок, що побудова ефективних систем аналізу неструктурованих даних потребує поєднання класичних методів статистичної обробки з сучасними підходами глибокого навчання та трансформерних моделей. Це дозволяє досягнути балансу між точністю, швидкодією та інтерпретованістю результатів.\nСаме тому у подальших розділах дипломної роботи буде розглянуто розробку універсального методу обробки неструктурованих текстових даних, який інтегрує сучасні бібліотеки (Unstructured, Transformers, SentenceTransformers, FAISS) у єдиний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів ін", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.476591", "order": 148, "lang": "uk"}}
{"id": "1_docx#149", "doc_id": "1_docx", "text": "ний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів інформації та створення інструменту, придатного для практичного використання у системах аналітики, досліджень і бізнесу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:04:37.478704", "order": 149, "lang": "uk"}}
{"id": "1_docx#1", "doc_id": "1_docx", "text": "Державний торговельно-економічний університет\nКафедра комп’ютерних наук та інформаційних систем\nКВАЛІФІКАЦІЙНА РОБОТА\nна тему:\n«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.892144", "order": 1, "lang": "uk"}}
{"id": "1_docx#2", "doc_id": "1_docx", "text": "Студента 2 курсу, 6м групи спеціальності 122 «Комп’ютерні науки» __________ підпис студента Оліфіренко Кирило Андрійович Науковий керівник кандидат фізико-математичних наук, доцент ___________ підпис керівника Томашевська Тетяна Володимирівна Гарант освітньої програми доктор фізико-математичних наук, професор ___________ підпис керівника Пурський Олег Іванович", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:16.894812", "order": 2, "lang": "uk"}}
{"id": "1_docx#3", "doc_id": "1_docx", "text": "Київ 2025\nДержавний торговельно-економічний університет\nФакультет інформаційних технологій\nКафедра комп’ютерних наук та інформаційних систем\nСпеціальність 122 «Комп’ютерні науки»\nОсвітня програма «Комп’ютерні науки»\nЗатверджую\nЗав. кафедри ____________Пурський О.І.\n«» грудня 2025р.\nЗавдання\nна кваліфікаційну роботу студенту\nОліфіренко Кирилу Андрійовичу\n(прізвище, ім’я, по батькові)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.897639", "order": 3, "lang": "uk"}}
{"id": "1_docx#4", "doc_id": "1_docx", "text": "Тема кваліфікаційної роботи", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:16.898964", "order": 4, "lang": "uk"}}
{"id": "1_docx#5", "doc_id": "1_docx", "text": "«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»\nЗатверджена наказом ректора від «» листопада 2025 р. № 4142", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.904885", "order": 5, "lang": "uk"}}
{"id": "1_docx#6", "doc_id": "1_docx", "text": "2. Строк здачі студентом закінченої роботи 15 листопада 2025 року", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.906906", "order": 6, "lang": "uk"}}
{"id": "1_docx#7", "doc_id": "1_docx", "text": "3. Цільова установка та вихідні дані до роботи\nМета роботи: розробка моделі та інформаційної технології оцінки конкурентоспроможності підприємств електронної торгівлі.\nОб’єкт дослідження: автоматизація процесів оцінювання конкурентоспроможності підприємств електронної торгівлі.\nПредмет дослідження: моделі, методи та інформаційні технології оцінювання конкурентоспроможності.\n4. Перелік графічного матеріалу______________________________________ ______________________________________________________________________________________________________________________________________________________________________________________________________\n5. Консультанти по роботі із зазначенням розділів, за якими здійснюється консультування:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.910237", "order": 7, "lang": "uk"}}
{"id": "1_docx#8", "doc_id": "1_docx", "text": "Розділ Консультант (прізвище, ініціали) Підпис, дата Завдання видав Завдання прийняв 1 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 2 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 3 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р.", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:16.913047", "order": 8, "lang": "uk"}}
{"id": "1_docx#9", "doc_id": "1_docx", "text": "6. Зміст кваліфікаційної роботи (перелік питань за кожним розділом)\nВСТУП\nРОЗДІЛ 1. ТЕОРЕТИЧНІ АСПЕКТИ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВА\n1.1. Аналіз проблематики та існуючих методів управління конкурентоспроможністю\n1.2. Особливості оцінки та управління конкурентоспроможністю підприємств електронної комерції\n1.3. Концептуальна модель оцінки та управління конкурентоспроможністю підприємств електронної комерції\nРОЗДІЛ 2. МАТЕМАТИЧНІ МОДЕЛІ ОЦІНКИ ТА УПРАВЛІННЯ КОНКУРЕНТОСПРОМОЖНІСТЮ ПІДПРИЄМСТВА\n2.1. Система показників та модель оцінки конкурентоспроможності підприємств електронної комерції\n2.2. Модель управління конкурентоспроможністю підприємства\n2.3. Моделювання процесу оцінки конкурентоспроможності підприємства\nРОЗДІЛ 3. ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВ ЕЛЕКТРОННОЇ ТОРГІВЛІ\n3.1. Інформаційно-логічна модель системи оцінки конкурентоспроможності підприємств\n3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств\n3.3.Технологія використання інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.919294", "order": 9, "lang": "uk"}}
{"id": "1_docx#10", "doc_id": "1_docx", "text": "інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі\nВИСНОВКИ\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання роботи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.921274", "order": 10, "lang": "uk"}}
{"id": "1_docx#11", "doc_id": "1_docx", "text": "№ Пор. Назва етапів кваліфікаційної роботи Строк виконання етапів роботи За планом фактично 1 2 3 4 1 Вибір теми кваліфікаційної роботи 01.11.2023 01.11.2023 2 Розробка та затвердження завдання на кваліфікаційну роботу 05.12.2023 05.12.2023 3 Вступ 01.05.2024 01.05.2024 4 РОЗДІЛ 1. Теоретичні аспекти оцінки конкурентоспроможності підприємства 14.06.2024 14.06.2024 5 Підготовка статті у збірник наукових статей магістрів 20.06.2024 20.06.2024 6 РОЗДІЛ 2. Математичні моделі оцінки та управління конкурентоспроможністю підприємства 05.09.2024 05.09.2024 7 РОЗДІЛ 3. Інформаційна технологія оцінки конкурентоспроможності підприємств електронної торгівлі 17.10.2024 17.10.2024 8 Висновки 21.10.2024 21.10.2024 9 Здача кваліфікаційної роботи на кафедру науковому керівнику 23.10.2024 23.10.2024 10 Попередній захист кваліфікаційної роботи 28.10.2024 28.10.2024 11 Виправлення зауважень, зовнішнє рецензування кваліфікаційної роботи 30.10.2024 30.10.2024 12 Представлення готової зшитої кваліфікаційної роботи на кафедру 04.11.2024 04.11.2024 13 Публічний захист кваліфікаційної роботи За розкладом роботи ЕК", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:16.925837", "order": 11, "lang": "uk"}}
{"id": "1_docx#12", "doc_id": "1_docx", "text": "8. Дата видачі завдання «5» грудня 2023 р", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.927331", "order": 12, "lang": "uk"}}
{"id": "1_docx#13", "doc_id": "1_docx", "text": "9. Керівник випускного кваліфікаційної роботи Самойленко Г.Т\n(прізвище, ініціали, підпис)\n10. Гарант освітньої програми Пурський О.І.\n(прізвище, ініціали, підпис)\n11. Завдання прийняв до виконання студент Оліфіренко К. ________\n(прізвище, ініціали, підпис)\n12. Відгук керівника кваліфікаційної роботи\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n________________________________________________________________\n______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.930490", "order": 13, "lang": "uk"}}
{"id": "1_docx#14", "doc_id": "1_docx", "text": "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\nКерівник кваліфікаційної роботи\n______________________\n(підпис, дата)\n13. Висновок про кваліфікаційну роботу\nКваліфікаційна робота студента _______ Оліфіренко К.А.\n(прізвище, ініціали)\nможе бути допущена до захисту в екзаменаційній комісії.\nГарант освітньої програми Пурський О.І.\n(підпис, прізвище, ініціали)\nЗавідувач кафедри Пурський О.І.\n(підпис, прізвище, ініціали)\n«_____»_________________2025 р.\nАнотація\nУ кваліфікаційній роботі здійснено комплексну розробку моделей та інформаційної технології моніторингу соціально-економічних показників з", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.934603", "order": 14, "lang": "uk"}}
{"id": "1_docx#15", "doc_id": "1_docx", "text": "метою підвищення ефективності управління регіональним розвитком. Теоретично обґрунтовано основні положення формування і проведення соціально-економічного моніторингу та запропоновано концепцію створення інформаційної системи оціньовання показників соціально-економічного розвитку регіону. Розроблено метод автоматизованого розрахунку комплексної оцінки показників соціально-економічного розвитку. Створено автоматизовану Web-систему оціньовання показників соціально-економічного розвитку регіонів України.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.938236", "order": 15, "lang": "uk"}}
{"id": "1_docx#16", "doc_id": "1_docx", "text": "Ключові слова: соціально-економічний розвиток, математична модель, інтегральні показники, інформаційна технологія.\nAnotation", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.940116", "order": 16, "lang": "uk"}}
{"id": "1_docx#17", "doc_id": "1_docx", "text": "The qualification work is devoted to development of model and information technology of monitoring of social and economic indexes for the purpose of management efficiency increase by regional development The mechanism of the taking into account of differentiation of regional development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of social and economic development is offered and programmed. The Web-system for monitoring indicators of social and economic development of Ukraine regions is created and the technology of its use is developed.\nKeywords: social and economic development, mathematical model, integrated indicators, information technology.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.943904", "order": 17, "lang": "en"}}
{"id": "1_docx#18", "doc_id": "1_docx", "text": "ЗМІСТ\nВСТУП…………………………………………………..…………………………9\nРОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ…………….……..………….12\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних……………………………………………………..……………….….12", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.948031", "order": 18, "lang": "uk"}}
{"id": "1_docx#19", "doc_id": "1_docx", "text": "1.2 Поняття неструктурованих даних, їх роль у сучасних інформаційних системах………………………………………………...………….………..16", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.950342", "order": 19, "lang": "uk"}}
{"id": "1_docx#20", "doc_id": "1_docx", "text": "1.3 Методи та підходи до обробки текстових даних у системах штучного інтелекту ………………………………………………..…………………...20", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.953573", "order": 20, "lang": "uk"}}
{"id": "1_docx#21", "doc_id": "1_docx", "text": "1.4 Алгоритми й моделі обробки природної мови (NLP).……...………...20\n1.5 Огляд інструментів і бібліотек для роботи з текстовими даними………………………………………………...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.955634", "order": 21, "lang": "uk"}}
{"id": "1_docx#22", "doc_id": "1_docx", "text": "1.5.1 Традиційні бібліотеки Python для NLP (NLTK, spaCy, scikit-learn)……………………..…..……………...…………………………….....23", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.957999", "order": 22, "lang": "uk"}}
{"id": "1_docx#23", "doc_id": "1_docx", "text": "1.5.2 Сучасні фреймворки, сервіси (Transformers, OpenAI API, LangChain, Unstructured)……………………..…..…………………….…...23\n1.5.3 Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)….……………………………………...23\n1.6 Проблематика та напрями вдосконалення методів обробки неструктурованих даних………………………...…...……………………..20", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.960526", "order": 23, "lang": "uk"}}
{"id": "1_docx#24", "doc_id": "1_docx", "text": "1.7 Висновки до розділу 1……………………………..…………………...20\nРОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………………………………………………….…..26\n2.1 Постановка задачі, вимоги та функціональні можливості системи…26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.962667", "order": 24, "lang": "uk"}}
{"id": "1_docx#25", "doc_id": "1_docx", "text": "2.2 Архітектура методу обробки неструктурованих текстових даних…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.968317", "order": 25, "lang": "uk"}}
{"id": "1_docx#26", "doc_id": "1_docx", "text": "2.3. Розробка алгоритму розбиття документів на семантичні фрагменти (чанки)………………………………………………………………….……33\n2.4 Методи векторизації тексту та побудова семантичного індексу…...…………………….………………………………….…………26", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.971056", "order": 26, "lang": "uk"}}
{"id": "1_docx#27", "doc_id": "1_docx", "text": "2.5 Реалізація бази знань та пошукового механізму ……..…………..…..30", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.972390", "order": 27, "lang": "uk"}}
{"id": "1_docx#28", "doc_id": "1_docx", "text": "2.6 Висновки до розділу 2…………………………...……..…………..…..30\nРОЗДІЛ 3. ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ІНТЕРФЕЙС СИСТЕМИ………………………………………………………….....…………36\n3.1 Загальна структура програмного комплексу UXText Pipeline.………36", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.979195", "order": 28, "lang": "uk"}}
{"id": "1_docx#29", "doc_id": "1_docx", "text": "3.2 Інтеграція з бібліотекою Unstructured.io та FAISS ………….………..38\n3.3 Реалізація API-сервісу (FastAPI) для обробки запитів.………....……42\n3.4 Розробка користувацького інтерфейсу (Streamlit)…….…….………..38", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.980836", "order": 29, "lang": "uk"}}
{"id": "1_docx#30", "doc_id": "1_docx", "text": "3.5 Візуалізація результатів аналізу текстів .………...………...…………42\n3.6 Тестування та оцінювання ефективності роботи системи.……..……42\n3.7 Висновки до розділу 3.………...………………………….....…………42\nВИСНОВКИ……………………………..………………………………………48\nСПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ……………...……………………..50\nДОДАТОК………………………………………………..……………..………52\nВСТУП", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.983863", "order": 30, "lang": "uk"}}
{"id": "1_docx#31", "doc_id": "1_docx", "text": "У сучасному світі, коли інформаційні потоки стрімко зростають, значна частина даних, які створює людина чи система, має неструктурований характер. Це тексти документів, електронні листи, новини, пости у соціальних мережах, коментарі, відгуки, звіти чи наукові статті. За статистичними оцінками, понад 80 відсотків усієї цифрової інформації становлять саме неструктуровані дані, які не мають чіткої схеми чи фіксованої структури зберігання. Проте саме вони містять найціннішу інформацію для аналізу змісту, виявлення тенденцій, автоматизованого прийняття рішень чи побудови систем штучного інтелекту. Проблема полягає в тому, що традиційні інструменти обробки даних не пристосовані для ефективної роботи з такими джерелами. Методи на кшталт статистичного аналізу або пошуку за ключовими словами не здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’язки між ними.\nАктуальність теми зумовлена необхідністю створення ефективних методів, що дозволяють обробляти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штуч", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.990538", "order": 31, "lang": "uk"}}
{"id": "1_docx#32", "doc_id": "1_docx", "text": "яти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штучного інтелекту, зокрема векторизації текстів і нейромережевих моделей природної мови (NLP), відкриває нові можливості для створення інтелектуальних систем аналізу тексту. Ці системи здатні не лише класифікувати чи шукати інформацію, а й знаходити семантичні зв’язки між фрагментами тексту, узагальнювати їх зміст і будувати рекомендації. Саме тому тема «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» є актуальною і відповідає сучасним тенденціям розвитку інформаційних технологій.\nМетою даної роботи є розробка методу та програмного забезпечення, що забезпечують ефективну обробку неструктурованих текстових даних шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завдання: провести аналіз існуючих методів обробки текстових даних і", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:16.995785", "order": 32, "lang": "uk"}}
{"id": "1_docx#33", "doc_id": "1_docx", "text": "сучасних підходів до представлення тексту; дослідити технології побудови векторних моделей, зокрема SentenceTransformers та FAISS; спроєктувати архітектуру програмного комплексу для інтеграції різних джерел даних; реалізувати систему UXText Pipeline, яка поєднує модулі обробки, зберігання та пошуку текстів; провести експериментальне тестування для оцінки ефективності запропонованого методу.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:16.999027", "order": 33, "lang": "uk"}}
{"id": "1_docx#34", "doc_id": "1_docx", "text": "Об’єктом дослідження є процес обробки неструктурованих текстових даних у системах аналітики. Предметом дослідження — методи й алгоритми нормалізації, векторизації та індексації текстів, що дозволяють підвищити якість пошуку, класифікації й аналізу текстової інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.002631", "order": 34, "lang": "uk"}}
{"id": "1_docx#35", "doc_id": "1_docx", "text": "Під час виконання роботи застосовувалися методи аналітичного огляду та синтезу для вивчення літератури й порівняння існуючих підходів; алгоритмічного проєктування — для створення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також статистичні методи оцінювання результатів, що дозволили кількісно виміряти покращення якості обробки текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.005777", "order": 35, "lang": "uk"}}
{"id": "1_docx#36", "doc_id": "1_docx", "text": "Наукова новизна роботи полягає у створенні універсального підходу до обробки неструктурованих даних, який поєднує модулі виділення тексту, нормалізації, семантичного подання та побудови індекса для швидкого пошуку. Запропонований метод дозволяє працювати з різними форматами документів (PDF, DOCX, HTML), об’єднуючи їх у єдину структуру даних без втрати змістової інформації. Крім того, у роботі реалізовано алгоритм побудови семантичного індекса з використанням SentenceTransformers і FAISS, що підвищує ефективність пошуку за змістом, а не лише за словами.\nПрактичне значення роботи полягає в тому, що розроблений програмний комплекс UXText Pipeline може бути використаний як основа для створення аналітичних систем у різних сферах — від бізнес-аналітики та освіти до наукових досліджень і документообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і\nпідвищує точність інформаційного пошуку. Розроблені рішення можуть бути впроваджені в установах, де необхідна швидка обробка текстових звітів, статей або відгуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.011573", "order": 36, "lang": "uk"}}
{"id": "1_docx#37", "doc_id": "1_docx", "text": "гуків.\nРезультати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» (2024 р.) та при реалізації програмного прототипу, опублікованого у відкритому доступі на GitHub. Під час експериментів підтверджено ефективність запропонованого методу — час обробки документів скоротився, а точність семантичного пошуку зросла порівняно з базовими алгоритмами.\nДипломна робота складається зі вступу, чотирьох розділів, висновків, списку використаних джерел і додатків. Загальний обсяг становить 70 сторінок, включає 25 рисунків, 12 таблиць і 3 додатки. У роботі послідовно викладено теоретичні основи, аналіз існуючих рішень, власну методику обробки текстів, результати експериментів і приклади практичного використання створеного програмного комплексу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.016846", "order": 37, "lang": "uk"}}
{"id": "1_docx#38", "doc_id": "1_docx", "text": "РОЗДІЛ 1.\nАНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ\n1.1. Постановка проблеми та актуальність обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.021754", "order": 38, "lang": "uk"}}
{"id": "1_docx#39", "doc_id": "1_docx", "text": "У сучасному цифровому світі обсяг інформації, який щоденно створюється, зростає з небаченою швидкістю. За оцінками аналітичної компанії IDC (International Data Corporation), до 2025 року загальний обсяг глобальних даних перевищить 175 зетабайт, причому понад 80 відсотків із них становитимуть саме неструктуровані дані — тексти, зображення, відео, аудіозаписи, повідомлення в соціальних мережах тощо [1]. Ці дані генеруються користувачами, організаціями, сенсорами та системами штучного інтелекту. Проте, попри їхню кількість і потенційну цінність, лише невелика частина реально використовується для аналітики через відсутність ефективних методів автоматичної обробки.\nПроблема полягає у тому, що більшість традиційних інформаційних систем побудовані для роботи зі структурованими даними — таблицями, числовими показниками, логами, базами SQL. Такі системи ефективно працюють лише тоді, коли дані мають чітко визначену схему зберігання: є поля, типи, формат і зв’язки між ними. Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані з", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.027627", "order": 39, "lang": "uk"}}
{"id": "1_docx#40", "doc_id": "1_docx", "text": "тувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані зображення. Як наслідок, традиційні системи управління даними (DBMS) не здатні безпосередньо інтерпретувати зміст таких файлів, а тому значна частина потенційно корисної інформації залишається поза увагою аналітичних алгоритмів [2].\nЯ, Оліфіренко Кирило, як студент спеціальності «Комп’ютерні науки», у процесі дослідження дійшов висновку, що проблема обробки неструктурованих даних є не лише технічною, а й методологічною. Навіть у сучасних інформаційних середовищах — як у бізнесі, так і в державних установах — зберігаються великі масиви документів у різних форматах: звіти у PDF, службові документи у DOCX, переписки у форматі електронної пошти, повідомлення з месенджерів, а також текстові поля у CRM-системах. Всі ці", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.032389", "order": 40, "lang": "uk"}}
{"id": "1_docx#41", "doc_id": "1_docx", "text": "джерела містять важливу інформацію, яку складно уніфікувати без єдиної системи автоматичної обробки.\nАктуальність цієї проблеми підтверджується низкою об’єктивних чинників:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.035615", "order": 41, "lang": "uk"}}
{"id": "1_docx#42", "doc_id": "1_docx", "text": "Стрімке зростання обсягів текстових даних.\nЩосекунди в інтернеті з’являються мільйони повідомлень — лише у Twitter щодня публікується понад 500 млн твітів, у Facebook — понад 300 млн постів, а також понад 4,5 млрд електронних листів у глобальному масштабі [3]. Така кількість текстів робить ручну обробку неможливою, що потребує створення інтелектуальних алгоритмів аналізу.\nНеоднорідність джерел та форматів.\nТексти можуть бути представлені в різних форматах (PDF, DOCX, HTML, TXT, Markdown), з різною структурою, мовою, стилем і контекстом. У багатьох випадках інформація поєднує текст і візуальні елементи (таблиці, формули, підписи до зображень), що створює додаткові труднощі при обробці.\nЗростання ролі неструктурованих даних у прийнятті рішень.\nУ багатьох сферах — бізнесі, медицині, освіті, науці, державному управлінні — від якісного аналізу текстової інформації залежить точність прогнозів і ефективність управлінських дій.\nУ бізнесі — аналіз відгуків клієнтів дозволяє виявляти рівень задоволеності.\nУ медицині — аналіз медичних карт допомагає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального ", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.041300", "order": 42, "lang": "uk"}}
{"id": "1_docx#43", "doc_id": "1_docx", "text": "агає визначати закономірності захворювань.\nУ науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.\nВідсутність універсального методу обробки.\nІснуючі рішення часто обмежені конкретними форматами або мовами, а результати залежать від якості текстів, наявності помилок, скорочень,\nзмішаних мов. Тому актуальним є завдання створення гнучкої системи, здатної об’єднувати різні етапи обробки — від вилучення тексту до його семантичного аналізу.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.044290", "order": 43, "lang": "uk"}}
{"id": "1_docx#44", "doc_id": "1_docx", "text": "Ключовою метою сучасних досліджень у цій галузі є розробка методів, які поєднують традиційні алгоритми лінгвістичного аналізу з сучасними моделями машинного навчання та глибокого навчання (Deep Learning). Ці методи включають такі етапи, як:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.046255", "order": 44, "lang": "uk"}}
{"id": "1_docx#45", "doc_id": "1_docx", "text": "вилучення тексту з документів різних форматів;\nочищення від шуму (HTML-тегів, непотрібних символів);\nтокенізація й лематизація;\nвизначення мови;\nпобудова векторних подань (векторизація);\nіндексація та семантичний пошук.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.048321", "order": 45, "lang": "uk"}}
{"id": "1_docx#46", "doc_id": "1_docx", "text": "Зокрема, у моїй роботі розглядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семантичного пошуку за змістом). Такий підхід дозволяє створити універсальний пайплайн UXText Pipeline, який автоматизує процес перетворення неструктурованих даних у придатну для аналізу форму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.052048", "order": 46, "lang": "uk"}}
{"id": "1_docx#47", "doc_id": "1_docx", "text": "Разом із тим, залишається низка викликів, які обумовлюють актуальність подальших досліджень:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.053754", "order": 47, "lang": "uk"}}
{"id": "1_docx#48", "doc_id": "1_docx", "text": "неоднорідність форматів і різна якість текстових даних;\nпотреба у великих обчислювальних ресурсах для сучасних моделей (BERT, GPT);\nвідсутність єдиних стандартів інтеграції NLP-моделей у корпоративні системи;\nскладність пояснення результатів глибоких моделей (“чорна скринька”);\nобмежена підтримка української мови у більшості відкритих NLP-бібліотек.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.056411", "order": 48, "lang": "uk"}}
{"id": "1_docx#49", "doc_id": "1_docx", "text": "З огляду на це, можна зробити висновок, що обробка неструктурованих даних є однією з центральних проблем сучасної науки про дані (Data Science). Її ефективне вирішення дозволить перейти від хаотичних текстових потоків до системного аналізу, що підвищить якість рішень у бізнесі, науці й суспільстві.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.058856", "order": 49, "lang": "uk"}}
{"id": "1_docx#50", "doc_id": "1_docx", "text": "Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих текстових даних, який забезпечить:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.061739", "order": 50, "lang": "uk"}}
{"id": "1_docx#51", "doc_id": "1_docx", "text": "універсальність щодо типів і форматів документів;\nавтоматичність та масштабованість;\nточність семантичного пошуку;\nможливість інтеграції з реальними інформаційними системами.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.063602", "order": 51, "lang": "uk"}}
{"id": "1_docx#52", "doc_id": "1_docx", "text": "Саме ця мета визначає подальшу логіку роботи — у наступних розділах буде розроблено архітектуру, реалізацію та апробацію програмного комплексу UXText Pipeline, який поєднує сучасні алгоритми NLP, бібліотеки Python і векторні бази даних для побудови ефективної системи аналізу неструктурованих текстів.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.066373", "order": 52, "lang": "uk"}}
{"id": "1_docx#53", "doc_id": "1_docx", "text": "1.2. Поняття неструктурованих даних, їх роль у сучасних інформаційних системах", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.068834", "order": 53, "lang": "uk"}}
{"id": "1_docx#54", "doc_id": "1_docx", "text": "У сучасних інформаційних системах усі дані, з якими працюють користувачі, аналітики або системи штучного інтелекту, умовно поділяються на три основні типи — структуровані, напівструктуровані та неструктуровані. Така класифікація визначається рівнем організованості інформації, способом її зберігання та можливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управління знаннями.\nСтруктуровані дані — це дані, які зберігаються у чітко визначеній формі та організовані за наперед встановленою схемою. Вони мають фіксовану структуру — таблиці, поля, типи даних, зв’язки між записами. Прикладами таких даних є бази даних SQL, електронні таблиці Excel, реєстри або облікові системи. Основною перевагою структурованих даних є можливість швидкого аналізу за допомогою мов запитів (SQL), статистичних методів чи систем бізнес-аналітики (BI). Проте їхній головний недолік полягає у тому, що вони описують лише формалізовану частину реальності — числа, категорії, дати, тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.074971", "order": 54, "lang": "uk"}}
{"id": "1_docx#55", "doc_id": "1_docx", "text": "тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].\nНапівструктуровані дані займають проміжне місце між структурованими й неструктурованими. Вони мають певну внутрішню організацію, але не підпорядковуються жорсткій схемі. Такі дані часто представлені у форматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має стандартні поля (“від кого”, “кому”, “дата”), але зміст повідомлення — це вільний текст. Подібні дані активно використовуються у вебтехнологіях, обміні інформацією між сервісами (API) та у NoSQL-базах даних (MongoDB, Cassandra, Firebase). Їхня гнучкість є перевагою, однак аналіз таких даних потребує спеціалізованих методів парсингу, фільтрації й нормалізації [3].\nНеструктуровані дані — це інформація, яка не має фіксованої структури і не може бути представлена у вигляді таблиць або полів. До них належать тексти документів, повідомлення у месенджерах, публікації у соціальних мережах, звіти, аудіо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Осн", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.080220", "order": 55, "lang": "uk"}}
{"id": "1_docx#56", "doc_id": "1_docx", "text": "іо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Основна особливість таких даних полягає в тому, що вони передають значення через зміст, а не через форму. Два документи можуть мати зовсім різну структуру, але описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу\nконтексту, семантики, синтаксису і навіть прихованих зв’язків між словами [4].\nЗа оцінками компанії IBM, близько 80% усієї інформації, створеної людьми, належить до неструктурованих даних [5]. Це означає, що лише невелика частина всієї інформації у світі може бути легко оброблена стандартними базами даних, тоді як решта — тексти, документи, звіти, статті, пости, коментарі — залишаються невикористаними. Саме тому обробка неструктурованих даних стає однією з найважливіших задач сучасної комп’ютерної науки та аналітики.\nУ контексті архітектури інформаційних систем неструктуровані дані зберігаються переважно у файлових системах, хмарних сховищах або об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasti", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.087949", "order": 56, "lang": "uk"}}
{"id": "1_docx#57", "doc_id": "1_docx", "text": "бо об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasticsearch, дозволяють зберігати документи у будь-якому форматі та індексувати їх для пошуку. Проте ключову роль тут відіграють алгоритми попередньої обробки — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [6].\nРоль неструктурованих даних у сучасних ІТ-системах особливо важлива в контексті розвитку штучного інтелекту (AI) та Natural Language Processing (NLP). Саме ці дані є основним джерелом навчання для великих мовних моделей (LLM), таких як GPT, BERT, RoBERTa, LLaMA, T5 тощо. Їх тренування здійснюється на величезних масивах текстів — статтях, наукових працях, форумах, енциклопедіях, книгах [7]. Без таких джерел штучний інтелект не міг би досягти нинішнього рівня розуміння природної мови, контексту, емоційних відтінків і логічних зв’язків.\nЗ точки зору бізнесу, аналіз неструктурованих даних відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцме", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.093592", "order": 57, "lang": "uk"}}
{"id": "1_docx#58", "doc_id": "1_docx", "text": "відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцмережах або результатів опитувань, отримують конкурентну перевагу — вони швидше\nреагують на ринкові зміни, прогнозують поведінку споживачів і вдосконалюють свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині — аналіз клінічних описів допомагає автоматично виявляти симптоми, а в державному секторі — підвищує ефективність управлінських рішень.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.097214", "order": 58, "lang": "uk"}}
{"id": "1_docx#59", "doc_id": "1_docx", "text": "Таблиця 1.1 Порівняльна характеристика структурованих, напівструктурованих та неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.101502", "order": 59, "lang": "uk"}}
{"id": "1_docx#60", "doc_id": "1_docx", "text": "Тип даних Приклади Характеристика Приклади використання Структуровані SQL, Excel, CRM Чітка схема, таблиці, поля Облік, звітність, фінансовий аналіз Напівструктуровані JSON, XML, CSV Часткова організація, відсутність жорсткої схеми Обмін даними між системами, API Неструктуровані Тексти, PDF, аудіо, відео Відсутня структура, потребує інтелектуальної обробки NLP, аналітика, AI, Data Mining", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:17.104333", "order": 60, "lang": "uk"}}
{"id": "1_docx#61", "doc_id": "1_docx", "text": "Таким чином, неструктуровані дані становлять основну частину сучасного цифрового контенту. Вони є не лише джерелом знань, але й рушієм розвитку інтелектуальних технологій, аналітики та систем прийняття рішень. Їхнє ефективне використання вимагає поєднання методів комп’ютерної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що дозволяють перетворювати неструктуровані тексти у структуровану форму", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.107845", "order": 61, "lang": "uk"}}
{"id": "1_docx#62", "doc_id": "1_docx", "text": "для подальшого аналізу, що є важливим кроком до вдосконалення систем аналізу тексту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.109987", "order": 62, "lang": "uk"}}
{"id": "1_docx#63", "doc_id": "1_docx", "text": "1.3. Методи та підходи до обробки текстових даних у системах штучного інтелекту", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.112839", "order": 63, "lang": "uk"}}
{"id": "1_docx#64", "doc_id": "1_docx", "text": "Обробка текстових даних у системах штучного інтелекту є одним із найскладніших і водночас найдинамічніших напрямів розвитку сучасних інформаційних технологій. Людська мова — це гнучкий і багаторівневий механізм комунікації, який поєднує граматику, контекст, емоції, стиль та приховані смисли. Навчити комп’ютер розуміти текст означає створити модель, здатну не лише розпізнавати слова, а й аналізувати їхні зв’язки, контекст використання та семантичне навантаження [1].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.117233", "order": 64, "lang": "uk"}}
{"id": "1_docx#65", "doc_id": "1_docx", "text": "Залежно від рівня складності та точності, методи обробки текстових даних умовно поділяються на три основні групи:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.119680", "order": 65, "lang": "uk"}}
{"id": "1_docx#66", "doc_id": "1_docx", "text": "статистичні,\nметоди машинного навчання,\nнейромережеві (глибокі).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.121848", "order": 66, "lang": "uk"}}
{"id": "1_docx#67", "doc_id": "1_docx", "text": "Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити закономірності у частоті вживання слів [2]. Найвідомішим став метод Bag of Words (BoW), у якому кожен документ подається як набір слів без урахування порядку. Такий підхід простий у реалізації, але не враховує контекст, тому використовується для класифікації текстів і тематичного аналізу.\nІнший фундаментальний метод — TF-IDF (Term Frequency — Inverse Document Frequency) — оцінює важливість слова у конкретному документі відносно всієї колекції. Наприклад, слово “дані” зустрічається у всіх документах, тому має низьку вагу, тоді як термін “лематизація” зустрічається\nрідше, отже має більшу інформативність. TF-IDF залишається основою у пошукових системах (зокрема Google Search) і є відправною точкою для формування векторних подань текстів [3].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.126222", "order": 67, "lang": "uk"}}
{"id": "1_docx#68", "doc_id": "1_docx", "text": "Методи машинного навчання", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.129921", "order": 68, "lang": "uk"}}
{"id": "1_docx#69", "doc_id": "1_docx", "text": "Подальший розвиток комп’ютерних технологій призвів до появи методів машинного навчання (Machine Learning), які дозволили враховувати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йому подати достатньо прикладів [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.132926", "order": 69, "lang": "uk"}}
{"id": "1_docx#70", "doc_id": "1_docx", "text": "Серед найпоширеніших алгоритмів:", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.135086", "order": 70, "lang": "uk"}}
{"id": "1_docx#71", "doc_id": "1_docx", "text": "Наївний баєсівський класифікатор (Naive Bayes) — базується на теоремі Байєса і використовується для класифікації документів;\nМетод опорних векторів (SVM) — визначає межі між класами у багатовимірному просторі ознак;\nМетод k-ближчих сусідів (kNN) — класифікує тексти на основі схожості;\nЛогістична регресія — визначає ймовірність належності тексту до певної категорії.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.137910", "order": 71, "lang": "uk"}}
{"id": "1_docx#72", "doc_id": "1_docx", "text": "Такі алгоритми застосовувалися у задачах аналізу тональності тексту (sentiment analysis), тематичної класифікації, виявлення спаму чи автоматичного реферування. Їхнім обмеженням є потреба у ручному створенні ознак (feature engineering) — тобто необхідність перетворювати тексти у числові вектори на основі частотних характеристик (уніграми, біграми, триграми тощо).", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.140696", "order": 72, "lang": "uk"}}
{"id": "1_docx#73", "doc_id": "1_docx", "text": "Нейромережеві методи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.143327", "order": 73, "lang": "uk"}}
{"id": "1_docx#74", "doc_id": "1_docx", "text": "Наступним етапом еволюції стали нейромережеві моделі, які дозволили навчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які зберігають інформацію про значення та контекст.\nПершим проривом стала модель Word2Vec, розроблена в Google Research [5]. Вона навчалася на мільярдах слів і дозволяла розміщувати їх у спільному багатовимірному просторі, де схожі за змістом слова (наприклад, “кіт” і “пес”) знаходяться поруч. Подібні принципи використовують моделі GloVe (Global Vectors for Word Representation) [6] і FastText, розроблений у Meta AI Research, який враховує морфологічну структуру слова.\nГоловний прорив у галузі стався з появою трансформерних архітектур, які суттєво підвищили якість аналізу природної мови. Моделі типу BERT (Bidirectional Encoder Representations from Transformers) [7], GPT (Generative Pre-trained Transformer) [8] і SentenceTransformers використовують механізм Self-Attention (“механізм уваги”), який дозволяє моделі концентруватися на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьом", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.148178", "order": 74, "lang": "uk"}}
{"id": "1_docx#75", "doc_id": "1_docx", "text": "на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем і когнітивних аналітичних інструментів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.151541", "order": 75, "lang": "uk"}}
{"id": "1_docx#76", "doc_id": "1_docx", "text": "Етапи попередньої обробки тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.154405", "order": 76, "lang": "uk"}}
{"id": "1_docx#77", "doc_id": "1_docx", "text": "Перед застосуванням будь-яких алгоритмів штучного інтелекту текст проходить кілька стандартних етапів підготовки [9]:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.156340", "order": 77, "lang": "uk"}}
{"id": "1_docx#78", "doc_id": "1_docx", "text": "Токенізація — розбиття тексту на окремі одиниці (токени): слова, частини слів або символи.\nНормалізація — приведення тексту до єдиного вигляду (зменшення регістру, видалення пунктуації, стоп-слів).\nЛематизація або стемінг — зведення слів до базової форми (наприклад, “працюю”, “працював” → “працювати”).\nPOS-тегування (Part-of-Speech Tagging) — визначення частини мови для кожного слова.\nСинтаксичний аналіз (Parsing) — встановлення граматичних зв’язків між словами у реченні.\nСемантичний аналіз — визначення смислових відношень (“автор написав книгу” → дія, суб’єкт, об’єкт).", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.160172", "order": 78, "lang": "uk"}}
{"id": "1_docx#79", "doc_id": "1_docx", "text": "Таблиця 1.2 Еволюція методів обробки текстових даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.162431", "order": 79, "lang": "uk"}}
{"id": "1_docx#80", "doc_id": "1_docx", "text": "Покоління Основні методи Приклади моделей Ключові характеристики Статистичні Bag of Words, TF-IDF – Простота реалізації, не враховує контекст Машинного навчання SVM, Naive Bayes, kNN Scikit-learn Використання вручну створених ознак, середня точність Нейромережеві Word2Vec, FastText, BERT, GPT TensorFlow, PyTorch Висока точність, контекстне розуміння, потреба у великих ресурсах", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:17.165232", "order": 80, "lang": "uk"}}
{"id": "1_docx#81", "doc_id": "1_docx", "text": "Інтеграція методів у сучасні системи", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.167534", "order": 81, "lang": "uk"}}
{"id": "1_docx#82", "doc_id": "1_docx", "text": "Сучасні системи штучного інтелекту об’єднують усі ці методи у єдиний пайплайн обробки тексту — від очищення даних до формування векторного подання та пошуку за змістом [10].\nТиповий приклад:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.170094", "order": 82, "lang": "uk"}}
{"id": "1_docx#83", "doc_id": "1_docx", "text": "вилучення тексту з файлів різних форматів (PDF, DOCX, HTML);\nлематизація й очищення;\nперетворення тексту у вектори (SentenceTransformers);\nпобудова індексу у FAISS;\nздійснення семантичного пошуку.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.172021", "order": 83, "lang": "uk"}}
{"id": "1_docx#84", "doc_id": "1_docx", "text": "Саме цей підхід реалізовано у моєму дипломному проєкті UXText Pipeline, який демонструє практичну інтеграцію моделей NLP, бібліотеки Unstructured та індексаційного механізму FAISS у єдину систему.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.173834", "order": 84, "lang": "uk"}}
{"id": "1_docx#85", "doc_id": "1_docx", "text": "Отже, методи обробки текстових даних пройшли довгий шлях від простих статистичних моделей до глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не лише аналізують тексти, а й здатні взаємодіяти з користувачем природною мовою, пояснювати результати й формувати нові знання на основі отриманої інформації.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.177268", "order": 85, "lang": "uk"}}
{"id": "1_docx#86", "doc_id": "1_docx", "text": "1.4. Алгоритми й моделі обробки природної мови (NLP)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.179011", "order": 86, "lang": "uk"}}
{"id": "1_docx#87", "doc_id": "1_docx", "text": "Обробка природної мови (Natural Language Processing, скорочено NLP) є однією з ключових галузей штучного інтелекту, що вивчає методи взаємодії комп’ютерів із людською мовою. Основна мета NLP полягає у тому, щоб навчити машини розуміти, інтерпретувати й генерувати тексти так, як це робить людина [1]. Протягом останніх десятиліть у цій сфері було створено велику кількість алгоритмів і моделей, які поступово вдосконалювали здатність систем аналізувати тексти — від простого підрахунку частоти слів до глибокого семантичного розуміння контексту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.182902", "order": 87, "lang": "uk"}}
{"id": "1_docx#88", "doc_id": "1_docx", "text": "Класичні статистичні моделі", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.185655", "order": 88, "lang": "uk"}}
{"id": "1_docx#89", "doc_id": "1_docx", "text": "На початковому етапі розвитку NLP основну роль відігравали статистичні методи, засновані на кількісному аналізі тексту. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього порядку [2]. Наприклад, фрази “кіт спить на дивані” та “на дивані спить кіт” будуть представлені однаково, оскільки містять ті самі слова. Попри простоту реалізації, цей метод ігнорує синтаксичні та семантичні зв’язки, що обмежує його застосування в задачах, де важливий контекст.\nДля підвищення точності аналізу було запропоновано метод TF-IDF (Term Frequency — Inverse Document Frequency) [3], який враховує не лише частоту слова в документі, а й його рідкість у колекції текстів. TF-IDF дозволяє\nнадавати більшої ваги унікальним термінам, які є змістовно значущими для конкретного документа. Цей метод став базовим у побудові пошукових систем, тематичній класифікації та фільтрації спаму.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.190680", "order": 89, "lang": "uk"}}
{"id": "1_docx#90", "doc_id": "1_docx", "text": "Векторні моделі представлення текстів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.192558", "order": 90, "lang": "uk"}}
{"id": "1_docx#91", "doc_id": "1_docx", "text": "Подальшим етапом розвитку NLP стали векторні моделі, що дозволили описати слова у вигляді багатовимірних числових векторів. Такі моделі відображають семантичну близькість між словами — чим подібніші значення, тим менша відстань між їхніми векторами [4].\nОднією з перших таких моделей стала Word2Vec, розроблена командою Google Research [5]. Вона дозволяє вловлювати смислові зв’язки між словами: наприклад, векторна операція “король” – “чоловік” + “жінка” ≈ “королева” демонструє, що модель засвоює відношення між поняттями.\nІнша популярна модель — GloVe (Global Vectors for Word Representation), створена в Stanford NLP Group [6], поєднує статистичні принципи з нейронними мережами. Вона формує вектори на основі статистики спільних появ слів у тексті, що підвищує стабільність представлень.\nМодель FastText, розроблена компанією Meta (Facebook) [7], враховує морфологічну структуру слів, розбиваючи їх на підрядки (n-грамні вектори). Завдяки цьому система може розуміти нові або рідкісні слова, що особливо корисно для української мови, де багато словоформ.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.197991", "order": 91, "lang": "uk"}}
{"id": "1_docx#92", "doc_id": "1_docx", "text": "Таблиця 1.3 Порівняння класичних моделей представлення тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.202430", "order": 92, "lang": "uk"}}
{"id": "1_docx#93", "doc_id": "1_docx", "text": "Модель Принцип роботи Переваги Обмеження Bag of Words Підрахунок частоти слів у тексті Простота реалізації, ефективність для малих даних Ігнорує порядок і контекст TF-IDF Врахування частоти і рідкості слів Підвищує точність пошуку, придатний для класифікації Не враховує семантичні зв’язки Word2Vec Навчання векторів слів на великих корпусах Враховує семантику, компактне представлення Не розрізняє контексти одного слова GloVe Аналіз спільних появ слів у тексті Поєднує статистику й нейронний підхід Вимагає великих ресурсів FastText Представлення слова через підрядки Добре працює з новими словами Потребує тонкого налаштування параметрів", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:17.207007", "order": 93, "lang": "uk"}}
{"id": "1_docx#94", "doc_id": "1_docx", "text": "Контекстні моделі та архітектура трансформерів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.208623", "order": 94, "lang": "uk"}}
{"id": "1_docx#95", "doc_id": "1_docx", "text": "Обмеження класичних моделей полягали в тому, що кожне слово мало одне фіксоване представлення, незалежно від контексту. У реченнях “банк річки” та “кредитний банк” слово “банк” має різні значення, однак старі моделі цього не враховували. Революційним рішенням стало впровадження трансформерних моделей, які враховують контекст кожного слова в межах речення або документа [8].\nМодель BERT (Bidirectional Encoder Representations from Transformers), створена компанією Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT може розуміти, які слова в контексті впливають на значення інших, що забезпечує високу точність у задачах класифікації, пошуку відповідей і машинного перекладу. Модифікації RoBERTa, DistilBERT і ALBERT підвищили швидкість і ефективність обчислень без втрати якості [10].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.213658", "order": 95, "lang": "uk"}}
{"id": "1_docx#96", "doc_id": "1_docx", "text": "Іншу еволюційну гілку представляють генеративні трансформери, серед яких найвідомішою є GPT (Generative Pre-trained Transformer),", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.215206", "order": 96, "lang": "uk"}}
{"id": "1_docx#97", "doc_id": "1_docx", "text": "розроблена OpenAI [11]. GPT здатна не лише аналізувати текст, а й генерувати новий контент — логічно зв’язні речення, відповіді, коди чи описи. Її принцип полягає у передбаченні наступного слова за попереднім контекстом, що робить можливим природне спілкування людини з машиною.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.219313", "order": 97, "lang": "uk"}}
{"id": "1_docx#98", "doc_id": "1_docx", "text": "Сучасні моделі векторизації тексту", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.220703", "order": 98, "lang": "uk"}}
{"id": "1_docx#99", "doc_id": "1_docx", "text": "Для практичних задач класифікації, кластеризації та семантичного пошуку активно застосовується модель SentenceTransformers, яка є модифікацією BERT і оптимізована для створення векторів речень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді.\nТакий принцип лежить в основі семантичного пошуку, де збігаються не слова, а змісти. Наприклад, запит “літак затримався” може знайти документ із фразою “рейс відкладено”, навіть без жодного спільного слова. Саме це робить сучасні пошукові системи, чат-боти та аналітичні інструменти ефективнішими, ніж будь-коли раніше [13].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.224664", "order": 99, "lang": "uk"}}
{"id": "1_docx#100", "doc_id": "1_docx", "text": "Еволюція моделей обробки природної мови пройшла шлях від простих статистичних методів до складних багаторівневих нейромережевих архітектур.\nКласичні моделі, як Bag of Words та TF-IDF, заклали математичну основу для подання текстів, тоді як Word2Vec, GloVe та FastText зробили перші кроки до розуміння семантики.\nНатомість сучасні трансформери (BERT, GPT, SentenceTransformers) дозволили досягти справжнього контекстного розуміння мови, наблизивши штучний інтелект до людського рівня сприйняття.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.228892", "order": 100, "lang": "uk"}}
{"id": "1_docx#101", "doc_id": "1_docx", "text": "У межах цієї дипломної роботи я, Оліфіренко Кирило, досліджую можливості поєднання таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до векторизації та семантичного пошуку.\nРис. 1.1. Схематичне представлення методу обробки неструктурованих текстових даних (UXText Pipeline)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.232803", "order": 101, "lang": "uk"}}
{"id": "1_docx#102", "doc_id": "1_docx", "text": "1.5. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.236449", "order": 102, "lang": "uk"}}
{"id": "1_docx#103", "doc_id": "1_docx", "text": "У розвитку інструментів для обробки природної мови простежується еволюція від класичних бібліотек, орієнтованих на базові лінгвістичні операції, до сучасних фреймворків, що працюють із великими мовними моделями (LLM) та векторними базами даних. Для систематизації екосистеми NLP-технологій доцільно виділити три основні групи:", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.239366", "order": 103, "lang": "uk"}}
{"id": "1_docx#104", "doc_id": "1_docx", "text": "традиційні бібліотеки Python для обробки текстів,\nсучасні фреймворки та сервіси глибокого навчання,\nінтеграційні рішення для побудови аналітичних систем.", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:12:17.241978", "order": 104, "lang": "uk"}}
{"id": "1_docx#105", "doc_id": "1_docx", "text": "Ця класифікація дозволяє простежити логічну послідовність розвитку підходів — від токенізації й морфологічного аналізу до комплексного семантичного пошуку та генеративного текстового інтелекту.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.244201", "order": 105, "lang": "uk"}}
{"id": "1_docx#106", "doc_id": "1_docx", "text": "1.5.1. Огляд інструментів і бібліотек для роботи з текстовими даними", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.245734", "order": 106, "lang": "uk"}}
{"id": "1_docx#107", "doc_id": "1_docx", "text": "Першими інструментами, які забезпечили автоматизацію лінгвістичного аналізу, стали бібліотеки NLTK, spaCy та scikit-learn.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.247581", "order": 107, "lang": "uk"}}
{"id": "1_docx#108", "doc_id": "1_docx", "text": "NLTK (Natural Language Toolkit) — одна з найстаріших бібліотек для роботи з природною мовою у Python [1]. Вона містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко використовується в освіті та дослідженнях, оскільки дозволяє швидко створювати експериментальні моделі та аналізувати тексти. Проте через низьку швидкодію і складність масштабування її рідко застосовують у великих проєктах.\nspaCy стала відповіддю на потребу у високопродуктивних NLP-рішеннях. Вона орієнтована на промислове використання та забезпечує обробку текстів у десятки разів швидше, ніж NLTK [2]. SpaCy підтримує понад 60 мов, має модулі для виявлення іменованих сутностей (NER), синтаксичних залежностей і визначення частин мови. Завдяки інтеграції з TensorFlow і PyTorch, spaCy поєднує класичну обробку з глибоким навчанням.\nscikit-learn, хоча й не є суто NLP-бібліотекою, використовується для класифікації текстів, тематичного моделювання та аналізу тональності [3]. Бібліотека надає інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме sci", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.255157", "order": 108, "lang": "uk"}}
{"id": "1_docx#109", "doc_id": "1_docx", "text": "інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.257991", "order": 109, "lang": "uk"}}
{"id": "1_docx#110", "doc_id": "1_docx", "text": "Таблиця 1.4 Порівняння традиційних бібліотек Python для NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.259746", "order": 110, "lang": "uk"}}
{"id": "1_docx#111", "doc_id": "1_docx", "text": "Бібліотека Основні функції Переваги Недоліки та обмеження NLTK Токенізація, лематизація, морфологічний аналіз Простота, освітня цінність Повільна робота, не для продакшну spaCy POS-tagging, NER, синтаксичний аналіз Висока швидкодія, сучасні моделі Обмежена кількість мовних ресурсів scikit-learn Класифікація, TF-IDF, машинне навчання Гнучкість, модульність Не обробляє мову напряму, потребує препроцесингу", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:17.262620", "order": 111, "lang": "uk"}}
{"id": "1_docx#112", "doc_id": "1_docx", "text": "1.5.1. Сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.280529", "order": 112, "lang": "ro"}}
{"id": "1_docx#113", "doc_id": "1_docx", "text": "З появою глибоких нейронних мереж і трансформерних архітектур відбувся різкий стрибок у можливостях NLP.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.283909", "order": 113, "lang": "uk"}}
{"id": "1_docx#114", "doc_id": "1_docx", "text": "Фреймворк Transformers від Hugging Face став центральною платформою для роботи з великими мовними моделями [4]. Він надає доступ до сотень попередньо навчених моделей, серед яких BERT, GPT, T5, RoBERTa, DistilBERT, SentenceTransformers тощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних досліджень у NLP.\nІншим ключовим інструментом є OpenAI API, що надає доступ до моделей сімейства GPT (Generative Pre-trained Transformer) [5]. Ці моделі дозволяють не лише аналізувати текст, а й генерувати узагальнення, відповіді,", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.287344", "order": 114, "lang": "uk"}}
{"id": "1_docx#115", "doc_id": "1_docx", "text": "описи чи навіть програмний код, що відкриває новий рівень взаємодії людини та машини.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.289211", "order": 115, "lang": "uk"}}
{"id": "1_docx#116", "doc_id": "1_docx", "text": "Фреймворк LangChain вирішує проблему інтеграції мовних моделей із базами знань, надаючи можливість побудови логічних ланцюгів (chains) між LLM і зовнішніми джерелами [6]. Цей підхід використовується у системах Retrieval-Augmented Generation (RAG), де модель не лише генерує текст, а й отримує релевантну інформацію з баз даних чи документів.\nДля обробки документів різних форматів (PDF, DOCX, HTML, TXT) використовується бібліотека Unstructured.io, яка автоматично виділяє текстові елементи, розпізнає структуру документа і зберігає контекст [7]. Саме вона використовується в моєму проєкті UXText Pipeline для імпорту та нормалізації текстів перед векторизацією.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.292567", "order": 116, "lang": "uk"}}
{"id": "1_docx#117", "doc_id": "1_docx", "text": "Рис. 1.2. Архітектура екосистеми інструментів NLP", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.294149", "order": 117, "lang": "uk"}}
{"id": "1_docx#118", "doc_id": "1_docx", "text": "1.5.3. Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.295562", "order": 118, "lang": "uk"}}
{"id": "1_docx#119", "doc_id": "1_docx", "text": "Завершальним етапом є зберігання, індексація та пошук текстових даних. Для цього застосовуються інтеграційні рішення, які поєднують властивості баз даних та пошукових систем.\nFAISS (Facebook AI Similarity Search) — бібліотека для швидкого пошуку найближчих сусідів у багатовимірних просторах [8]. Вона оптимізована для GPU та дозволяє працювати з мільйонами векторів, що робить її основою систем семантичного пошуку.\nChromaDB — векторна база даних нового покоління, яка інтегрується з мовними моделями через LangChain та інші API [9]. Вона використовується в системах типу RAG для пошуку інформації за змістом. ChromaDB проста у використанні та підтримує локальне й хмарне розгортання.\nElasticSearch — потужна пошукова платформа, яка поєднує індексацію текстів і машинне навчання [10]. Вона підтримує пошук як за ключовими словами, так і за векторними поданнями, завдяки чому часто використовується у корпоративних аналітичних системах.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.300543", "order": 119, "lang": "uk"}}
{"id": "1_docx#120", "doc_id": "1_docx", "text": "Таблиця 1.5 Порівняльна характеристика інтеграційних інструментів", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.302236", "order": 120, "lang": "uk"}}
{"id": "1_docx#121", "doc_id": "1_docx", "text": "Інструмент Тип рішення Основні можливості Переваги FAISS Бібліотека векторного пошуку GPU-пошук, кластеризація, масштабованість Висока швидкість, підтримка великих обсягів ChromaDB Векторна база даних Семантичний пошук, інтеграція з LLM Простота, інтеграція з LangChain ElasticSearch Пошукова система Повнотекстовий та семантичний пошук Масштабованість, корпоративний рівень", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:17.305658", "order": 121, "lang": "uk"}}
{"id": "1_docx#122", "doc_id": "1_docx", "text": "Таким чином, сучасна екосистема обробки текстів складається з трьох взаємодоповнювальних рівнів:\nБазовий рівень — бібліотеки для лінгвістичного аналізу (NLTK, spaCy, scikit-learn).\nАналітичний рівень — фреймворки глибокого навчання (Transformers, LangChain, OpenAI API, Unstructured).\nІнфраструктурний рівень — рішення для зберігання, пошуку та індексації даних (FAISS, ChromaDB, ElasticSearch).\nПоєднання цих технологій створює основу для побудови інтелектуальних систем аналізу неструктурованих даних, здатних здійснювати пошук, класифікацію, узагальнення та інтерпретацію текстів із точністю, наближеною до людського розуміння.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.309196", "order": 122, "lang": "uk"}}
{"id": "1_docx#123", "doc_id": "1_docx", "text": "1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.313211", "order": 123, "lang": "uk"}}
{"id": "1_docx#124", "doc_id": "1_docx", "text": "Проблематика обробки неструктурованих даних залишається однією з найактуальніших тем сучасної комп’ютерної лінгвістики та аналітики даних. За оцінками International Data Corporation (IDC), понад 80 % усієї інформації у світі має неструктурований характер — це тексти, аудіо, відео, зображення, документи різних форматів [1]. Попри суттєвий прогрес у розвитку штучного інтелекту та глибоких нейронних мереж, реальні виклики пов’язані не лише з алгоритмами, а й з їх практичним застосуванням, масштабуванням і пояснюваністю результатів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.317107", "order": 124, "lang": "uk"}}
{"id": "1_docx#125", "doc_id": "1_docx", "text": "Під час розроблення власної дипломної системи UXText Pipeline я, Оліфіренко Кирило, переконався, що найбільші труднощі виникають саме на\nстику технічних, мовних і організаційних аспектів — там, де алгоритми мають взаємодіяти з реальними, часто «брудними» даними користувачів.\nНеоднорідність форматів і структур даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.320009", "order": 125, "lang": "uk"}}
{"id": "1_docx#126", "doc_id": "1_docx", "text": "Однією з найсерйозніших проблем є неоднорідність форматів джерел інформації. Тексти можуть бути представлені у вигляді PDF, DOCX, HTML, TXT, JSON, електронних листів або навіть відсканованих зображень. У кожного з форматів — своя структура, кодування, набір метаданих і правила відображення.\nНаприклад, при вилученні тексту з PDF часто трапляється втрата логічної послідовності слів або ігнорування таблиць, що ускладнює подальшу обробку [2].\nДля уніфікації таких даних застосовуються спеціальні парсери, зокрема бібліотека Unstructured.io, яка розділяє документ на структурні елементи — абзаци, списки, таблиці. Проте навіть вона не гарантує ідеального результату для зображень чи рукописних нотаток.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.323693", "order": 126, "lang": "uk"}}
{"id": "1_docx#127", "doc_id": "1_docx", "text": "Таким чином, перший виклик — створення універсальних конвертерів, здатних коректно вилучати текст незалежно від формату вхідного документа.\nРесурсоємність і масштабування NLP-моделей", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.326164", "order": 127, "lang": "uk"}}
{"id": "1_docx#128", "doc_id": "1_docx", "text": "Сучасні моделі, такі як BERT, GPT чи T5, вимагають надзвичайно великих обчислювальних ресурсів. Для їхнього навчання потрібні потужні графічні процесори (GPU), великі обсяги оперативної пам’яті та тривалі обчислення [3]. Навіть часткове донавчання моделей для конкретного домену може тривати годинами чи днями.\nУ корпоративних і навчальних умовах це створює фінансові та енергетичні обмеження. Саме тому розвивається напрям ефективних моделей\n(efficient AI) — таких, як DistilBERT, TinyLlama чи ALBERT, які споживають менше ресурсів при збереженні високої точності [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.329673", "order": 128, "lang": "uk"}}
{"id": "1_docx#129", "doc_id": "1_docx", "text": "Проблема пояснюваності (Explainability)", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.335337", "order": 129, "lang": "uk"}}
{"id": "1_docx#130", "doc_id": "1_docx", "text": "Ще один суттєвий виклик — інтерпретація результатів моделей. Класичні методи (TF-IDF, SVM) дають змогу зрозуміти, які саме ознаки вплинули на результат. Натомість нейронні мережі — це «чорні скриньки», логіка роботи яких не завжди зрозуміла [5].\nУ наукових або юридичних системах це неприйнятно, адже користувач має знати, чому модель зробила певний висновок. Саме тому активно розвивається напрям Explainable AI (XAI), що поєднує точність глибинного навчання з інтерпретованістю статистичних методів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.338989", "order": 130, "lang": "uk"}}
{"id": "1_docx#131", "doc_id": "1_docx", "text": "Прикладом можуть бути візуалізації важливості токенів у реченні чи методи LIME і SHAP, які показують, які слова вплинули на класифікацію.\nМовна неоднорідність і підтримка української мови", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.340824", "order": 131, "lang": "uk"}}
{"id": "1_docx#132", "doc_id": "1_docx", "text": "Суттєвим бар’єром залишається багатомовність і різна якість підтримки мов у бібліотеках. Для англійської мови існують великі корпуси, тоді як для української — вони обмежені. Наприклад, spaCy має часткову підтримку української морфології, але без повноцінного синтаксичного аналізу [6].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.342970", "order": 132, "lang": "uk"}}
{"id": "1_docx#133", "doc_id": "1_docx", "text": "Це ускладнює автоматизацію аналітики в українських організаціях і підкреслює потребу у національних корпусах даних та відкритих моделях, адаптованих до українського контексту.\nІнтеграція NLP із реальними ІТ-системами", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.344883", "order": 133, "lang": "uk"}}
{"id": "1_docx#134", "doc_id": "1_docx", "text": "Нерідко навіть високоточні моделі залишаються ізольованими — їх важко впровадити у реальні бізнес-процеси. CRM-платформи, ERP-системи чи\nдокументообіги використовують різні технологічні стеки (SQL, NoSQL, REST, GraphQL), тому інтеграція NLP-алгоритмів вимагає розробки універсальних API та конекторів [7].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.347534", "order": 134, "lang": "uk"}}
{"id": "1_docx#135", "doc_id": "1_docx", "text": "Це стало передумовою появи таких фреймворків, як LangChain та LlamaIndex, що поєднують мовні моделі з базами даних і зовнішніми джерелами знань.\nЯкість текстових даних і семантична неоднозначність\nЖодна модель не працює добре без якісних вхідних даних.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.350693", "order": 135, "lang": "uk"}}
{"id": "1_docx#136", "doc_id": "1_docx", "text": "Помилки, жаргон, скорочення, неоднозначність (наприклад, слово «банк» — фінансова установа чи берег річки) значно впливають на результати [8]. Для їх усунення використовують етапи очищення, нормалізації, лематизації та векторизації, які дозволяють зменшити шум у даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.353887", "order": 136, "lang": "uk"}}
{"id": "1_docx#137", "doc_id": "1_docx", "text": "Саме на цьому етапі реалізується найбільша частина інтелектуальної роботи у системах обробки неструктурованих текстів.\nБезпека та конфіденційність даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.355896", "order": 137, "lang": "uk"}}
{"id": "1_docx#138", "doc_id": "1_docx", "text": "Оскільки більшість систем NLP працюють з чутливою інформацією, актуальним є питання захисту персональних даних і локальної обробки. Хмарні сервіси (як-от OpenAI API чи Claude AI) не завжди відповідають вимогам корпоративної або державної безпеки.\nТому зростає попит на офлайн-моделі та приватні рішення, що розгортаються у внутрішніх мережах підприємств без передачі даних третім сторонам [9].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.358640", "order": 138, "lang": "uk"}}
{"id": "1_docx#139", "doc_id": "1_docx", "text": "Напрями вдосконалення\nДля подолання наведених проблем сформувалися кілька ключових напрямів розвитку методів обробки неструктурованих даних:\nМультимодальні моделі — поєднання тексту, зображень, аудіо та відео (наприклад, CLIP, Gemini, Kosmos-1), що дозволяє системам аналізувати інформацію комплексно.\nОптимізація ресурсів — створення полегшених архітектур (DistilBERT, TinyLlama, Mistral 7B), які зберігають продуктивність, але працюють на споживацькому обладнанні.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.361607", "order": 139, "lang": "uk"}}
{"id": "1_docx#140", "doc_id": "1_docx", "text": "Пояснюваність моделей (XAI) — розробка інтерпретованих методів аналізу, що дають змогу пояснювати результати користувачам.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.363646", "order": 140, "lang": "uk"}}
{"id": "1_docx#141", "doc_id": "1_docx", "text": "Покращення якості даних — автоматичне виявлення шуму, нормалізація, переклад, фільтрація та сегментація контенту.\nСтандартизація інтеграцій — розробка відкритих API та протоколів взаємодії для універсальної роботи NLP-модулів у різних системах.\nТаблиця 1.6 Основні проблеми та напрями їх вирішення", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.366662", "order": 141, "lang": "uk"}}
{"id": "1_docx#142", "doc_id": "1_docx", "text": "Проблема Причина Можливі напрями вдосконалення Неоднорідність форматів Різні структури документів (PDF, HTML, DOCX) Використання універсальних парсерів (Unstructured.io) Висока ресурсоємність Складні моделі з мільйонами параметрів Використання полегшених архітектур (DistilBERT, TinyLlama) Непояснюваність результатів «Чорна скринька» нейронних мереж Методи XAI, візуалізація уваги, SHAP-аналіз Багатомовність Обмежені корпуси української мови Створення відкритих національних корпусів Складність інтеграції Несумісні технологічні стеки Використання LangChain, REST API, FAISS Якість тексту Наявність шуму, помилок Очищення, нормалізація, векторизація Безпека даних Обробка чутливої інформації Локальні (on-premise) рішення, приватні LLM", "type": "Table", "meta": {"ingested_at": "2025-11-13T19:12:17.370952", "order": 142, "lang": "uk"}}
{"id": "1_docx#143", "doc_id": "1_docx", "text": "Рис. 1.3. Узагальнена схема проблем і шляхів удосконалення методів обробки неструктурованих даних", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.373935", "order": 143, "lang": "uk"}}
{"id": "1_docx#144", "doc_id": "1_docx", "text": "1.7. Висновки до розділу 1", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:12:17.376071", "order": 144, "lang": "uk"}}
{"id": "1_docx#145", "doc_id": "1_docx", "text": "У першому розділі було проведено всебічний теоретичний аналіз проблеми обробки неструктурованих даних, окреслено сучасні тенденції та виклики, а також визначено методологічні основи, на яких базується подальша частина дипломного дослідження. Аналіз показав, що обсяг неструктурованої інформації у світі стрімко зростає, і більшість даних, які сьогодні генеруються людиною чи машинами, мають саме неструктурований характер — це тексти,\nзображення, відео, аудіо, повідомлення у соціальних мережах, електронна пошта, технічна документація тощо. Такі дані не мають чіткої схеми зберігання і потребують спеціальних методів для вилучення, структуризації та аналізу.\nБуло встановлено, що традиційні методи обробки текстової інформації, зокрема статистичні підходи на кшталт Bag of Words або TF-IDF, мають обмежену здатність до розуміння контексту та семантики. Їх перевагою є простота реалізації та зрозумілість результатів, однак у сучасних умовах, коли мова йде про глибоке семантичне розпізнавання, ці підходи стають недостатніми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у ко", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.382276", "order": 145, "lang": "uk"}}
{"id": "1_docx#146", "doc_id": "1_docx", "text": "зі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у контексті, аналізувати зв’язки між фразами та реченнями, а також формувати осмислені векторні представлення тексту. Ці технології створили основу для побудови систем нового покоління — інтелектуальних пошукових механізмів, аналітичних платформ і мовних асистентів.\nУ ході огляду інструментів було виокремлено три рівні екосистеми для роботи з текстовими даними. Перший рівень становлять традиційні бібліотеки Python (NLTK, spaCy, scikit-learn), які забезпечують базову лінгвістичну обробку та підготовку текстів. Другий рівень охоплює сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured), що дозволяють реалізовувати глибокий семантичний аналіз, генерацію текстів і роботу з великими мовними моделями. Третій рівень формують інтеграційні рішення (FAISS, ChromaDB, ElasticSearch), які забезпечують збереження, векторизацію, пошук і зв’язок між компонентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки неструктурованих даних.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.389192", "order": 146, "lang": "uk"}}
{"id": "1_docx#147", "doc_id": "1_docx", "text": "Разом із тим, проведений аналіз виявив низку ключових проблем, які залишаються невирішеними у сфері NLP та аналітики текстів. Серед них —", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:12:17.391298", "order": 147, "lang": "uk"}}
{"id": "1_docx#148", "doc_id": "1_docx", "text": "неоднорідність форматів даних, висока ресурсомісткість сучасних моделей, складність інтерпретації результатів, брак універсальних методів для багатомовного аналізу, а також питання інтеграції таких систем у реальні бізнес-процеси. Водночас визначено основні напрями вдосконалення існуючих методів: розвиток мультимодальних моделей, підвищення ефективності обчислень, створення пояснюваних моделей (Explainable AI) та вдосконалення інструментів попередньої обробки тексту.\nПідсумовуючи результати теоретичної частини, можна зробити висновок, що побудова ефективних систем аналізу неструктурованих даних потребує поєднання класичних методів статистичної обробки з сучасними підходами глибокого навчання та трансформерних моделей. Це дозволяє досягнути балансу між точністю, швидкодією та інтерпретованістю результатів.\nСаме тому у подальших розділах дипломної роботи буде розглянуто розробку універсального методу обробки неструктурованих текстових даних, який інтегрує сучасні бібліотеки (Unstructured, Transformers, SentenceTransformers, FAISS) у єдиний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів ін", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.398110", "order": 148, "lang": "uk"}}
{"id": "1_docx#149", "doc_id": "1_docx", "text": "ний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів інформації та створення інструменту, придатного для практичного використання у системах аналітики, досліджень і бізнесу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:12:17.401684", "order": 149, "lang": "uk"}}
{"id": "txt#1", "doc_id": "txt", "text": "Державний торговельно\nекономічний університет\nКафедра комп’ютерних наук та інформаційних систем\nКВАЛІФІКАЦІЙНА РОБОТА\nна тему:\n«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.057748", "order": 1, "lang": "uk"}}
{"id": "txt#2", "doc_id": "txt", "text": "Студента 2 курсу, 6м групи спеціальності 122 «Комп’ютерні науки»", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.060305", "order": 2, "lang": "uk"}}
{"id": "txt#3", "doc_id": "txt", "text": "підпис студента Оліфіренко Кирило Андрійович\nНауковий керівник\nкандидат фізико\nматематичних наук, доцент", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.062021", "order": 3, "lang": "uk"}}
{"id": "txt#4", "doc_id": "txt", "text": "підпис керівника\nТомашевська Тетяна Володимирівна\nГарант освітньої програми\nдоктор фізико\nматематичних наук, професор", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.064538", "order": 4, "lang": "uk"}}
{"id": "txt#5", "doc_id": "txt", "text": "підпис керівника\nПурський Олег Іванович\nКиїв 2025\nДержавний торговельно\nекономічний університет", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.066839", "order": 5, "lang": "uk"}}
{"id": "txt#6", "doc_id": "txt", "text": "Факультет інформаційних технологій Кафедра комп’ютерних наук та інформаційних систем Спеціальність 122 «Комп’ютерні науки» Освітня програма «Комп’ютерні науки»", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.069280", "order": 6, "lang": "uk"}}
{"id": "txt#7", "doc_id": "txt", "text": "Затверджую\nЗав. кафедри ____________Пурський О.І.\n«» грудня 2025р.\nЗавдання\nна кваліфікаційну роботу студенту\nОліфіренко Кирилу Андрійовичу\n(прізвище, ім’я, по батькові)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.071256", "order": 7, "lang": "uk"}}
{"id": "txt#8", "doc_id": "txt", "text": "1. Тема кваліфікаційної роботи «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» Затверджена наказом ректора від «» листопада 2025 р. № 4142\n2. Строк здачі студентом закінченої роботи 15 листопада 2025 року\n3. Цільова установка та вихідні дані до роботи Мета роботи: розробка моделі та інформаційної технології оцінки конкурентоспроможності підприємств електронної торгівлі. Об’єкт дослідження: автоматизація процесів оцінювання конкурентоспроможності підприємств електронної торгівлі. Предмет дослідження: моделі, методи та інформаційні технології оцінювання конкурентоспроможності.\n4. Перелік графічного матеріалу______________________________________ ______________________________________________________________________________________________________________________________________________________________________________________________________ 5. Консультанти по роботі із зазначенням розділів, за якими здійснюється консультування: Розділ Консультант (прізвище, ініціали) Підпис, дата Завдання видав Завдання прийняв 1 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 2 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 3 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р.\n6. Змі", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:40:48.076339", "order": 8, "lang": "uk"}}
{"id": "txt#9", "doc_id": "txt", "text": "ийняв 1 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 2 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р. 3 Самойленко Г.Т. 05.12.2023 р. 05.12.2023 р.\n6. Зміст кваліфікаційної роботи (перелік питань за кожним розділом) ВСТУП РОЗДІЛ 1. ТЕОРЕТИЧНІ АСПЕКТИ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВА 1.1. Аналіз проблематики та існуючих методів управління конкурентоспроможністю 1.2. Особливості оцінки та управління конкурентоспроможністю підприємств електронної комерції 1.3. Концептуальна модель оцінки та управління конкурентоспроможністю підприємств електронної комерції РОЗДІЛ 2. МАТЕМАТИЧНІ МОДЕЛІ ОЦІНКИ ТА УПРАВЛІННЯ КОНКУРЕНТОСПРОМОЖНІСТЮ ПІДПРИЄМСТВА 2.1. Система показників та модель оцінки конкурентоспроможності підприємств електронної комерції 2.2. Модель управління конкурентоспроможністю підприємства 2.3. Моделювання процесу оцінки конкурентоспроможності підприємства РОЗДІЛ 3. ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВ ЕЛЕКТРОННОЇ ТОРГІВЛІ 3.1. Інформаційно-логічна модель системи оцінки конкурентоспроможності підприємств 3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств 3.3.Технологія використання інформ", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:40:48.082700", "order": 9, "lang": "uk"}}
{"id": "txt#10", "doc_id": "txt", "text": "ємств 3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств 3.3.Технологія використання інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі ВИСНОВКИ СПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ\n7. Календарний план виконання роботи № Пор. Назва етапів кваліфікаційної роботи Строк виконання етапів роботи За планом фактично 1 2 3 4 1 Вибір теми кваліфікаційної роботи 01.11.2023 01.11.2023 2 Розробка та затвердження завдання на кваліфікаційну роботу 05.12.2023 05.12.2023 3 Вступ 01.05.2024 01.05.2024 4 РОЗДІЛ 1. Теоретичні аспекти оцінки конкурентоспроможності підприємства 14.06.2024 14.06.2024 5 Підготовка статті у збірник наукових статей магістрів 20.06.2024 20.06.2024", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:40:48.086682", "order": 10, "lang": "uk"}}
{"id": "txt#11", "doc_id": "txt", "text": "6 РОЗДІЛ 2. Математичні моделі оцінки та управління конкурентоспроможністю підприємства 05.09.2024 05.09.2024 7 РОЗДІЛ 3. Інформаційна технологія оцінки конкурентоспроможності підприємств електронної торгівлі 17.10.2024 17.10.2024\n8 Висновки 21.10.2024 21.10.2024 9 Здача кваліфікаційної роботи на кафедру науковому керівнику 23.10.2024 23.10.2024\n10 Попередній захист кваліфікаційної роботи 28.10.2024 28.10.2024 11 Виправлення зауважень, зовнішнє рецензування кваліфікаційної роботи 30.10.2024 30.10.2024 12 Представлення готової зшитої кваліфікаційної роботи на кафедру 04.11.2024 04.11.2024 13 Публічний захист кваліфікаційної роботи За розкладом роботи ЕК 8. Дата видачі завдання «5» грудня 2023 р 9. Керівник випускного кваліфікаційної роботи Самойленко Г.Т (прізвище, ініціали, підпис) 10. Гарант освітньої програми Пурський О.І. (прізвище, ініціали, підпис) 11. Завдання прийняв до виконання студент Оліфіренко К. ________ (прізвище, ініціали, підпис)", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.090822", "order": 11, "lang": "uk"}}
{"id": "txt#12", "doc_id": "txt", "text": "12. Відгук керівника кваліфікаційної роботи ________________________________________________________________ ________________________________________________________________ ________________________________________________________________ ________________________________________________________________ ________________________________________________________________ _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:40:48.094003", "order": 12, "lang": "uk"}}
{"id": "txt#13", "doc_id": "txt", "text": "___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:40:48.094854", "order": 13, "lang": "und"}}
{"id": "txt#14", "doc_id": "txt", "text": "Керівник кваліфікаційної роботи", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.095998", "order": 14, "lang": "uk"}}
{"id": "txt#15", "doc_id": "txt", "text": "______________________", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:40:48.096485", "order": 15, "lang": "und"}}
{"id": "txt#16", "doc_id": "txt", "text": "13. Висновок про кваліфікаційну роботу", "type": "ListItem", "meta": {"ingested_at": "2025-11-13T19:40:48.097766", "order": 16, "lang": "uk"}}
{"id": "txt#17", "doc_id": "txt", "text": "Кваліфікаційна робота студента _______ Оліфіренко К.А. (прізвище, ініціали) може бути допущена до захисту в екзаменаційній комісії.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.100152", "order": 17, "lang": "uk"}}
{"id": "txt#18", "doc_id": "txt", "text": "Гарант освітньої програми Пурський О.І.\n(підпис, прізвище, ініціали)\nЗавідувач кафедри Пурський О.І.\n(підпис, прізвище, ініціали)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.101764", "order": 18, "lang": "uk"}}
{"id": "txt#19", "doc_id": "txt", "text": "«_____»_________________2025 р.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:40:48.103688", "order": 19, "lang": "uk"}}
{"id": "txt#20", "doc_id": "txt", "text": "Анотація У кваліфікаційній роботі здійснено комплексну розробку моделей та інформаційної технології моніторингу соціально-економічних показників з метою підвищення ефективності управління регіональним розвитком. Теоретично обґрунтовано основні положення формування і проведення соціально-економічного моніторингу та запропоновано концепцію створення інформаційної системи оціньовання показників соціально-економічного розвитку регіону. Розроблено метод автоматизованого розрахунку комплексної оцінки показників соціально-економічного розвитку. Створено автоматизовану Web-систему оціньовання показників соціально-економічного розвитку регіонів України. Ключові слова: соціально-економічний розвиток, математична модель, інтегральні показники, інформаційна технологія.\nAnotation The qualification work is devoted to development of model and information technology of monitoring of social and economic indexes for the purpose of management efficiency increase by regional development The mechanism of the taking into account of differentiation of regional development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of soc", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.110074", "order": 20, "lang": "uk"}}
{"id": "txt#21", "doc_id": "txt", "text": "nal development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of social and economic development is offered and programmed. The Web-system for monitoring indicators of social and economic development of Ukraine regions is created and the technology of its use is developed. Keywords: social and economic development, mathematical model, integrated indicators, information technology.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.113395", "order": 21, "lang": "en"}}
{"id": "txt#22", "doc_id": "txt", "text": "ВСТУП…………………………………………………..…………………………9 РОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ…………….……..………….12 1.1. Постановка проблеми та актуальність обробки неструктурованих даних……………………………………………………..……………….….12 1.2 Поняття неструктурованих даних, їх роль у сучасних інформаційних системах………………………………………………...………….………..16 1.3 Методи та підходи до обробки текстових даних у системах штучного інтелекту ………………………………………………..…………………...20 1.4 Алгоритми й моделі обробки природної мови (NLP).……...………...20 1.5 Огляд інструментів і бібліотек для роботи з текстовими даними………………………………………………...……………………..20 1.5.1 Традиційні бібліотеки Python для NLP (NLTK, spaCy, scikit-learn)……………………..…..……………...…………………………….....23 1.5.2 Сучасні фреймворки, сервіси (Transformers, OpenAI API, LangChain, Unstructured)……………………..…..…………………….…...23 1.5.3 Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)….……………………………………...23 1.6 Проблематика та напрями вдосконалення методів обробки неструктурованих даних………………………...…...……………………..20 1.7 Висновки до розділу 1……………………………..…………………...20 РОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.119312", "order": 22, "lang": "uk"}}
{"id": "txt#23", "doc_id": "txt", "text": "них………………………...…...……………………..20 1.7 Висновки до розділу 1……………………………..…………………...20 РОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………………………………………………….…..26 2.1 Постановка задачі, вимоги та функціональні можливості системи…26 2.2 Архітектура методу обробки неструктурованих текстових даних…..30 2.3. Розробка алгоритму розбиття документів на семантичні фрагменти (чанки)………………………………………………………………….……33 2.4 Методи векторизації тексту та побудова семантичного індексу…...…………………….………………………………….…………26 2.5 Реалізація бази знань та пошукового механізму ……..…………..…..30 2.6 Висновки до розділу 2…………………………...……..…………..…..30 РОЗДІЛ 3. ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ІНТЕРФЕЙС СИСТЕМИ………………………………………………………….....…………36 3.1 Загальна структура програмного комплексу UXText Pipeline.………36 3.2 Інтеграція з бібліотекою Unstructured.io та FAISS ………….………..38 3.3 Реалізація API-сервісу (FastAPI) для обробки запитів.………....……42 3.4 Розробка користувацького інтерфейсу (Streamlit)…….…….………..38 3.5 Візуалізація результатів аналізу текстів .………...………...…………42 3.6 Тестування та оцінювання ефективності роботи системи.……..……42 3.7 Висновки до розділу 3.………...………………………….....…………42 ВИСНОВКИ……………………………..…………………", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.124238", "order": 23, "lang": "uk"}}
{"id": "txt#24", "doc_id": "txt", "text": "2 3.6 Тестування та оцінювання ефективності роботи системи.……..……42 3.7 Висновки до розділу 3.………...………………………….....…………42 ВИСНОВКИ……………………………..………………………………………48 СПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ……………...……………………..50 ДОДАТОК………………………………………………..……………..………52\nВСТУП У сучасному світі, коли інформаційні потоки стрімко зростають, значна частина даних, які створює людина чи система, має неструктурований характер. Це тексти документів, електронні листи, новини, пости у соціальних мережах, коментарі, відгуки, звіти чи наукові статті. За статистичними оцінками, понад 80 відсотків усієї цифрової інформації становлять саме неструктуровані дані, які не мають чіткої схеми чи фіксованої структури зберігання. Проте саме вони містять найціннішу інформацію для аналізу змісту, виявлення тенденцій, автоматизованого прийняття рішень чи побудови систем штучного інтелекту. Проблема полягає в тому, що традиційні інструменти обробки даних не пристосовані для ефективної роботи з такими джерелами. Методи на кшталт статистичного аналізу або пошуку за ключовими словами не здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’яз", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.130350", "order": 24, "lang": "uk"}}
{"id": "txt#25", "doc_id": "txt", "text": " здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’язки між ними. Актуальність теми зумовлена необхідністю створення ефективних методів, що дозволяють обробляти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штучного інтелекту, зокрема векторизації текстів і нейромережевих моделей природної мови (NLP), відкриває нові можливості для створення інтелектуальних систем аналізу тексту. Ці системи здатні не лише класифікувати чи шукати інформацію, а й знаходити семантичні зв’язки між фрагментами тексту, узагальнювати їх зміст і будувати рекомендації. Саме тому тема «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» є актуальною і відповідає сучасним тенденціям розвитку інформаційних технологій. Метою даної роботи є розробка методу та програмного забезпечення, що забезпечують ефективну обробку неструктурованих текстових даних шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завд", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.136910", "order": 25, "lang": "uk"}}
{"id": "txt#26", "doc_id": "txt", "text": "шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завдання: провести аналіз існуючих методів обробки текстових даних і сучасних підходів до представлення тексту; дослідити технології побудови векторних моделей, зокрема SentenceTransformers та FAISS; спроєктувати архітектуру програмного комплексу для інтеграції різних джерел даних; реалізувати систему UXText Pipeline, яка поєднує модулі обробки, зберігання та пошуку текстів; провести експериментальне тестування для оцінки ефективності запропонованого методу. Об’єктом дослідження є процес обробки неструктурованих текстових даних у системах аналітики. Предметом дослідження — методи й алгоритми нормалізації, векторизації та індексації текстів, що дозволяють підвищити якість пошуку, класифікації й аналізу текстової інформації. Під час виконання роботи застосовувалися методи аналітичного огляду та синтезу для вивчення літератури й порівняння існуючих підходів; алгоритмічного проєктування — для створення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також стати", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.143543", "order": 26, "lang": "uk"}}
{"id": "txt#27", "doc_id": "txt", "text": "ворення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також статистичні методи оцінювання результатів, що дозволили кількісно виміряти покращення якості обробки текстів. Наукова новизна роботи полягає у створенні універсального підходу до обробки неструктурованих даних, який поєднує модулі виділення тексту, нормалізації, семантичного подання та побудови індекса для швидкого пошуку. Запропонований метод дозволяє працювати з різними форматами документів (PDF, DOCX, HTML), об’єднуючи їх у єдину структуру даних без втрати змістової інформації. Крім того, у роботі реалізовано алгоритм побудови семантичного індекса з використанням SentenceTransformers і FAISS, що підвищує ефективність пошуку за змістом, а не лише за словами. Практичне значення роботи полягає в тому, що розроблений програмний комплекс UXText Pipeline може бути використаний як основа для створення аналітичних систем у різних сферах — від бізнес-аналітики та освіти до наукових досліджень і документообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і підвищує точність інформаційного пошуку. Розроблені ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.150074", "order": 27, "lang": "uk"}}
{"id": "txt#28", "doc_id": "txt", "text": "ументообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і підвищує точність інформаційного пошуку. Розроблені рішення можуть бути впроваджені в установах, де необхідна швидка обробка текстових звітів, статей або відгуків. Результати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» (2024 р.) та при реалізації програмного прототипу, опублікованого у відкритому доступі на GitHub. Під час експериментів підтверджено ефективність запропонованого методу — час обробки документів скоротився, а точність семантичного пошуку зросла порівняно з базовими алгоритмами. Дипломна робота складається зі вступу, чотирьох розділів, висновків, списку використаних джерел і додатків. Загальний обсяг становить 70 сторінок, включає 25 рисунків, 12 таблиць і 3 додатки. У роботі послідовно викладено теоретичні основи, аналіз існуючих рішень, власну методику обробки текстів, результати експериментів і приклади практичного використання створеного програмного комплексу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.155583", "order": 28, "lang": "uk"}}
{"id": "txt#29", "doc_id": "txt", "text": "РОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.160753", "order": 29, "lang": "bg"}}
{"id": "txt#30", "doc_id": "txt", "text": "1.1. Постановка проблеми та актуальність обробки неструктурованих даних У сучасному цифровому світі обсяг інформації, який щоденно створюється, зростає з небаченою швидкістю. За оцінками аналітичної компанії IDC (International Data Corporation), до 2025 року загальний обсяг глобальних даних перевищить 175 зетабайт, причому понад 80 відсотків із них становитимуть саме неструктуровані дані — тексти, зображення, відео, аудіозаписи, повідомлення в соціальних мережах тощо [1]. Ці дані генеруються користувачами, організаціями, сенсорами та системами штучного інтелекту. Проте, попри їхню кількість і потенційну цінність, лише невелика частина реально використовується для аналітики через відсутність ефективних методів автоматичної обробки. Проблема полягає у тому, що більшість традиційних інформаційних систем побудовані для роботи зі структурованими даними — таблицями, числовими показниками, логами, базами SQL. Такі системи ефективно працюють лише тоді, коли дані мають чітко визначену схему зберігання: є поля, типи, формат і зв’язки між ними. Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неодно", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.167599", "order": 30, "lang": "uk"}}
{"id": "txt#31", "doc_id": "txt", "text": "Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані зображення. Як наслідок, традиційні системи управління даними (DBMS) не здатні безпосередньо інтерпретувати зміст таких файлів, а тому значна частина потенційно корисної інформації залишається поза увагою аналітичних алгоритмів [2]. Я, Оліфіренко Кирило, як студент спеціальності «Комп’ютерні науки», у процесі дослідження дійшов висновку, що проблема обробки неструктурованих даних є не лише технічною, а й методологічною. Навіть у сучасних інформаційних середовищах — як у бізнесі, так і в державних установах — зберігаються великі масиви документів у різних форматах: звіти у PDF, службові документи у DOCX, переписки у форматі електронної пошти, повідомлення з месенджерів, а також текстові поля у CRM-системах. Всі ці джерела містять важливу інформацію, яку складно уніфікувати без єдиної системи автоматичної обробки. Актуальність цієї проблеми підтверджується низкою об’єктивних чинників: 1. Стрімке зростання обсягів текстових даних. Щосекунди в інтернеті з’являються міл", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.173474", "order": 31, "lang": "uk"}}
{"id": "txt#32", "doc_id": "txt", "text": "льність цієї проблеми підтверджується низкою об’єктивних чинників: 1. Стрімке зростання обсягів текстових даних. Щосекунди в інтернеті з’являються мільйони повідомлень — лише у Twitter щодня публікується понад 500 млн твітів, у Facebook — понад 300 млн постів, а також понад 4,5 млрд електронних листів у глобальному масштабі [3]. Така кількість текстів робить ручну обробку неможливою, що потребує створення інтелектуальних алгоритмів аналізу. 2. Неоднорідність джерел та форматів. Тексти можуть бути представлені в різних форматах (PDF, DOCX, HTML, TXT, Markdown), з різною структурою, мовою, стилем і контекстом. У багатьох випадках інформація поєднує текст і візуальні елементи (таблиці, формули, підписи до зображень), що створює додаткові труднощі при обробці. 3. Зростання ролі неструктурованих даних у прийнятті рішень. У багатьох сферах — бізнесі, медицині, освіті, науці, державному управлінні — від якісного аналізу текстової інформації залежить точність прогнозів і ефективність управлінських дій. o У бізнесі — аналіз відгуків клієнтів дозволяє виявляти рівень задоволеності. o У медицині — аналіз медичних карт допомагає визначати закономірності захворювань. o У науці — автоматизований", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.179604", "order": 32, "lang": "uk"}}
{"id": "txt#33", "doc_id": "txt", "text": "озволяє виявляти рівень задоволеності. o У медицині — аналіз медичних карт допомагає визначати закономірності захворювань. o У науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань. 4. Відсутність універсального методу обробки. Існуючі рішення часто обмежені конкретними форматами або мовами, а результати залежать від якості текстів, наявності помилок, скорочень, змішаних мов. Тому актуальним є завдання створення гнучкої системи, здатної об’єднувати різні етапи обробки — від вилучення тексту до його семантичного аналізу. Ключовою метою сучасних досліджень у цій галузі є розробка методів, які поєднують традиційні алгоритми лінгвістичного аналізу з сучасними моделями машинного навчання та глибокого навчання (Deep Learning). Ці методи включають такі етапи, як: • вилучення тексту з документів різних форматів; • очищення від шуму (HTML-тегів, непотрібних символів); • токенізація й лематизація; • визначення мови; • побудова векторних подань (векторизація); • індексація та семантичний пошук. Зокрема, у моїй роботі розглядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семан", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.185762", "order": 33, "lang": "uk"}}
{"id": "txt#34", "doc_id": "txt", "text": "ядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семантичного пошуку за змістом). Такий підхід дозволяє створити універсальний пайплайн UXText Pipeline, який автоматизує процес перетворення неструктурованих даних у придатну для аналізу форму. Разом із тим, залишається низка викликів, які обумовлюють актуальність подальших досліджень: • неоднорідність форматів і різна якість текстових даних; • потреба у великих обчислювальних ресурсах для сучасних моделей (BERT, GPT); • відсутність єдиних стандартів інтеграції NLP-моделей у корпоративні системи; • складність пояснення результатів глибоких моделей (“чорна скринька”); • обмежена підтримка української мови у більшості відкритих NLP-бібліотек. З огляду на це, можна зробити висновок, що обробка неструктурованих даних є однією з центральних проблем сучасної науки про дані (Data Science). Її ефективне вирішення дозволить перейти від хаотичних текстових потоків до системного аналізу, що підвищить якість рішень у бізнесі, науці й суспільстві. Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих тек", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.191665", "order": 34, "lang": "uk"}}
{"id": "txt#35", "doc_id": "txt", "text": "ість рішень у бізнесі, науці й суспільстві. Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих текстових даних, який забезпечить: • універсальність щодо типів і форматів документів; • автоматичність та масштабованість; • точність семантичного пошуку; • можливість інтеграції з реальними інформаційними системами. Саме ця мета визначає подальшу логіку роботи — у наступних розділах буде розроблено архітектуру, реалізацію та апробацію програмного комплексу UXText Pipeline, який поєднує сучасні алгоритми NLP, бібліотеки Python і векторні бази даних для побудови ефективної системи аналізу неструктурованих текстів.\n1.2. Поняття неструктурованих даних, їх роль у сучасних інформаційних системах У сучасних інформаційних системах усі дані, з якими працюють користувачі, аналітики або системи штучного інтелекту, умовно поділяються на три основні типи — структуровані, напівструктуровані та неструктуровані. Така класифікація визначається рівнем організованості інформації, способом її зберігання та можливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управл", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.197957", "order": 35, "lang": "uk"}}
{"id": "txt#36", "doc_id": "txt", "text": "ожливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управління знаннями. Структуровані дані — це дані, які зберігаються у чітко визначеній формі та організовані за наперед встановленою схемою. Вони мають фіксовану структуру — таблиці, поля, типи даних, зв’язки між записами. Прикладами таких даних є бази даних SQL, електронні таблиці Excel, реєстри або облікові системи. Основною перевагою структурованих даних є можливість швидкого аналізу за допомогою мов запитів (SQL), статистичних методів чи систем бізнес-аналітики (BI). Проте їхній головний недолік полягає у тому, що вони описують лише формалізовану частину реальності — числа, категорії, дати, тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2]. Напівструктуровані дані займають проміжне місце між структурованими й неструктурованими. Вони мають певну внутрішню організацію, але не підпорядковуються жорсткій схемі. Такі дані часто представлені у форматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має ста", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.204177", "order": 36, "lang": "uk"}}
{"id": "txt#37", "doc_id": "txt", "text": "орматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має стандартні поля (“від кого”, “кому”, “дата”), але зміст повідомлення — це вільний текст. Подібні дані активно використовуються у вебтехнологіях, обміні інформацією між сервісами (API) та у NoSQL-базах даних (MongoDB, Cassandra, Firebase). Їхня гнучкість є перевагою, однак аналіз таких даних потребує спеціалізованих методів парсингу, фільтрації й нормалізації [3]. Неструктуровані дані — це інформація, яка не має фіксованої структури і не може бути представлена у вигляді таблиць або полів. До них належать тексти документів, повідомлення у месенджерах, публікації у соціальних мережах, звіти, аудіо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Основна особливість таких даних полягає в тому, що вони передають значення через зміст, а не через форму. Два документи можуть мати зовсім різну структуру, але описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу контексту, семантики, синтаксису і навіть прихованих зв", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.210449", "order": 37, "lang": "uk"}}
{"id": "txt#38", "doc_id": "txt", "text": "ле описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу контексту, семантики, синтаксису і навіть прихованих зв’язків між словами [4]. За оцінками компанії IBM, близько 80% усієї інформації, створеної людьми, належить до неструктурованих даних [5]. Це означає, що лише невелика частина всієї інформації у світі може бути легко оброблена стандартними базами даних, тоді як решта — тексти, документи, звіти, статті, пости, коментарі — залишаються невикористаними. Саме тому обробка неструктурованих даних стає однією з найважливіших задач сучасної комп’ютерної науки та аналітики. У контексті архітектури інформаційних систем неструктуровані дані зберігаються переважно у файлових системах, хмарних сховищах або об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasticsearch, дозволяють зберігати документи у будь-якому форматі та індексувати їх для пошуку. Проте ключову роль тут відіграють алгоритми попередньої обробки — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.216469", "order": 38, "lang": "uk"}}
{"id": "txt#39", "doc_id": "txt", "text": " — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [6]. Роль неструктурованих даних у сучасних ІТ-системах особливо важлива в контексті розвитку штучного інтелекту (AI) та Natural Language Processing (NLP). Саме ці дані є основним джерелом навчання для великих мовних моделей (LLM), таких як GPT, BERT, RoBERTa, LLaMA, T5 тощо. Їх тренування здійснюється на величезних масивах текстів — статтях, наукових працях, форумах, енциклопедіях, книгах [7]. Без таких джерел штучний інтелект не міг би досягти нинішнього рівня розуміння природної мови, контексту, емоційних відтінків і логічних зв’язків. З точки зору бізнесу, аналіз неструктурованих даних відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцмережах або результатів опитувань, отримують конкурентну перевагу — вони швидше реагують на ринкові зміни, прогнозують поведінку споживачів і вдосконалюють свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині —", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.222498", "order": 39, "lang": "uk"}}
{"id": "txt#40", "doc_id": "txt", "text": "свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині — аналіз клінічних описів допомагає автоматично виявляти симптоми, а в державному секторі — підвищує ефективність управлінських рішень. Таблиця 1.1 Порівняльна характеристика структурованих, напівструктурованих та неструктурованих даних Тип даних Приклади Характеристика Приклади використання Структуровані SQL, Excel, CRM Чітка схема, таблиці, поля Облік, звітність, фінансовий аналіз Напівструктуровані JSON, XML, CSV Часткова організація, відсутність жорсткої схеми Обмін даними між системами, API Неструктуровані Тексти, PDF, аудіо, відео Відсутня структура, потребує інтелектуальної обробки NLP, аналітика, AI, Data Mining Таким чином, неструктуровані дані становлять основну частину сучасного цифрового контенту. Вони є не лише джерелом знань, але й рушієм розвитку інтелектуальних технологій, аналітики та систем прийняття рішень. Їхнє ефективне використання вимагає поєднання методів комп’ютерної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.228887", "order": 40, "lang": "uk"}}
{"id": "txt#41", "doc_id": "txt", "text": "ної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що дозволяють перетворювати неструктуровані тексти у структуровану форму для подальшого аналізу, що є важливим кроком до вдосконалення систем аналізу тексту.\n1.3. Методи та підходи до обробки текстових даних у системах штучного інтелекту Обробка текстових даних у системах штучного інтелекту є одним із найскладніших і водночас найдинамічніших напрямів розвитку сучасних інформаційних технологій. Людська мова — це гнучкий і багаторівневий механізм комунікації, який поєднує граматику, контекст, емоції, стиль та приховані смисли. Навчити комп’ютер розуміти текст означає створити модель, здатну не лише розпізнавати слова, а й аналізувати їхні зв’язки, контекст використання та семантичне навантаження [1]. Залежно від рівня складності та точності, методи обробки текстових даних умовно поділяються на три основні групи: • статистичні, • методи машинного навчання, • нейромережеві (глибокі). Статистичні методи Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити за", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.235269", "order": 41, "lang": "uk"}}
{"id": "txt#42", "doc_id": "txt", "text": "ні методи Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити закономірності у частоті вживання слів [2]. Найвідомішим став метод Bag of Words (BoW), у якому кожен документ подається як набір слів без урахування порядку. Такий підхід простий у реалізації, але не враховує контекст, тому використовується для класифікації текстів і тематичного аналізу. Інший фундаментальний метод — TF-IDF (Term Frequency — Inverse Document Frequency) — оцінює важливість слова у конкретному документі відносно всієї колекції. Наприклад, слово “дані” зустрічається у всіх документах, тому має низьку вагу, тоді як термін “лематизація” зустрічається рідше, отже має більшу інформативність. TF-IDF залишається основою у пошукових системах (зокрема Google Search) і є відправною точкою для формування векторних подань текстів [3]. Методи машинного навчання Подальший розвиток комп’ютерних технологій призвів до появи методів машинного навчання (Machine Learning), які дозволили враховувати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йо", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.240882", "order": 42, "lang": "uk"}}
{"id": "txt#43", "doc_id": "txt", "text": "увати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йому подати достатньо прикладів [4]. Серед найпоширеніших алгоритмів: • Наївний баєсівський класифікатор (Naive Bayes) — базується на теоремі Байєса і використовується для класифікації документів; • Метод опорних векторів (SVM) — визначає межі між класами у багатовимірному просторі ознак; • Метод k-ближчих сусідів (kNN) — класифікує тексти на основі схожості; • Логістична регресія — визначає ймовірність належності тексту до певної категорії. Такі алгоритми застосовувалися у задачах аналізу тональності тексту (sentiment analysis), тематичної класифікації, виявлення спаму чи автоматичного реферування. Їхнім обмеженням є потреба у ручному створенні ознак (feature engineering) — тобто необхідність перетворювати тексти у числові вектори на основі частотних характеристик (уніграми, біграми, триграми тощо). Нейромережеві методи Наступним етапом еволюції стали нейромережеві моделі, які дозволили навчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які збе", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.247153", "order": 43, "lang": "uk"}}
{"id": "txt#44", "doc_id": "txt", "text": "авчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які зберігають інформацію про значення та контекст. Першим проривом стала модель Word2Vec, розроблена в Google Research [5]. Вона навчалася на мільярдах слів і дозволяла розміщувати їх у спільному багатовимірному просторі, де схожі за змістом слова (наприклад, “кіт” і “пес”) знаходяться поруч. Подібні принципи використовують моделі GloVe (Global Vectors for Word Representation) [6] і FastText, розроблений у Meta AI Research, який враховує морфологічну структуру слова. Головний прорив у галузі стався з появою трансформерних архітектур, які суттєво підвищили якість аналізу природної мови. Моделі типу BERT (Bidirectional Encoder Representations from Transformers) [7], GPT (Generative Pre-trained Transformer) [8] і SentenceTransformers використовують механізм Self-Attention (“механізм уваги”), який дозволяє моделі концентруватися на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.252613", "order": 44, "lang": "uk"}}
{"id": "txt#45", "doc_id": "txt", "text": "й, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем і когнітивних аналітичних інструментів. Етапи попередньої обробки тексту Перед застосуванням будь-яких алгоритмів штучного інтелекту текст проходить кілька стандартних етапів підготовки [9]: 1. Токенізація — розбиття тексту на окремі одиниці (токени): слова, частини слів або символи. 2. Нормалізація — приведення тексту до єдиного вигляду (зменшення регістру, видалення пунктуації, стоп-слів). 3. Лематизація або стемінг — зведення слів до базової форми (наприклад, “працюю”, “працював” → “працювати”). 4. POS-тегування (Part-of-Speech Tagging) — визначення частини мови для кожного слова. 5. Синтаксичний аналіз (Parsing) — встановлення граматичних зв’язків між словами у реченні. 6. Семантичний аналіз — визначення смислових відношень (“автор написав книгу” → дія, суб’єкт, об’єкт). Таблиця 1.2 Еволюція методів обробки текстових даних Покоління Основні методи Приклади моделей Ключові характеристики Статистичні Bag of Words, TF-IDF – Простота реалізації, не враховує контекст Машинного навчання SVM, Naive Bayes, kNN Scikit-learn Використання в", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.258875", "order": 45, "lang": "uk"}}
{"id": "txt#46", "doc_id": "txt", "text": "тики Статистичні Bag of Words, TF-IDF – Простота реалізації, не враховує контекст Машинного навчання SVM, Naive Bayes, kNN Scikit-learn Використання вручну створених ознак, середня точність Нейромережеві Word2Vec, FastText, BERT, GPT TensorFlow, PyTorch Висока точність, контекстне розуміння, потреба у великих ресурсах Інтеграція методів у сучасні системи Сучасні системи штучного інтелекту об’єднують усі ці методи у єдиний пайплайн обробки тексту — від очищення даних до формування векторного подання та пошуку за змістом [10]. Типовий приклад: 1. вилучення тексту з файлів різних форматів (PDF, DOCX, HTML); 2. лематизація й очищення; 3. перетворення тексту у вектори (SentenceTransformers); 4. побудова індексу у FAISS; 5. здійснення семантичного пошуку. Саме цей підхід реалізовано у моєму дипломному проєкті UXText Pipeline, який демонструє практичну інтеграцію моделей NLP, бібліотеки Unstructured та індексаційного механізму FAISS у єдину систему. Отже, методи обробки текстових даних пройшли довгий шлях від простих статистичних моделей до глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не л", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.265132", "order": 46, "lang": "uk"}}
{"id": "txt#47", "doc_id": "txt", "text": "глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не лише аналізують тексти, а й здатні взаємодіяти з користувачем природною мовою, пояснювати результати й формувати нові знання на основі отриманої інформації.\n1.4. Алгоритми й моделі обробки природної мови (NLP) Обробка природної мови (Natural Language Processing, скорочено NLP) є однією з ключових галузей штучного інтелекту, що вивчає методи взаємодії комп’ютерів із людською мовою. Основна мета NLP полягає у тому, щоб навчити машини розуміти, інтерпретувати й генерувати тексти так, як це робить людина [1]. Протягом останніх десятиліть у цій сфері було створено велику кількість алгоритмів і моделей, які поступово вдосконалювали здатність систем аналізувати тексти — від простого підрахунку частоти слів до глибокого семантичного розуміння контексту. Класичні статистичні моделі На початковому етапі розвитку NLP основну роль відігравали статистичні методи, засновані на кількісному аналізі тексту. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього пор", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.270978", "order": 47, "lang": "uk"}}
{"id": "txt#48", "doc_id": "txt", "text": "у. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього порядку [2]. Наприклад, фрази “кіт спить на дивані” та “на дивані спить кіт” будуть представлені однаково, оскільки містять ті самі слова. Попри простоту реалізації, цей метод ігнорує синтаксичні та семантичні зв’язки, що обмежує його застосування в задачах, де важливий контекст. Для підвищення точності аналізу було запропоновано метод TF-IDF (Term Frequency — Inverse Document Frequency) [3], який враховує не лише частоту слова в документі, а й його рідкість у колекції текстів. TF-IDF дозволяє надавати більшої ваги унікальним термінам, які є змістовно значущими для конкретного документа. Цей метод став базовим у побудові пошукових систем, тематичній класифікації та фільтрації спаму. Векторні моделі представлення текстів Подальшим етапом розвитку NLP стали векторні моделі, що дозволили описати слова у вигляді багатовимірних числових векторів. Такі моделі відображають семантичну близькість між словами — чим подібніші значення, тим менша відстань між їхніми векторами [4]. Однією з перших таких моделей стала Word2Vec, розроблена командою Goo", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.276653", "order": 48, "lang": "uk"}}
{"id": "txt#49", "doc_id": "txt", "text": "ж словами — чим подібніші значення, тим менша відстань між їхніми векторами [4]. Однією з перших таких моделей стала Word2Vec, розроблена командою Google Research [5]. Вона дозволяє вловлювати смислові зв’язки між словами: наприклад, векторна операція “король” – “чоловік” + “жінка” ≈ “королева” демонструє, що модель засвоює відношення між поняттями. Інша популярна модель — GloVe (Global Vectors for Word Representation), створена в Stanford NLP Group [6], поєднує статистичні принципи з нейронними мережами. Вона формує вектори на основі статистики спільних появ слів у тексті, що підвищує стабільність представлень. Модель FastText, розроблена компанією Meta (Facebook) [7], враховує морфологічну структуру слів, розбиваючи їх на підрядки (n-грамні вектори). Завдяки цьому система може розуміти нові або рідкісні слова, що особливо корисно для української мови, де багато словоформ. Таблиця 1.3 Порівняння класичних моделей представлення тексту Модель Принцип роботи Переваги Обмеження Bag of Words Підрахунок частоти слів у тексті Простота реалізації, ефективність для малих даних Ігнорує порядок і контекст TF-IDF Врахування частоти і рідкості слів Підвищує точність пошуку, придатний для класи", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.282847", "order": 49, "lang": "uk"}}
{"id": "txt#50", "doc_id": "txt", "text": "зації, ефективність для малих даних Ігнорує порядок і контекст TF-IDF Врахування частоти і рідкості слів Підвищує точність пошуку, придатний для класифікації Не враховує семантичні зв’язки Word2Vec Навчання векторів слів на великих корпусах Враховує семантику, компактне представлення Не розрізняє контексти одного слова GloVe Аналіз спільних появ слів у тексті Поєднує статистику й нейронний підхід Вимагає великих ресурсів FastText Представлення слова через підрядки Добре працює з новими словами Потребує тонкого налаштування параметрів\nКонтекстні моделі та архітектура трансформерів Обмеження класичних моделей полягали в тому, що кожне слово мало одне фіксоване представлення, незалежно від контексту. У реченнях “банк річки” та “кредитний банк” слово “банк” має різні значення, однак старі моделі цього не враховували. Революційним рішенням стало впровадження трансформерних моделей, які враховують контекст кожного слова в межах речення або документа [8]. Модель BERT (Bidirectional Encoder Representations from Transformers), створена компанією Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT мож", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.289161", "order": 50, "lang": "uk"}}
{"id": "txt#51", "doc_id": "txt", "text": "єю Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT може розуміти, які слова в контексті впливають на значення інших, що забезпечує високу точність у задачах класифікації, пошуку відповідей і машинного перекладу. Модифікації RoBERTa, DistilBERT і ALBERT підвищили швидкість і ефективність обчислень без втрати якості [10]. Іншу еволюційну гілку представляють генеративні трансформери, серед яких найвідомішою є GPT (Generative Pre-trained Transformer), розроблена OpenAI [11]. GPT здатна не лише аналізувати текст, а й генерувати новий контент — логічно зв’язні речення, відповіді, коди чи описи. Її принцип полягає у передбаченні наступного слова за попереднім контекстом, що робить можливим природне спілкування людини з машиною. Сучасні моделі векторизації тексту Для практичних задач класифікації, кластеризації та семантичного пошуку активно застосовується модель SentenceTransformers, яка є модифікацією BERT і оптимізована для створення векторів речень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді. Такий принцип ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.295170", "order": 51, "lang": "uk"}}
{"id": "txt#52", "doc_id": "txt", "text": "чень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді. Такий принцип лежить в основі семантичного пошуку, де збігаються не слова, а змісти. Наприклад, запит “літак затримався” може знайти документ із фразою “рейс відкладено”, навіть без жодного спільного слова. Саме це робить сучасні пошукові системи, чат-боти та аналітичні інструменти ефективнішими, ніж будь-коли раніше [13]. Узагальнення Еволюція моделей обробки природної мови пройшла шлях від простих статистичних методів до складних багаторівневих нейромережевих архітектур. Класичні моделі, як Bag of Words та TF-IDF, заклали математичну основу для подання текстів, тоді як Word2Vec, GloVe та FastText зробили перші кроки до розуміння семантики. Натомість сучасні трансформери (BERT, GPT, SentenceTransformers) дозволили досягти справжнього контекстного розуміння мови, наблизивши штучний інтелект до людського рівня сприйняття. У межах цієї дипломної роботи я, Оліфіренко Кирило, досліджую можливості поєднання таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.301682", "order": 52, "lang": "uk"}}
{"id": "txt#53", "doc_id": "txt", "text": "я таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до векторизації та семантичного пошуку.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.304332", "order": 53, "lang": "uk"}}
{"id": "txt#54", "doc_id": "txt", "text": "Рис. 1.1. Схематичне представлення методу обробки неструктурованих текстових даних (UXText Pipeline)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.311140", "order": 54, "lang": "ru"}}
{"id": "txt#55", "doc_id": "txt", "text": "1.5. Огляд інструментів і бібліотек для роботи з текстовими даними У розвитку інструментів для обробки природної мови простежується еволюція від класичних бібліотек, орієнтованих на базові лінгвістичні операції, до сучасних фреймворків, що працюють із великими мовними моделями (LLM) та векторними базами даних. Для систематизації екосистеми NLP-технологій доцільно виділити три основні групи: 1. традиційні бібліотеки Python для обробки текстів, 2. сучасні фреймворки та сервіси глибокого навчання, 3. інтеграційні рішення для побудови аналітичних систем. Ця класифікація дозволяє простежити логічну послідовність розвитку підходів — від токенізації й морфологічного аналізу до комплексного семантичного пошуку та генеративного текстового інтелекту.\n1.5.1. Огляд інструментів і бібліотек для роботи з текстовими даними Першими інструментами, які забезпечили автоматизацію лінгвістичного аналізу, стали бібліотеки NLTK, spaCy та scikit-learn. NLTK (Natural Language Toolkit) — одна з найстаріших бібліотек для роботи з природною мовою у Python [1]. Вона містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко ви", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.317336", "order": 55, "lang": "uk"}}
{"id": "txt#56", "doc_id": "txt", "text": "она містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко використовується в освіті та дослідженнях, оскільки дозволяє швидко створювати експериментальні моделі та аналізувати тексти. Проте через низьку швидкодію і складність масштабування її рідко застосовують у великих проєктах. spaCy стала відповіддю на потребу у високопродуктивних NLP-рішеннях. Вона орієнтована на промислове використання та забезпечує обробку текстів у десятки разів швидше, ніж NLTK [2]. SpaCy підтримує понад 60 мов, має модулі для виявлення іменованих сутностей (NER), синтаксичних залежностей і визначення частин мови. Завдяки інтеграції з TensorFlow і PyTorch, spaCy поєднує класичну обробку з глибоким навчанням. scikit-learn, хоча й не є суто NLP-бібліотекою, використовується для класифікації текстів, тематичного моделювання та аналізу тональності [3]. Бібліотека надає інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.323380", "order": 56, "lang": "uk"}}
{"id": "txt#57", "doc_id": "txt", "text": "огістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей. Таблиця 1.4 Порівняння традиційних бібліотек Python для NLP Бібліотека Основні функції Переваги Недоліки та обмеження NLTK Токенізація, лематизація, морфологічний аналіз Простота, освітня цінність Повільна робота, не для продакшну spaCy POS-tagging, NER, синтаксичний аналіз Висока швидкодія, сучасні моделі Обмежена кількість мовних ресурсів scikit-learn Класифікація, TF-IDF, машинне навчання Гнучкість, модульність Не обробляє мову напряму, потребує препроцесингу\n1.5.1. Сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured) З появою глибоких нейронних мереж і трансформерних архітектур відбувся різкий стрибок у можливостях NLP. Фреймворк Transformers від Hugging Face став центральною платформою для роботи з великими мовними моделями [4]. Він надає доступ до сотень попередньо навчених моделей, серед яких BERT, GPT, T5, RoBERTa, DistilBERT, SentenceTransformers тощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних дослідж", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.329135", "order": 57, "lang": "uk"}}
{"id": "txt#58", "doc_id": "txt", "text": "ощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних досліджень у NLP. Іншим ключовим інструментом є OpenAI API, що надає доступ до моделей сімейства GPT (Generative Pre-trained Transformer) [5]. Ці моделі дозволяють не лише аналізувати текст, а й генерувати узагальнення, відповіді, описи чи навіть програмний код, що відкриває новий рівень взаємодії людини та машини. Фреймворк LangChain вирішує проблему інтеграції мовних моделей із базами знань, надаючи можливість побудови логічних ланцюгів (chains) між LLM і зовнішніми джерелами [6]. Цей підхід використовується у системах Retrieval-Augmented Generation (RAG), де модель не лише генерує текст, а й отримує релевантну інформацію з баз даних чи документів. Для обробки документів різних форматів (PDF, DOCX, HTML, TXT) використовується бібліотека Unstructured.io, яка автоматично виділяє текстові елементи, розпізнає структуру документа і зберігає контекст [7]. Саме вона використовується в моєму проєкті UXText Pipeline для імпорту та нормалізації текстів перед векторизацією.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.334631", "order": 58, "lang": "uk"}}
{"id": "txt#59", "doc_id": "txt", "text": "Рис. 1.2. Архітектура екосистеми інструментів NLP", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.336296", "order": 59, "lang": "uk"}}
{"id": "txt#60", "doc_id": "txt", "text": "1.5.3. Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch) Завершальним етапом є зберігання, індексація та пошук текстових даних. Для цього застосовуються інтеграційні рішення, які поєднують властивості баз даних та пошукових систем. FAISS (Facebook AI Similarity Search) — бібліотека для швидкого пошуку найближчих сусідів у багатовимірних просторах [8]. Вона оптимізована для GPU та дозволяє працювати з мільйонами векторів, що робить її основою систем семантичного пошуку. ChromaDB — векторна база даних нового покоління, яка інтегрується з мовними моделями через LangChain та інші API [9]. Вона використовується в системах типу RAG для пошуку інформації за змістом. ChromaDB проста у використанні та підтримує локальне й хмарне розгортання. ElasticSearch — потужна пошукова платформа, яка поєднує індексацію текстів і машинне навчання [10]. Вона підтримує пошук як за ключовими словами, так і за векторними поданнями, завдяки чому часто використовується у корпоративних аналітичних системах. Таблиця 1.5 Порівняльна характеристика інтеграційних інструментів Інструмент Тип рішення Основні можливості Переваги FAISS Бібліотека векторного пошуку GPU-пошу", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.341449", "order": 60, "lang": "uk"}}
{"id": "txt#61", "doc_id": "txt", "text": "5 Порівняльна характеристика інтеграційних інструментів Інструмент Тип рішення Основні можливості Переваги FAISS Бібліотека векторного пошуку GPU-пошук, кластеризація, масштабованість Висока швидкість, підтримка великих обсягів ChromaDB Векторна база даних Семантичний пошук, інтеграція з LLM Простота, інтеграція з LangChain ElasticSearch Пошукова система Повнотекстовий та семантичний пошук Масштабованість, корпоративний рівень Таким чином, сучасна екосистема обробки текстів складається з трьох взаємодоповнювальних рівнів: Базовий рівень — бібліотеки для лінгвістичного аналізу (NLTK, spaCy, scikit-learn). Аналітичний рівень — фреймворки глибокого навчання (Transformers, LangChain, OpenAI API, Unstructured). Інфраструктурний рівень — рішення для зберігання, пошуку та індексації даних (FAISS, ChromaDB, ElasticSearch). Поєднання цих технологій створює основу для побудови інтелектуальних систем аналізу неструктурованих даних, здатних здійснювати пошук, класифікацію, узагальнення та інтерпретацію текстів із точністю, наближеною до людського розуміння.\n1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних Проблематика обробки неструктурованих даних залишається ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.348027", "order": 61, "lang": "uk"}}
{"id": "txt#62", "doc_id": "txt", "text": " розуміння.\n1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних Проблематика обробки неструктурованих даних залишається однією з найактуальніших тем сучасної комп’ютерної лінгвістики та аналітики даних. За оцінками International Data Corporation (IDC), понад 80 % усієї інформації у світі має неструктурований характер — це тексти, аудіо, відео, зображення, документи різних форматів [1]. Попри суттєвий прогрес у розвитку штучного інтелекту та глибоких нейронних мереж, реальні виклики пов’язані не лише з алгоритмами, а й з їх практичним застосуванням, масштабуванням і пояснюваністю результатів. Під час розроблення власної дипломної системи UXText Pipeline я, Оліфіренко Кирило, переконався, що найбільші труднощі виникають саме на стику технічних, мовних і організаційних аспектів — там, де алгоритми мають взаємодіяти з реальними, часто «брудними» даними користувачів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.353774", "order": 62, "lang": "uk"}}
{"id": "txt#63", "doc_id": "txt", "text": "Неоднорідність форматів і структур даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.355220", "order": 63, "lang": "uk"}}
{"id": "txt#64", "doc_id": "txt", "text": "Однією з найсерйозніших проблем є неоднорідність форматів джерел інформації. Тексти можуть бути представлені у вигляді PDF, DOCX, HTML, TXT, JSON, електронних листів або навіть відсканованих зображень. У кожного з форматів — своя структура, кодування, набір метаданих і правила відображення. Наприклад, при вилученні тексту з PDF часто трапляється втрата логічної послідовності слів або ігнорування таблиць, що ускладнює подальшу обробку [2]. Для уніфікації таких даних застосовуються спеціальні парсери, зокрема бібліотека Unstructured.io, яка розділяє документ на структурні елементи — абзаци, списки, таблиці. Проте навіть вона не гарантує ідеального результату для зображень чи рукописних нотаток. Таким чином, перший виклик — створення універсальних конвертерів, здатних коректно вилучати текст незалежно від формату вхідного документа.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.359957", "order": 64, "lang": "uk"}}
{"id": "txt#65", "doc_id": "txt", "text": "Ресурсоємність і масштабування NLP\nмоделей", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.361661", "order": 65, "lang": "uk"}}
{"id": "txt#66", "doc_id": "txt", "text": "Сучасні моделі, такі як BERT, GPT чи T5, вимагають надзвичайно великих обчислювальних ресурсів. Для їхнього навчання потрібні потужні графічні процесори (GPU), великі обсяги оперативної пам’яті та тривалі обчислення [3]. Навіть часткове донавчання моделей для конкретного домену може тривати годинами чи днями. У корпоративних і навчальних умовах це створює фінансові та енергетичні обмеження. Саме тому розвивається напрям ефективних моделей (efficient AI) — таких, як DistilBERT, TinyLlama чи ALBERT, які споживають менше ресурсів при збереженні високої точності [4].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.365356", "order": 66, "lang": "uk"}}
{"id": "txt#67", "doc_id": "txt", "text": "Проблема пояснюваності (Explainability)", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.370510", "order": 67, "lang": "uk"}}
{"id": "txt#68", "doc_id": "txt", "text": "Ще один суттєвий виклик — інтерпретація результатів моделей. Класичні методи (TF-IDF, SVM) дають змогу зрозуміти, які саме ознаки вплинули на результат. Натомість нейронні мережі — це «чорні скриньки», логіка роботи яких не завжди зрозуміла [5]. У наукових або юридичних системах це неприйнятно, адже користувач має знати, чому модель зробила певний висновок. Саме тому активно розвивається напрям Explainable AI (XAI), що поєднує точність глибинного навчання з інтерпретованістю статистичних методів. Прикладом можуть бути візуалізації важливості токенів у реченні чи методи LIME і SHAP, які показують, які слова вплинули на класифікацію.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.374314", "order": 68, "lang": "uk"}}
{"id": "txt#69", "doc_id": "txt", "text": "Мовна неоднорідність і підтримка української мови", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.375879", "order": 69, "lang": "uk"}}
{"id": "txt#70", "doc_id": "txt", "text": "Суттєвим бар’єром залишається багатомовність і різна якість підтримки мов у бібліотеках. Для англійської мови існують великі корпуси, тоді як для української — вони обмежені. Наприклад, spaCy має часткову підтримку української морфології, але без повноцінного синтаксичного аналізу [6]. Це ускладнює автоматизацію аналітики в українських організаціях і підкреслює потребу у національних корпусах даних та відкритих моделях, адаптованих до українського контексту.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.379061", "order": 70, "lang": "uk"}}
{"id": "txt#71", "doc_id": "txt", "text": "Інтеграція NLP із реальними ІТ-системами", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.380810", "order": 71, "lang": "uk"}}
{"id": "txt#72", "doc_id": "txt", "text": "Нерідко навіть високоточні моделі залишаються ізольованими — їх важко впровадити у реальні бізнес-процеси. CRM-платформи, ERP-системи чи документообіги використовують різні технологічні стеки (SQL, NoSQL, REST, GraphQL), тому інтеграція NLP-алгоритмів вимагає розробки універсальних API та конекторів [7]. Це стало передумовою появи таких фреймворків, як LangChain та LlamaIndex, що поєднують мовні моделі з базами даних і зовнішніми джерелами знань.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.384107", "order": 72, "lang": "uk"}}
{"id": "txt#73", "doc_id": "txt", "text": "Якість текстових даних і семантична неоднозначність", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.386037", "order": 73, "lang": "uk"}}
{"id": "txt#74", "doc_id": "txt", "text": "Жодна модель не працює добре без якісних вхідних даних. Помилки, жаргон, скорочення, неоднозначність (наприклад, слово «банк» — фінансова установа чи берег річки) значно впливають на результати [8]. Для їх усунення використовують етапи очищення, нормалізації, лематизації та векторизації, які дозволяють зменшити шум у даних. Саме на цьому етапі реалізується найбільша частина інтелектуальної роботи у системах обробки неструктурованих текстів.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.389225", "order": 74, "lang": "uk"}}
{"id": "txt#75", "doc_id": "txt", "text": "Безпека та конфіденційність даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.390626", "order": 75, "lang": "uk"}}
{"id": "txt#76", "doc_id": "txt", "text": "Оскільки більшість систем NLP працюють з чутливою інформацією, актуальним є питання захисту персональних даних і локальної обробки. Хмарні сервіси (як-от OpenAI API чи Claude AI) не завжди відповідають вимогам корпоративної або державної безпеки. Тому зростає попит на офлайн-моделі та приватні рішення, що розгортаються у внутрішніх мережах підприємств без передачі даних третім сторонам [9].", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.393812", "order": 76, "lang": "uk"}}
{"id": "txt#77", "doc_id": "txt", "text": "Напрями вдосконалення", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.396433", "order": 77, "lang": "uk"}}
{"id": "txt#78", "doc_id": "txt", "text": "Для подолання наведених проблем сформувалися кілька ключових напрямів розвитку методів обробки неструктурованих даних:\nМультимодальні моделі — поєднання тексту, зображень, аудіо та відео (наприклад, CLIP, Gemini, Kosmos-1), що дозволяє системам аналізувати інформацію комплексно.\nОптимізація ресурсів — створення полегшених архітектур (DistilBERT, TinyLlama, Mistral 7B), які зберігають продуктивність, але працюють на споживацькому обладнанні.", "type": "UncategorizedText", "meta": {"ingested_at": "2025-11-13T19:40:48.400122", "order": 78, "lang": "uk"}}
{"id": "txt#79", "doc_id": "txt", "text": "Пояснюваність моделей (XAI) — розробка інтерпретованих методів аналізу, що дають змогу пояснювати результати користувачам. Покращення якості даних — автоматичне виявлення шуму, нормалізація, переклад, фільтрація та сегментація контенту. Стандартизація інтеграцій — розробка відкритих API та протоколів взаємодії для універсальної роботи NLP-модулів у різних системах. Таблиця 1.6 Основні проблеми та напрями їх вирішення Проблема Причина Можливі напрями вдосконалення Неоднорідність форматів Різні структури документів (PDF, HTML, DOCX) Використання універсальних парсерів (Unstructured.io) Висока ресурсоємність Складні моделі з мільйонами параметрів Використання полегшених архітектур (DistilBERT, TinyLlama) Непояснюваність результатів «Чорна скринька» нейронних мереж Методи XAI, візуалізація уваги, SHAP-аналіз Багатомовність Обмежені корпуси української мови Створення відкритих національних корпусів Складність інтеграції Несумісні технологічні стеки Використання LangChain, REST API, FAISS Якість тексту Наявність шуму, помилок Очищення, нормалізація, векторизація Безпека даних Обробка чутливої інформації Локальні (on-premise) рішення, приватні LLM", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.405754", "order": 79, "lang": "uk"}}
{"id": "txt#80", "doc_id": "txt", "text": "Рис. 1.3. Узагальнена схема проблем і шляхів удосконалення методів обробки неструктурованих даних", "type": "Title", "meta": {"ingested_at": "2025-11-13T19:40:48.409304", "order": 80, "lang": "uk"}}
{"id": "txt#81", "doc_id": "txt", "text": "1.7. Висновки до розділу 1 У першому розділі було проведено всебічний теоретичний аналіз проблеми обробки неструктурованих даних, окреслено сучасні тенденції та виклики, а також визначено методологічні основи, на яких базується подальша частина дипломного дослідження. Аналіз показав, що обсяг неструктурованої інформації у світі стрімко зростає, і більшість даних, які сьогодні генеруються людиною чи машинами, мають саме неструктурований характер — це тексти, зображення, відео, аудіо, повідомлення у соціальних мережах, електронна пошта, технічна документація тощо. Такі дані не мають чіткої схеми зберігання і потребують спеціальних методів для вилучення, структуризації та аналізу. Було встановлено, що традиційні методи обробки текстової інформації, зокрема статистичні підходи на кшталт Bag of Words або TF-IDF, мають обмежену здатність до розуміння контексту та семантики. Їх перевагою є простота реалізації та зрозумілість результатів, однак у сучасних умовах, коли мова йде про глибоке семантичне розпізнавання, ці підходи стають недостатніми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині ", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.415333", "order": 81, "lang": "uk"}}
{"id": "txt#82", "doc_id": "txt", "text": "ми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у контексті, аналізувати зв’язки між фразами та реченнями, а також формувати осмислені векторні представлення тексту. Ці технології створили основу для побудови систем нового покоління — інтелектуальних пошукових механізмів, аналітичних платформ і мовних асистентів. У ході огляду інструментів було виокремлено три рівні екосистеми для роботи з текстовими даними. Перший рівень становлять традиційні бібліотеки Python (NLTK, spaCy, scikit-learn), які забезпечують базову лінгвістичну обробку та підготовку текстів. Другий рівень охоплює сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured), що дозволяють реалізовувати глибокий семантичний аналіз, генерацію текстів і роботу з великими мовними моделями. Третій рівень формують інтеграційні рішення (FAISS, ChromaDB, ElasticSearch), які забезпечують збереження, векторизацію, пошук і зв’язок між компонентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки нестру", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.421570", "order": 82, "lang": "uk"}}
{"id": "txt#83", "doc_id": "txt", "text": "понентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки неструктурованих даних. Разом із тим, проведений аналіз виявив низку ключових проблем, які залишаються невирішеними у сфері NLP та аналітики текстів. Серед них — неоднорідність форматів даних, висока ресурсомісткість сучасних моделей, складність інтерпретації результатів, брак універсальних методів для багатомовного аналізу, а також питання інтеграції таких систем у реальні бізнес-процеси. Водночас визначено основні напрями вдосконалення існуючих методів: розвиток мультимодальних моделей, підвищення ефективності обчислень, створення пояснюваних моделей (Explainable AI) та вдосконалення інструментів попередньої обробки тексту. Підсумовуючи результати теоретичної частини, можна зробити висновок, що побудова ефективних систем аналізу неструктурованих даних потребує поєднання класичних методів статистичної обробки з сучасними підходами глибокого навчання та трансформерних моделей. Це дозволяє досягнути балансу між точністю, швидкодією та інтерпретованістю результатів. Саме тому у подальших розділах дипломної роботи буде розглянуто розробку унів", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.428097", "order": 83, "lang": "uk"}}
{"id": "txt#84", "doc_id": "txt", "text": "гнути балансу між точністю, швидкодією та інтерпретованістю результатів. Саме тому у подальших розділах дипломної роботи буде розглянуто розробку універсального методу обробки неструктурованих текстових даних, який інтегрує сучасні бібліотеки (Unstructured, Transformers, SentenceTransformers, FAISS) у єдиний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів інформації та створення інструменту, придатного для практичного використання у системах аналітики, досліджень і бізнесу.", "type": "NarrativeText", "meta": {"ingested_at": "2025-11-13T19:40:48.431632", "order": 84, "lang": "uk"}}
