Державний торговельно-економічний університет

Кафедра комп’ютерних наук та інформаційних систем





КВАЛІФІКАЦІЙНА РОБОТА
на тему:

«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»



Студента 2 курсу, 6м групи
спеціальності  
122 «Комп’ютерні науки»


	

__________
підпис студента	Оліфіренко Кирило Андрійович
Науковий керівник
кандидат  фізико-математичних наук, доцент
	
___________
підпис керівника	
Томашевська Тетяна Володимирівна
Гарант освітньої програми
доктор фізико-математичних наук, професор	
___________
підпис керівника	
Пурський Олег Іванович



Київ 2025
 
Державний торговельно-економічний університет


Факультет інформаційних технологій
Кафедра комп’ютерних наук та інформаційних систем
Спеціальність 122 «Комп’ютерні науки»
Освітня програма «Комп’ютерні науки»



										Затверджую
					    Зав. кафедри ____________Пурський О.І.
								   	    «» грудня 2025р.



Завдання
на кваліфікаційну роботу студенту 

Оліфіренко Кирилу Андрійовичу
(прізвище, ім’я, по батькові)

1.	Тема кваліфікаційної роботи
«Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту»
Затверджена наказом ректора від «» листопада 2025 р. № 4142

2. Строк здачі студентом закінченої роботи  15 листопада 2025 року

3. Цільова установка та вихідні дані до роботи
Мета роботи: розробка моделі та інформаційної технології оцінки конкурентоспроможності підприємств електронної торгівлі.
Об’єкт дослідження: автоматизація процесів оцінювання конкурентоспроможності підприємств електронної торгівлі.
Предмет дослідження: моделі, методи та інформаційні технології оцінювання конкурентоспроможності.

4. Перелік графічного матеріалу______________________________________ ______________________________________________________________________________________________________________________________________________________________________________________________________
5.	Консультанти по роботі із зазначенням розділів, за якими здійснюється консультування:
Розділ	Консультант
(прізвище, ініціали)	Підпис, дата
		Завдання видав	Завдання прийняв
1	Самойленко Г.Т.	05.12.2023 р.	05.12.2023 р.
2	Самойленко Г.Т.	05.12.2023 р.	05.12.2023 р.
3	Самойленко Г.Т.	05.12.2023 р.	05.12.2023 р.

6. Зміст кваліфікаційної роботи (перелік питань за кожним розділом)
ВСТУП
РОЗДІЛ 1. ТЕОРЕТИЧНІ АСПЕКТИ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВА
1.1.	Аналіз проблематики та існуючих методів управління конкурентоспроможністю
1.2. Особливості оцінки та управління конкурентоспроможністю підприємств електронної комерції
1.3.	Концептуальна модель оцінки та управління конкурентоспроможністю підприємств електронної комерції
РОЗДІЛ 2. МАТЕМАТИЧНІ МОДЕЛІ ОЦІНКИ ТА УПРАВЛІННЯ КОНКУРЕНТОСПРОМОЖНІСТЮ ПІДПРИЄМСТВА
	2.1. Система показників та модель оцінки конкурентоспроможності підприємств електронної комерції
	2.2.  Модель управління конкурентоспроможністю підприємства
	2.3. Моделювання процесу оцінки конкурентоспроможності підприємства
РОЗДІЛ 3. ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ОЦІНКИ КОНКУРЕНТОСПРОМОЖНОСТІ ПІДПРИЄМСТВ ЕЛЕКТРОННОЇ ТОРГІВЛІ
3.1. Інформаційно-логічна модель системи оцінки конкурентоспроможності підприємств
3.2. Специфіка програмно-апаратної реалізації інформаційної системи оцінки конкурентоспроможності підприємств
3.3.Технологія використання інформаційної системи оцінки конкурентоспроможності підприємств електронної торгівлі
ВИСНОВКИ
СПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ

7. Календарний план виконання роботи
№
Пор.	Назва етапів кваліфікаційної роботи	Строк виконання етапів роботи
		За планом	фактично
1	2	3	4
1	Вибір теми кваліфікаційної роботи	01.11.2023	01.11.2023
2	Розробка та затвердження завдання на кваліфікаційну роботу	05.12.2023	05.12.2023
3	Вступ 	01.05.2024	01.05.2024
4	РОЗДІЛ 1. Теоретичні аспекти оцінки конкурентоспроможності підприємства	14.06.2024	14.06.2024
5	Підготовка статті у збірник наукових статей магістрів	20.06.2024
	20.06.2024

6	РОЗДІЛ 2. Математичні моделі оцінки та управління конкурентоспроможністю підприємства	05.09.2024	05.09.2024
7	РОЗДІЛ 3. Інформаційна технологія оцінки конкурентоспроможності підприємств електронної торгівлі	17.10.2024
	17.10.2024

8	Висновки	21.10.2024	21.10.2024
9	Здача кваліфікаційної роботи на кафедру науковому керівнику	 23.10.2024
	 23.10.2024

10	Попередній захист кваліфікаційної роботи	28.10.2024	28.10.2024
11	Виправлення зауважень, зовнішнє рецензування кваліфікаційної роботи	30.10.2024	30.10.2024
12	Представлення готової зшитої кваліфікаційної роботи на кафедру	04.11.2024	04.11.2024
13	Публічний захист кваліфікаційної  роботи	За розкладом роботи ЕК	
8. Дата видачі завдання  «5» грудня 2023 р
9. Керівник випускного кваліфікаційної роботи     Самойленко Г.Т		
(прізвище, ініціали, підпис)
10. Гарант освітньої програми 			      Пурський О.І.			
(прізвище, ініціали, підпис)
11. Завдання прийняв до виконання студент	      Оліфіренко К.     ________
    		(прізвище, ініціали, підпис)
 
12. Відгук керівника кваліфікаційної  роботи
________________________________________________________________
________________________________________________________________
________________________________________________________________
________________________________________________________________
________________________________________________________________
____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Керівник кваліфікаційної  роботи
______________________    
        (підпис, дата)


13. Висновок про кваліфікаційну  роботу

Кваліфікаційна робота  студента  _______                    Оліфіренко К.А.	   
                                                                                                                                (прізвище, ініціали)
може бути допущена до захисту в екзаменаційній комісії.


Гарант освітньої програми					Пурський О.І.	
                                                                                                                          (підпис, прізвище, ініціали)


Завідувач кафедри 						Пурський О.І.	
                                                                                                                          (підпис, прізвище, ініціали)
«_____»_________________2025 р.

Анотація
У кваліфікаційній роботі здійснено комплексну розробку моделей та інформаційної технології моніторингу соціально-економічних показників з метою підвищення ефективності управління регіональним розвитком. Теоретично обґрунтовано основні положення формування і проведення соціально-економічного моніторингу та запропоновано концепцію створення інформаційної системи оціньовання показників соціально-економічного розвитку регіону. Розроблено метод автоматизованого розрахунку комплексної оцінки показників соціально-економічного розвитку. Створено автоматизовану  Web-систему оціньовання показників соціально-економічного розвитку регіонів України.
Ключові слова: соціально-економічний розвиток, математична модель, інтегральні показники, інформаційна технологія.

Anotation
The qualification work is devoted to development of model and information technology of monitoring of social and economic indexes for the purpose of management efficiency increase by regional development The mechanism of the taking into account of differentiation of regional development in information system of social and economic monitoring is developed. The automated calculation method of integrated indicators of social and economic development is offered and programmed. The Web-system for monitoring indicators of social and economic development of Ukraine regions is created and the technology of its use is developed.
Keywords: social and economic development, mathematical model, integrated indicators, information technology.

ЗМІСТ

ВСТУП…………………………………………………..…………………………9
РОЗДІЛ 1. АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ…………….……..………….12
1.1. Постановка проблеми та актуальність обробки неструктурованих даних……………………………………………………..……………….….12
1.2 Поняття неструктурованих даних, їх роль у сучасних інформаційних системах………………………………………………...………….………..16
1.3 Методи та підходи до обробки текстових даних у системах штучного інтелекту ………………………………………………..…………………...20
1.4 Алгоритми й моделі обробки природної мови (NLP).……...………...20
1.5 Огляд інструментів і бібліотек для роботи з текстовими даними………………………………………………...……………………..20
    1.5.1 Традиційні бібліотеки Python для NLP (NLTK, spaCy, scikit-learn)……………………..…..……………...…………………………….....23
    1.5.2 Сучасні фреймворки, сервіси (Transformers, OpenAI API, LangChain, Unstructured)……………………..…..…………………….…...23
    1.5.3 Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)….……………………………………...23
1.6 Проблематика та напрями вдосконалення методів обробки неструктурованих даних………………………...…...……………………..20
1.7 Висновки до розділу 1……………………………..…………………...20
РОЗДІЛ 2. РОЗРОБКА МЕТОДУ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ……………………………………………………………………….…..26
2.1 Постановка задачі, вимоги та функціональні можливості системи…26
2.2 Архітектура методу обробки неструктурованих текстових даних…..30
2.3. Розробка алгоритму розбиття документів на семантичні фрагменти (чанки)………………………………………………………………….……33
2.4 Методи векторизації тексту та побудова семантичного індексу…...…………………….………………………………….…………26
2.5 Реалізація бази знань та пошукового механізму ……..…………..…..30
2.6 Висновки до розділу 2…………………………...……..…………..…..30
РОЗДІЛ 3. ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ІНТЕРФЕЙС СИСТЕМИ………………………………………………………….....…………36
3.1 Загальна структура програмного комплексу UXText Pipeline.………36
3.2 Інтеграція з бібліотекою Unstructured.io та FAISS ………….………..38
3.3 Реалізація API-сервісу (FastAPI) для обробки запитів.………....……42
3.4 Розробка користувацького інтерфейсу (Streamlit)…….…….………..38
3.5 Візуалізація результатів аналізу текстів .………...………...…………42
3.6 Тестування та оцінювання ефективності роботи системи.……..……42
3.7 Висновки до розділу 3.………...………………………….....…………42
ВИСНОВКИ……………………………..………………………………………48
СПИСОК ВИКОРИСТАНИХ ДЖЕРЕЛ……………...……………………..50
ДОДАТОК………………………………………………..……………..………52







ВСТУП
У сучасному світі, коли інформаційні потоки стрімко зростають, значна частина даних, які створює людина чи система, має неструктурований характер. Це тексти документів, електронні листи, новини, пости у соціальних мережах, коментарі, відгуки, звіти чи наукові статті. За статистичними оцінками, понад 80 відсотків усієї цифрової інформації становлять саме неструктуровані дані, які не мають чіткої схеми чи фіксованої структури зберігання. Проте саме вони містять найціннішу інформацію для аналізу змісту, виявлення тенденцій, автоматизованого прийняття рішень чи побудови систем штучного інтелекту. Проблема полягає в тому, що традиційні інструменти обробки даних не пристосовані для ефективної роботи з такими джерелами. Методи на кшталт статистичного аналізу або пошуку за ключовими словами не здатні розпізнавати контекст і семантику тексту. Це створює бар’єр у розвитку аналітичних систем, які мають розуміти не просто слова, а зміст і зв’язки між ними.
Актуальність теми зумовлена необхідністю створення ефективних методів, що дозволяють обробляти неструктуровані тексти, витягувати з них суттєву інформацію та перетворювати її у форму, придатну для подальшого аналізу. Розвиток технологій штучного інтелекту, зокрема векторизації текстів і нейромережевих моделей природної мови (NLP), відкриває нові можливості для створення інтелектуальних систем аналізу тексту. Ці системи здатні не лише класифікувати чи шукати інформацію, а й знаходити семантичні зв’язки між фрагментами тексту, узагальнювати їх зміст і будувати рекомендації. Саме тому тема «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» є актуальною і відповідає сучасним тенденціям розвитку інформаційних технологій.
Метою даної роботи є розробка методу та програмного забезпечення, що забезпечують ефективну обробку неструктурованих текстових даних шляхом поєднання алгоритмів нормалізації, семантичного подання та індексації тексту. Для досягнення поставленої мети необхідно було вирішити такі завдання: провести аналіз існуючих методів обробки текстових даних і сучасних підходів до представлення тексту; дослідити технології побудови векторних моделей, зокрема SentenceTransformers та FAISS; спроєктувати архітектуру програмного комплексу для інтеграції різних джерел даних; реалізувати систему UXText Pipeline, яка поєднує модулі обробки, зберігання та пошуку текстів; провести експериментальне тестування для оцінки ефективності запропонованого методу.
Об’єктом дослідження є процес обробки неструктурованих текстових даних у системах аналітики. Предметом дослідження — методи й алгоритми нормалізації, векторизації та індексації текстів, що дозволяють підвищити якість пошуку, класифікації й аналізу текстової інформації.
Під час виконання роботи застосовувалися методи аналітичного огляду та синтезу для вивчення літератури й порівняння існуючих підходів; алгоритмічного проєктування — для створення архітектури програмного комплексу; моделювання й експериментального аналізу — для перевірки ефективності запропонованого методу; а також статистичні методи оцінювання результатів, що дозволили кількісно виміряти покращення якості обробки текстів.
Наукова новизна роботи полягає у створенні універсального підходу до обробки неструктурованих даних, який поєднує модулі виділення тексту, нормалізації, семантичного подання та побудови індекса для швидкого пошуку. Запропонований метод дозволяє працювати з різними форматами документів (PDF, DOCX, HTML), об’єднуючи їх у єдину структуру даних без втрати змістової інформації. Крім того, у роботі реалізовано алгоритм побудови семантичного індекса з використанням SentenceTransformers і FAISS, що підвищує ефективність пошуку за змістом, а не лише за словами.
Практичне значення роботи полягає в тому, що розроблений програмний комплекс UXText Pipeline може бути використаний як основа для створення аналітичних систем у різних сферах — від бізнес-аналітики та освіти до наукових досліджень і документообігу. Він забезпечує автоматизацію процесів обробки великих обсягів текстових документів і підвищує точність інформаційного пошуку. Розроблені рішення можуть бути впроваджені в установах, де необхідна швидка обробка текстових звітів, статей або відгуків.
Результати дослідження були апробовані у науковій статті Оліфіренка Кирила Андрійовича «Метод обробки неструктурованих даних для вдосконалення систем аналізу тексту» (2024 р.) та при реалізації програмного прототипу, опублікованого у відкритому доступі на GitHub. Під час експериментів підтверджено ефективність запропонованого методу — час обробки документів скоротився, а точність семантичного пошуку зросла порівняно з базовими алгоритмами.
Дипломна робота складається зі вступу, чотирьох розділів, висновків, списку використаних джерел і додатків. Загальний обсяг становить 70 сторінок, включає 25 рисунків, 12 таблиць і 3 додатки. У роботі послідовно викладено теоретичні основи, аналіз існуючих рішень, власну методику обробки текстів, результати експериментів і приклади практичного використання створеного програмного комплексу.



РОЗДІЛ 1.
АНАЛІЗ ПРОБЛЕМИ ТА ТЕОРЕТИЧНІ ОСНОВИ ОБРОБКИ НЕСТРУКТУРОВАНИХ ДАНИХ


1.1. Постановка проблеми та актуальність обробки неструктурованих даних
У сучасному цифровому світі обсяг інформації, який щоденно створюється, зростає з небаченою швидкістю. За оцінками аналітичної компанії IDC (International Data Corporation), до 2025 року загальний обсяг глобальних даних перевищить 175 зетабайт, причому понад 80 відсотків із них становитимуть саме неструктуровані дані — тексти, зображення, відео, аудіозаписи, повідомлення в соціальних мережах тощо [1]. Ці дані генеруються користувачами, організаціями, сенсорами та системами штучного інтелекту. Проте, попри їхню кількість і потенційну цінність, лише невелика частина реально використовується для аналітики через відсутність ефективних методів автоматичної обробки.
Проблема полягає у тому, що більшість традиційних інформаційних систем побудовані для роботи зі структурованими даними — таблицями, числовими показниками, логами, базами SQL. Такі системи ефективно працюють лише тоді, коли дані мають чітко визначену схему зберігання: є поля, типи, формат і зв’язки між ними. Однак у випадку текстових документів, електронних листів, відгуків користувачів чи звітів у PDF цієї структури немає. Текстовий зміст може бути неоднорідним, містити візуальні елементи, коди, таблиці або навіть вбудовані зображення. Як наслідок, традиційні системи управління даними (DBMS) не здатні безпосередньо інтерпретувати зміст таких файлів, а тому значна частина потенційно корисної інформації залишається поза увагою аналітичних алгоритмів [2].
Я, Оліфіренко Кирило, як студент спеціальності «Комп’ютерні науки», у процесі дослідження дійшов висновку, що проблема обробки неструктурованих даних є не лише технічною, а й методологічною. Навіть у сучасних інформаційних середовищах — як у бізнесі, так і в державних установах — зберігаються великі масиви документів у різних форматах: звіти у PDF, службові документи у DOCX, переписки у форматі електронної пошти, повідомлення з месенджерів, а також текстові поля у CRM-системах. Всі ці джерела містять важливу інформацію, яку складно уніфікувати без єдиної системи автоматичної обробки.
Актуальність цієї проблеми підтверджується низкою об’єктивних чинників:
1.	Стрімке зростання обсягів текстових даних.
Щосекунди в інтернеті з’являються мільйони повідомлень — лише у Twitter щодня публікується понад 500 млн твітів, у Facebook — понад 300 млн постів, а також понад 4,5 млрд електронних листів у глобальному масштабі [3]. Така кількість текстів робить ручну обробку неможливою, що потребує створення інтелектуальних алгоритмів аналізу.
2.	Неоднорідність джерел та форматів.
Тексти можуть бути представлені в різних форматах (PDF, DOCX, HTML, TXT, Markdown), з різною структурою, мовою, стилем і контекстом. У багатьох випадках інформація поєднує текст і візуальні елементи (таблиці, формули, підписи до зображень), що створює додаткові труднощі при обробці.
3.	Зростання ролі неструктурованих даних у прийнятті рішень.
У багатьох сферах — бізнесі, медицині, освіті, науці, державному управлінні — від якісного аналізу текстової інформації залежить точність прогнозів і ефективність управлінських дій.
o	У бізнесі — аналіз відгуків клієнтів дозволяє виявляти рівень задоволеності.
o	У медицині — аналіз медичних карт допомагає визначати закономірності захворювань.
o	У науці — автоматизований аналіз публікацій сприяє швидкому пошуку нових знань.
4.	Відсутність універсального методу обробки.
Існуючі рішення часто обмежені конкретними форматами або мовами, а результати залежать від якості текстів, наявності помилок, скорочень, змішаних мов. Тому актуальним є завдання створення гнучкої системи, здатної об’єднувати різні етапи обробки — від вилучення тексту до його семантичного аналізу.
Ключовою метою сучасних досліджень у цій галузі є розробка методів, які поєднують традиційні алгоритми лінгвістичного аналізу з сучасними моделями машинного навчання та глибокого навчання (Deep Learning). Ці методи включають такі етапи, як:
•	вилучення тексту з документів різних форматів;
•	очищення від шуму (HTML-тегів, непотрібних символів);
•	токенізація й лематизація;
•	визначення мови;
•	побудова векторних подань (векторизація);
•	індексація та семантичний пошук.
Зокрема, у моїй роботі розглядається інтеграція бібліотек Unstructured.io (для вилучення тексту), SentenceTransformers (для векторизації текстових фрагментів) та FAISS (для семантичного пошуку за змістом). Такий підхід дозволяє створити універсальний пайплайн UXText Pipeline, який автоматизує процес перетворення неструктурованих даних у придатну для аналізу форму.
Разом із тим, залишається низка викликів, які обумовлюють актуальність подальших досліджень:
•	неоднорідність форматів і різна якість текстових даних;
•	потреба у великих обчислювальних ресурсах для сучасних моделей (BERT, GPT);
•	відсутність єдиних стандартів інтеграції NLP-моделей у корпоративні системи;
•	складність пояснення результатів глибоких моделей (“чорна скринька”);
•	обмежена підтримка української мови у більшості відкритих NLP-бібліотек.
З огляду на це, можна зробити висновок, що обробка неструктурованих даних є однією з центральних проблем сучасної науки про дані (Data Science). Її ефективне вирішення дозволить перейти від хаотичних текстових потоків до системного аналізу, що підвищить якість рішень у бізнесі, науці й суспільстві.
Таким чином, актуальність дослідження полягає у створенні комплексного методу обробки неструктурованих текстових даних, який забезпечить:
•	універсальність щодо типів і форматів документів;
•	автоматичність та масштабованість;
•	точність семантичного пошуку;
•	можливість інтеграції з реальними інформаційними системами.
Саме ця мета визначає подальшу логіку роботи — у наступних розділах буде розроблено архітектуру, реалізацію та апробацію програмного комплексу UXText Pipeline, який поєднує сучасні алгоритми NLP, бібліотеки Python і векторні бази даних для побудови ефективної системи аналізу неструктурованих текстів.


1.2. Поняття неструктурованих даних, їх роль у сучасних інформаційних системах
У сучасних інформаційних системах усі дані, з якими працюють користувачі, аналітики або системи штучного інтелекту, умовно поділяються на три основні типи — структуровані, напівструктуровані та неструктуровані. Така класифікація визначається рівнем організованості інформації, способом її зберігання та можливістю автоматизованої обробки [1]. Розуміння цих відмінностей є важливим для побудови ефективних систем аналітики, пошуку, прогнозування та управління знаннями.
Структуровані дані — це дані, які зберігаються у чітко визначеній формі та організовані за наперед встановленою схемою. Вони мають фіксовану структуру — таблиці, поля, типи даних, зв’язки між записами. Прикладами таких даних є бази даних SQL, електронні таблиці Excel, реєстри або облікові системи. Основною перевагою структурованих даних є можливість швидкого аналізу за допомогою мов запитів (SQL), статистичних методів чи систем бізнес-аналітики (BI). Проте їхній головний недолік полягає у тому, що вони описують лише формалізовану частину реальності — числа, категорії, дати, тоді як більшість якісних і контекстних характеристик лишається поза межами такого представлення [2].
Напівструктуровані дані займають проміжне місце між структурованими й неструктурованими. Вони мають певну внутрішню організацію, але не підпорядковуються жорсткій схемі. Такі дані часто представлені у форматах JSON, XML, YAML, CSV або у вигляді логів систем, де структура запису може змінюватися залежно від джерела. Наприклад, електронна пошта має стандартні поля (“від кого”, “кому”, “дата”), але зміст повідомлення — це вільний текст. Подібні дані активно використовуються у вебтехнологіях, обміні інформацією між сервісами (API) та у NoSQL-базах даних (MongoDB, Cassandra, Firebase). Їхня гнучкість є перевагою, однак аналіз таких даних потребує спеціалізованих методів парсингу, фільтрації й нормалізації [3].
Неструктуровані дані — це інформація, яка не має фіксованої структури і не може бути представлена у вигляді таблиць або полів. До них належать тексти документів, повідомлення у месенджерах, публікації у соціальних мережах, звіти, аудіо- та відеофайли, зображення, наукові статті, скани документів, а також будь-які інші ресурси, зміст яких не підпорядковується формальній моделі. Основна особливість таких даних полягає в тому, що вони передають значення через зміст, а не через форму. Два документи можуть мати зовсім різну структуру, але описувати те саме явище чи подію. Через це їхня обробка вимагає складних алгоритмів аналізу контексту, семантики, синтаксису і навіть прихованих зв’язків між словами [4].
За оцінками компанії IBM, близько 80% усієї інформації, створеної людьми, належить до неструктурованих даних [5]. Це означає, що лише невелика частина всієї інформації у світі може бути легко оброблена стандартними базами даних, тоді як решта — тексти, документи, звіти, статті, пости, коментарі — залишаються невикористаними. Саме тому обробка неструктурованих даних стає однією з найважливіших задач сучасної комп’ютерної науки та аналітики.
У контексті архітектури інформаційних систем неструктуровані дані зберігаються переважно у файлових системах, хмарних сховищах або об’єктних сховищах (object storage), де немає жорсткої схеми даних. Такі системи, як Amazon S3, Google Cloud Storage, Azure Blob Storage або Elasticsearch, дозволяють зберігати документи у будь-якому форматі та індексувати їх для пошуку. Проте ключову роль тут відіграють алгоритми попередньої обробки — OCR (розпізнавання тексту), токенізація, лематизація, класифікація та векторизація тексту, які переводять вміст у формат, придатний для аналітики [6].
Роль неструктурованих даних у сучасних ІТ-системах особливо важлива в контексті розвитку штучного інтелекту (AI) та Natural Language Processing (NLP). Саме ці дані є основним джерелом навчання для великих мовних моделей (LLM), таких як GPT, BERT, RoBERTa, LLaMA, T5 тощо. Їх тренування здійснюється на величезних масивах текстів — статтях, наукових працях, форумах, енциклопедіях, книгах [7]. Без таких джерел штучний інтелект не міг би досягти нинішнього рівня розуміння природної мови, контексту, емоційних відтінків і логічних зв’язків.
З точки зору бізнесу, аналіз неструктурованих даних відкриває нові можливості для прийняття стратегічних рішень. Компанії, що використовують автоматизовану аналітику текстів клієнтів, коментарів у соцмережах або результатів опитувань, отримують конкурентну перевагу — вони швидше реагують на ринкові зміни, прогнозують поведінку споживачів і вдосконалюють свої продукти [8]. У науці ці підходи дозволяють дослідникам обробляти тисячі наукових публікацій, що значно прискорює пошук нових знань. У медицині — аналіз клінічних описів допомагає автоматично виявляти симптоми, а в державному секторі — підвищує ефективність управлінських рішень.
Таблиця 1.1 Порівняльна характеристика структурованих, напівструктурованих та неструктурованих даних
Тип даних	Приклади	Характеристика	Приклади використання
Структуровані	SQL, Excel, CRM	Чітка схема, таблиці, поля	Облік, звітність, фінансовий аналіз
Напівструктуровані	JSON, XML, CSV	Часткова організація, відсутність жорсткої схеми	Обмін даними між системами, API
Неструктуровані	Тексти, PDF, аудіо, відео	Відсутня структура, потребує інтелектуальної обробки	NLP, аналітика, AI, Data Mining
Таким чином, неструктуровані дані становлять основну частину сучасного цифрового контенту. Вони є не лише джерелом знань, але й рушієм розвитку інтелектуальних технологій, аналітики та систем прийняття рішень. Їхнє ефективне використання вимагає поєднання методів комп’ютерної лінгвістики, машинного навчання та інформаційного моделювання. Саме тому в рамках цієї дипломної роботи я, Оліфіренко Кирило, досліджую методи, що дозволяють перетворювати неструктуровані тексти у структуровану форму для подальшого аналізу, що є важливим кроком до вдосконалення систем аналізу тексту.


1.3. Методи та підходи до обробки текстових даних у системах штучного інтелекту
Обробка текстових даних у системах штучного інтелекту є одним із найскладніших і водночас найдинамічніших напрямів розвитку сучасних інформаційних технологій. Людська мова — це гнучкий і багаторівневий механізм комунікації, який поєднує граматику, контекст, емоції, стиль та приховані смисли. Навчити комп’ютер розуміти текст означає створити модель, здатну не лише розпізнавати слова, а й аналізувати їхні зв’язки, контекст використання та семантичне навантаження [1].
Залежно від рівня складності та точності, методи обробки текстових даних умовно поділяються на три основні групи:
•	статистичні,
•	методи машинного навчання,
•	нейромережеві (глибокі).
Статистичні методи
Перші спроби автоматизованої обробки текстів базувалися саме на статистичних підходах, які не враховували змісту, проте дозволяли виявити закономірності у частоті вживання слів [2]. Найвідомішим став метод Bag of Words (BoW), у якому кожен документ подається як набір слів без урахування порядку. Такий підхід простий у реалізації, але не враховує контекст, тому використовується для класифікації текстів і тематичного аналізу.
Інший фундаментальний метод — TF-IDF (Term Frequency — Inverse Document Frequency) — оцінює важливість слова у конкретному документі відносно всієї колекції. Наприклад, слово “дані” зустрічається у всіх документах, тому має низьку вагу, тоді як термін “лематизація” зустрічається рідше, отже має більшу інформативність. TF-IDF залишається основою у пошукових системах (зокрема Google Search) і є відправною точкою для формування векторних подань текстів [3].
Методи машинного навчання
Подальший розвиток комп’ютерних технологій призвів до появи методів машинного навчання (Machine Learning), які дозволили враховувати закономірності, виявлені у великих наборах даних. Основна ідея полягає у тому, що алгоритм може навчитися розпізнавати структури тексту, якщо йому подати достатньо прикладів [4].
Серед найпоширеніших алгоритмів:
•	Наївний баєсівський класифікатор (Naive Bayes) — базується на теоремі Байєса і використовується для класифікації документів;
•	Метод опорних векторів (SVM) — визначає межі між класами у багатовимірному просторі ознак;
•	Метод k-ближчих сусідів (kNN) — класифікує тексти на основі схожості;
•	Логістична регресія — визначає ймовірність належності тексту до певної категорії.
Такі алгоритми застосовувалися у задачах аналізу тональності тексту (sentiment analysis), тематичної класифікації, виявлення спаму чи автоматичного реферування. Їхнім обмеженням є потреба у ручному створенні ознак (feature engineering) — тобто необхідність перетворювати тексти у числові вектори на основі частотних характеристик (уніграми, біграми, триграми тощо).
Нейромережеві методи
Наступним етапом еволюції стали нейромережеві моделі, які дозволили навчати систему без ручної побудови ознак. Основна ідея полягає у тому, що слова або речення можна подати у вигляді векторів (word embeddings), які зберігають інформацію про значення та контекст.
Першим проривом стала модель Word2Vec, розроблена в Google Research [5]. Вона навчалася на мільярдах слів і дозволяла розміщувати їх у спільному багатовимірному просторі, де схожі за змістом слова (наприклад, “кіт” і “пес”) знаходяться поруч. Подібні принципи використовують моделі GloVe (Global Vectors for Word Representation) [6] і FastText, розроблений у Meta AI Research, який враховує морфологічну структуру слова.
Головний прорив у галузі стався з появою трансформерних архітектур, які суттєво підвищили якість аналізу природної мови. Моделі типу BERT (Bidirectional Encoder Representations from Transformers) [7], GPT (Generative Pre-trained Transformer) [8] і SentenceTransformers використовують механізм Self-Attention (“механізм уваги”), який дозволяє моделі концентруватися на найважливіших словах у контексті. На відміну від попередніх моделей, трансформери навчаються розуміти значення не лише в межах речення, а й у всьому документі, що зробило можливим створення сучасних діалогових систем і когнітивних аналітичних інструментів.
Етапи попередньої обробки тексту
Перед застосуванням будь-яких алгоритмів штучного інтелекту текст проходить кілька стандартних етапів підготовки [9]:
1.	Токенізація — розбиття тексту на окремі одиниці (токени): слова, частини слів або символи.
2.	Нормалізація — приведення тексту до єдиного вигляду (зменшення регістру, видалення пунктуації, стоп-слів).
3.	Лематизація або стемінг — зведення слів до базової форми (наприклад, “працюю”, “працював” → “працювати”).
4.	POS-тегування (Part-of-Speech Tagging) — визначення частини мови для кожного слова.
5.	Синтаксичний аналіз (Parsing) — встановлення граматичних зв’язків між словами у реченні.
6.	Семантичний аналіз — визначення смислових відношень (“автор написав книгу” → дія, суб’єкт, об’єкт).
Таблиця 1.2 Еволюція методів обробки текстових даних
Покоління	Основні методи	Приклади моделей	Ключові характеристики
Статистичні	Bag of Words, TF-IDF	–	Простота реалізації, не враховує контекст
Машинного навчання	SVM, Naive Bayes, kNN	Scikit-learn	Використання вручну створених ознак, середня точність
Нейромережеві	Word2Vec, FastText, BERT, GPT	TensorFlow, PyTorch	Висока точність, контекстне розуміння, потреба у великих ресурсах
Інтеграція методів у сучасні системи
Сучасні системи штучного інтелекту об’єднують усі ці методи у єдиний пайплайн обробки тексту — від очищення даних до формування векторного подання та пошуку за змістом [10].
Типовий приклад:
1.	вилучення тексту з файлів різних форматів (PDF, DOCX, HTML);
2.	лематизація й очищення;
3.	перетворення тексту у вектори (SentenceTransformers);
4.	побудова індексу у FAISS;
5.	здійснення семантичного пошуку.
Саме цей підхід реалізовано у моєму дипломному проєкті UXText Pipeline, який демонструє практичну інтеграцію моделей NLP, бібліотеки Unstructured та індексаційного механізму FAISS у єдину систему.
Отже, методи обробки текстових даних пройшли довгий шлях від простих статистичних моделей до глибоких нейронних мереж, здатних розуміти контекст і значення. Їхня еволюція стала основою для створення сучасних систем штучного інтелекту, які не лише аналізують тексти, а й здатні взаємодіяти з користувачем природною мовою, пояснювати результати й формувати нові знання на основі отриманої інформації.


1.4. Алгоритми й моделі обробки природної мови (NLP)
Обробка природної мови (Natural Language Processing, скорочено NLP) є однією з ключових галузей штучного інтелекту, що вивчає методи взаємодії комп’ютерів із людською мовою. Основна мета NLP полягає у тому, щоб навчити машини розуміти, інтерпретувати й генерувати тексти так, як це робить людина [1]. Протягом останніх десятиліть у цій сфері було створено велику кількість алгоритмів і моделей, які поступово вдосконалювали здатність систем аналізувати тексти — від простого підрахунку частоти слів до глибокого семантичного розуміння контексту.
Класичні статистичні моделі
На початковому етапі розвитку NLP основну роль відігравали статистичні методи, засновані на кількісному аналізі тексту. Найвідомішим з них є Bag of Words (мішок слів) — підхід, у якому кожен документ представляється як набір унікальних слів без урахування їхнього порядку [2]. Наприклад, фрази “кіт спить на дивані” та “на дивані спить кіт” будуть представлені однаково, оскільки містять ті самі слова. Попри простоту реалізації, цей метод ігнорує синтаксичні та семантичні зв’язки, що обмежує його застосування в задачах, де важливий контекст.
Для підвищення точності аналізу було запропоновано метод TF-IDF (Term Frequency — Inverse Document Frequency) [3], який враховує не лише частоту слова в документі, а й його рідкість у колекції текстів. TF-IDF дозволяє надавати більшої ваги унікальним термінам, які є змістовно значущими для конкретного документа. Цей метод став базовим у побудові пошукових систем, тематичній класифікації та фільтрації спаму.
Векторні моделі представлення текстів
Подальшим етапом розвитку NLP стали векторні моделі, що дозволили описати слова у вигляді багатовимірних числових векторів. Такі моделі відображають семантичну близькість між словами — чим подібніші значення, тим менша відстань між їхніми векторами [4].
Однією з перших таких моделей стала Word2Vec, розроблена командою Google Research [5]. Вона дозволяє вловлювати смислові зв’язки між словами: наприклад, векторна операція “король” – “чоловік” + “жінка” ≈ “королева” демонструє, що модель засвоює відношення між поняттями.
Інша популярна модель — GloVe (Global Vectors for Word Representation), створена в Stanford NLP Group [6], поєднує статистичні принципи з нейронними мережами. Вона формує вектори на основі статистики спільних появ слів у тексті, що підвищує стабільність представлень.
Модель FastText, розроблена компанією Meta (Facebook) [7], враховує морфологічну структуру слів, розбиваючи їх на підрядки (n-грамні вектори). Завдяки цьому система може розуміти нові або рідкісні слова, що особливо корисно для української мови, де багато словоформ.
Таблиця 1.3 Порівняння класичних моделей представлення тексту
Модель	Принцип роботи	Переваги	Обмеження
Bag of Words	Підрахунок частоти слів у тексті	Простота реалізації, ефективність для малих даних	Ігнорує порядок і контекст
TF-IDF	Врахування частоти і рідкості слів	Підвищує точність пошуку, придатний для класифікації	Не враховує семантичні зв’язки
Word2Vec	Навчання векторів слів на великих корпусах	Враховує семантику, компактне представлення	Не розрізняє контексти одного слова
GloVe	Аналіз спільних появ слів у тексті	Поєднує статистику й нейронний підхід	Вимагає великих ресурсів
FastText	Представлення слова через підрядки	Добре працює з новими словами	Потребує тонкого налаштування параметрів

Контекстні моделі та архітектура трансформерів
Обмеження класичних моделей полягали в тому, що кожне слово мало одне фіксоване представлення, незалежно від контексту. У реченнях “банк річки” та “кредитний банк” слово “банк” має різні значення, однак старі моделі цього не враховували. Революційним рішенням стало впровадження трансформерних моделей, які враховують контекст кожного слова в межах речення або документа [8].
Модель BERT (Bidirectional Encoder Representations from Transformers), створена компанією Google AI [9], стала першою, що застосувала двонаправлену обробку тексту — зліва направо і справа наліво. Завдяки механізму Self-Attention BERT може розуміти, які слова в контексті впливають на значення інших, що забезпечує високу точність у задачах класифікації, пошуку відповідей і машинного перекладу. Модифікації RoBERTa, DistilBERT і ALBERT підвищили швидкість і ефективність обчислень без втрати якості [10].
Іншу еволюційну гілку представляють генеративні трансформери, серед яких найвідомішою є GPT (Generative Pre-trained Transformer), розроблена OpenAI [11]. GPT здатна не лише аналізувати текст, а й генерувати новий контент — логічно зв’язні речення, відповіді, коди чи описи. Її принцип полягає у передбаченні наступного слова за попереднім контекстом, що робить можливим природне спілкування людини з машиною.
Сучасні моделі векторизації тексту
Для практичних задач класифікації, кластеризації та семантичного пошуку активно застосовується модель SentenceTransformers, яка є модифікацією BERT і оптимізована для створення векторів речень або документів [12]. Вона дозволяє обчислювати семантичну подібність між двома текстами й знаходити найбільш релевантні відповіді.
Такий принцип лежить в основі семантичного пошуку, де збігаються не слова, а змісти. Наприклад, запит “літак затримався” може знайти документ із фразою “рейс відкладено”, навіть без жодного спільного слова. Саме це робить сучасні пошукові системи, чат-боти та аналітичні інструменти ефективнішими, ніж будь-коли раніше [13].
Узагальнення
Еволюція моделей обробки природної мови пройшла шлях від простих статистичних методів до складних багаторівневих нейромережевих архітектур.
Класичні моделі, як Bag of Words та TF-IDF, заклали математичну основу для подання текстів, тоді як Word2Vec, GloVe та FastText зробили перші кроки до розуміння семантики.
Натомість сучасні трансформери (BERT, GPT, SentenceTransformers) дозволили досягти справжнього контекстного розуміння мови, наблизивши штучний інтелект до людського рівня сприйняття.
У межах цієї дипломної роботи я, Оліфіренко Кирило, досліджую можливості поєднання таких моделей у програмному комплексі UXText Pipeline, який реалізує повний цикл обробки неструктурованих текстів — від вилучення й нормалізації до векторизації та семантичного пошуку.
 
Рис. 1.1. Схематичне представлення методу обробки неструктурованих текстових даних (UXText Pipeline)


1.5. Огляд інструментів і бібліотек для роботи з текстовими даними
У розвитку інструментів для обробки природної мови простежується еволюція від класичних бібліотек, орієнтованих на базові лінгвістичні операції, до сучасних фреймворків, що працюють із великими мовними моделями (LLM) та векторними базами даних. Для систематизації екосистеми NLP-технологій доцільно виділити три основні групи:
1.	традиційні бібліотеки Python для обробки текстів,
2.	сучасні фреймворки та сервіси глибокого навчання,
3.	інтеграційні рішення для побудови аналітичних систем.
Ця класифікація дозволяє простежити логічну послідовність розвитку підходів — від токенізації й морфологічного аналізу до комплексного семантичного пошуку та генеративного текстового інтелекту.


1.5.1. Огляд інструментів і бібліотек для роботи з текстовими даними
Першими інструментами, які забезпечили автоматизацію лінгвістичного аналізу, стали бібліотеки NLTK, spaCy та scikit-learn.
NLTK (Natural Language Toolkit) — одна з найстаріших бібліотек для роботи з природною мовою у Python [1]. Вона містить численні корпуси, словники, модулі токенізації, лематизації, частиномовного аналізу (POS-tagging) та синтаксичного розбору. NLTK широко використовується в освіті та дослідженнях, оскільки дозволяє швидко створювати експериментальні моделі та аналізувати тексти. Проте через низьку швидкодію і складність масштабування її рідко застосовують у великих проєктах.
spaCy стала відповіддю на потребу у високопродуктивних NLP-рішеннях. Вона орієнтована на промислове використання та забезпечує обробку текстів у десятки разів швидше, ніж NLTK [2]. SpaCy підтримує понад 60 мов, має модулі для виявлення іменованих сутностей (NER), синтаксичних залежностей і визначення частин мови. Завдяки інтеграції з TensorFlow і PyTorch, spaCy поєднує класичну обробку з глибоким навчанням.
scikit-learn, хоча й не є суто NLP-бібліотекою, використовується для класифікації текстів, тематичного моделювання та аналізу тональності [3]. Бібліотека надає інструменти для векторизації тексту (CountVectorizer, TF-IDF), а також алгоритми машинного навчання (SVM, логістична регресія, наївний Баєс). Саме scikit-learn забезпечила базову основу для перших систем автоматичного розпізнавання текстових закономірностей.
Таблиця 1.4 Порівняння традиційних бібліотек Python для NLP
Бібліотека	Основні функції	Переваги	Недоліки та обмеження
NLTK	Токенізація, лематизація, морфологічний аналіз	Простота, освітня цінність	Повільна робота, не для продакшну
spaCy	POS-tagging, NER, синтаксичний аналіз	Висока швидкодія, сучасні моделі	Обмежена кількість мовних ресурсів
scikit-learn	Класифікація, TF-IDF, машинне навчання	Гнучкість, модульність	Не обробляє мову напряму, потребує препроцесингу


1.5.1. Сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured)
З появою глибоких нейронних мереж і трансформерних архітектур відбувся різкий стрибок у можливостях NLP.
Фреймворк Transformers від Hugging Face став центральною платформою для роботи з великими мовними моделями [4]. Він надає доступ до сотень попередньо навчених моделей, серед яких BERT, GPT, T5, RoBERTa, DistilBERT, SentenceTransformers тощо. Завдяки спільноті користувачів і підтримці двох фреймворків (PyTorch і TensorFlow), Transformers забезпечує єдність наукових і прикладних досліджень у NLP.
Іншим ключовим інструментом є OpenAI API, що надає доступ до моделей сімейства GPT (Generative Pre-trained Transformer) [5]. Ці моделі дозволяють не лише аналізувати текст, а й генерувати узагальнення, відповіді, описи чи навіть програмний код, що відкриває новий рівень взаємодії людини та машини.
Фреймворк LangChain вирішує проблему інтеграції мовних моделей із базами знань, надаючи можливість побудови логічних ланцюгів (chains) між LLM і зовнішніми джерелами [6]. Цей підхід використовується у системах Retrieval-Augmented Generation (RAG), де модель не лише генерує текст, а й отримує релевантну інформацію з баз даних чи документів.
Для обробки документів різних форматів (PDF, DOCX, HTML, TXT) використовується бібліотека Unstructured.io, яка автоматично виділяє текстові елементи, розпізнає структуру документа і зберігає контекст [7]. Саме вона використовується в моєму проєкті UXText Pipeline для імпорту та нормалізації текстів перед векторизацією.
 

Рис. 1.2. Архітектура екосистеми інструментів NLP


1.5.3. Інтеграційні рішення для побудови текстових аналітичних систем (FAISS, ChromaDB, ElasticSearch)
Завершальним етапом є зберігання, індексація та пошук текстових даних. Для цього застосовуються інтеграційні рішення, які поєднують властивості баз даних та пошукових систем.
FAISS (Facebook AI Similarity Search) — бібліотека для швидкого пошуку найближчих сусідів у багатовимірних просторах [8]. Вона оптимізована для GPU та дозволяє працювати з мільйонами векторів, що робить її основою систем семантичного пошуку.
ChromaDB — векторна база даних нового покоління, яка інтегрується з мовними моделями через LangChain та інші API [9]. Вона використовується в системах типу RAG для пошуку інформації за змістом. ChromaDB проста у використанні та підтримує локальне й хмарне розгортання.
ElasticSearch — потужна пошукова платформа, яка поєднує індексацію текстів і машинне навчання [10]. Вона підтримує пошук як за ключовими словами, так і за векторними поданнями, завдяки чому часто використовується у корпоративних аналітичних системах.
Таблиця 1.5 Порівняльна характеристика інтеграційних інструментів
Інструмент	Тип рішення	Основні можливості	Переваги
FAISS	Бібліотека векторного пошуку	GPU-пошук, кластеризація, масштабованість	Висока швидкість, підтримка великих обсягів
ChromaDB	Векторна база даних	Семантичний пошук, інтеграція з LLM	Простота, інтеграція з LangChain
ElasticSearch	Пошукова система	Повнотекстовий та семантичний пошук	Масштабованість, корпоративний рівень
Таким чином, сучасна екосистема обробки текстів складається з трьох взаємодоповнювальних рівнів:
Базовий рівень — бібліотеки для лінгвістичного аналізу (NLTK, spaCy, scikit-learn).
Аналітичний рівень — фреймворки глибокого навчання (Transformers, LangChain, OpenAI API, Unstructured).
Інфраструктурний рівень — рішення для зберігання, пошуку та індексації даних (FAISS, ChromaDB, ElasticSearch).
Поєднання цих технологій створює основу для побудови інтелектуальних систем аналізу неструктурованих даних, здатних здійснювати пошук, класифікацію, узагальнення та інтерпретацію текстів із точністю, наближеною до людського розуміння.


1.6. Проблематика та напрями вдосконалення методів обробки неструктурованих даних
Проблематика обробки неструктурованих даних залишається однією з найактуальніших тем сучасної комп’ютерної лінгвістики та аналітики даних. За оцінками International Data Corporation (IDC), понад 80 % усієї інформації у світі має неструктурований характер — це тексти, аудіо, відео, зображення, документи різних форматів [1]. Попри суттєвий прогрес у розвитку штучного інтелекту та глибоких нейронних мереж, реальні виклики пов’язані не лише з алгоритмами, а й з їх практичним застосуванням, масштабуванням і пояснюваністю результатів.
Під час розроблення власної дипломної системи UXText Pipeline я, Оліфіренко Кирило, переконався, що найбільші труднощі виникають саме на стику технічних, мовних і організаційних аспектів — там, де алгоритми мають взаємодіяти з реальними, часто «брудними» даними користувачів.

Неоднорідність форматів і структур даних

Однією з найсерйозніших проблем є неоднорідність форматів джерел інформації. Тексти можуть бути представлені у вигляді PDF, DOCX, HTML, TXT, JSON, електронних листів або навіть відсканованих зображень. У кожного з форматів — своя структура, кодування, набір метаданих і правила відображення.
Наприклад, при вилученні тексту з PDF часто трапляється втрата логічної послідовності слів або ігнорування таблиць, що ускладнює подальшу обробку [2].
Для уніфікації таких даних застосовуються спеціальні парсери, зокрема бібліотека Unstructured.io, яка розділяє документ на структурні елементи — абзаци, списки, таблиці. Проте навіть вона не гарантує ідеального результату для зображень чи рукописних нотаток.
Таким чином, перший виклик — створення універсальних конвертерів, здатних коректно вилучати текст незалежно від формату вхідного документа.

Ресурсоємність і масштабування NLP-моделей

Сучасні моделі, такі як BERT, GPT чи T5, вимагають надзвичайно великих обчислювальних ресурсів. Для їхнього навчання потрібні потужні графічні процесори (GPU), великі обсяги оперативної пам’яті та тривалі обчислення [3]. Навіть часткове донавчання моделей для конкретного домену може тривати годинами чи днями.
У корпоративних і навчальних умовах це створює фінансові та енергетичні обмеження. Саме тому розвивається напрям ефективних моделей (efficient AI) — таких, як DistilBERT, TinyLlama чи ALBERT, які споживають менше ресурсів при збереженні високої точності [4].

Проблема пояснюваності (Explainability)

Ще один суттєвий виклик — інтерпретація результатів моделей. Класичні методи (TF-IDF, SVM) дають змогу зрозуміти, які саме ознаки вплинули на результат. Натомість нейронні мережі — це «чорні скриньки», логіка роботи яких не завжди зрозуміла [5].
У наукових або юридичних системах це неприйнятно, адже користувач має знати, чому модель зробила певний висновок. Саме тому активно розвивається напрям Explainable AI (XAI), що поєднує точність глибинного навчання з інтерпретованістю статистичних методів.
Прикладом можуть бути візуалізації важливості токенів у реченні чи методи LIME і SHAP, які показують, які слова вплинули на класифікацію.

Мовна неоднорідність і підтримка української мови

Суттєвим бар’єром залишається багатомовність і різна якість підтримки мов у бібліотеках. Для англійської мови існують великі корпуси, тоді як для української — вони обмежені. Наприклад, spaCy має часткову підтримку української морфології, але без повноцінного синтаксичного аналізу [6].
Це ускладнює автоматизацію аналітики в українських організаціях і підкреслює потребу у національних корпусах даних та відкритих моделях, адаптованих до українського контексту.

Інтеграція NLP із реальними ІТ-системами

Нерідко навіть високоточні моделі залишаються ізольованими — їх важко впровадити у реальні бізнес-процеси. CRM-платформи, ERP-системи чи документообіги використовують різні технологічні стеки (SQL, NoSQL, REST, GraphQL), тому інтеграція NLP-алгоритмів вимагає розробки універсальних API та конекторів [7].
Це стало передумовою появи таких фреймворків, як LangChain та LlamaIndex, що поєднують мовні моделі з базами даних і зовнішніми джерелами знань.

Якість текстових даних і семантична неоднозначність

Жодна модель не працює добре без якісних вхідних даних.
Помилки, жаргон, скорочення, неоднозначність (наприклад, слово «банк» — фінансова установа чи берег річки) значно впливають на результати [8]. Для їх усунення використовують етапи очищення, нормалізації, лематизації та векторизації, які дозволяють зменшити шум у даних.
Саме на цьому етапі реалізується найбільша частина інтелектуальної роботи у системах обробки неструктурованих текстів.

Безпека та конфіденційність даних

Оскільки більшість систем NLP працюють з чутливою інформацією, актуальним є питання захисту персональних даних і локальної обробки. Хмарні сервіси (як-от OpenAI API чи Claude AI) не завжди відповідають вимогам корпоративної або державної безпеки.
Тому зростає попит на офлайн-моделі та приватні рішення, що розгортаються у внутрішніх мережах підприємств без передачі даних третім сторонам [9].

Напрями вдосконалення

Для подолання наведених проблем сформувалися кілька ключових напрямів розвитку методів обробки неструктурованих даних:

Мультимодальні моделі — поєднання тексту, зображень, аудіо та відео (наприклад, CLIP, Gemini, Kosmos-1), що дозволяє системам аналізувати інформацію комплексно.

Оптимізація ресурсів — створення полегшених архітектур (DistilBERT, TinyLlama, Mistral 7B), які зберігають продуктивність, але працюють на споживацькому обладнанні.

Пояснюваність моделей (XAI) — розробка інтерпретованих методів аналізу, що дають змогу пояснювати результати користувачам.
Покращення якості даних — автоматичне виявлення шуму, нормалізація, переклад, фільтрація та сегментація контенту.
Стандартизація інтеграцій — розробка відкритих API та протоколів взаємодії для універсальної роботи NLP-модулів у різних системах.
Таблиця 1.6 Основні проблеми та напрями їх вирішення
Проблема	Причина	Можливі напрями вдосконалення
Неоднорідність форматів	Різні структури документів (PDF, HTML, DOCX)	Використання універсальних парсерів (Unstructured.io)
Висока ресурсоємність	Складні моделі з мільйонами параметрів	Використання полегшених архітектур (DistilBERT, TinyLlama)
Непояснюваність результатів	«Чорна скринька» нейронних мереж	Методи XAI, візуалізація уваги, SHAP-аналіз
Багатомовність	Обмежені корпуси української мови	Створення відкритих національних корпусів
Складність інтеграції	Несумісні технологічні стеки	Використання LangChain, REST API, FAISS
Якість тексту	Наявність шуму, помилок	Очищення, нормалізація, векторизація
Безпека даних	Обробка чутливої інформації	Локальні (on-premise) рішення, приватні LLM

 
Рис. 1.3. Узагальнена схема проблем і шляхів удосконалення методів обробки неструктурованих даних

1.7. Висновки до розділу 1
У першому розділі було проведено всебічний теоретичний аналіз проблеми обробки неструктурованих даних, окреслено сучасні тенденції та виклики, а також визначено методологічні основи, на яких базується подальша частина дипломного дослідження. Аналіз показав, що обсяг неструктурованої інформації у світі стрімко зростає, і більшість даних, які сьогодні генеруються людиною чи машинами, мають саме неструктурований характер — це тексти, зображення, відео, аудіо, повідомлення у соціальних мережах, електронна пошта, технічна документація тощо. Такі дані не мають чіткої схеми зберігання і потребують спеціальних методів для вилучення, структуризації та аналізу.
Було встановлено, що традиційні методи обробки текстової інформації, зокрема статистичні підходи на кшталт Bag of Words або TF-IDF, мають обмежену здатність до розуміння контексту та семантики. Їх перевагою є простота реалізації та зрозумілість результатів, однак у сучасних умовах, коли мова йде про глибоке семантичне розпізнавання, ці підходи стають недостатніми. Справжній прорив у галузі відбувся з появою нейронних моделей — Word2Vec, GloVe, FastText, BERT, GPT, SentenceTransformers — які дозволили машині розуміти значення слів у контексті, аналізувати зв’язки між фразами та реченнями, а також формувати осмислені векторні представлення тексту. Ці технології створили основу для побудови систем нового покоління — інтелектуальних пошукових механізмів, аналітичних платформ і мовних асистентів.
У ході огляду інструментів було виокремлено три рівні екосистеми для роботи з текстовими даними. Перший рівень становлять традиційні бібліотеки Python (NLTK, spaCy, scikit-learn), які забезпечують базову лінгвістичну обробку та підготовку текстів. Другий рівень охоплює сучасні фреймворки та сервіси (Transformers, OpenAI API, LangChain, Unstructured), що дозволяють реалізовувати глибокий семантичний аналіз, генерацію текстів і роботу з великими мовними моделями. Третій рівень формують інтеграційні рішення (FAISS, ChromaDB, ElasticSearch), які забезпечують збереження, векторизацію, пошук і зв’язок між компонентами аналітичної системи. Саме поєднання цих трьох груп інструментів створює можливість побудови комплексних систем для ефективної обробки неструктурованих даних.
Разом із тим, проведений аналіз виявив низку ключових проблем, які залишаються невирішеними у сфері NLP та аналітики текстів. Серед них — неоднорідність форматів даних, висока ресурсомісткість сучасних моделей, складність інтерпретації результатів, брак універсальних методів для багатомовного аналізу, а також питання інтеграції таких систем у реальні бізнес-процеси. Водночас визначено основні напрями вдосконалення існуючих методів: розвиток мультимодальних моделей, підвищення ефективності обчислень, створення пояснюваних моделей (Explainable AI) та вдосконалення інструментів попередньої обробки тексту.
Підсумовуючи результати теоретичної частини, можна зробити висновок, що побудова ефективних систем аналізу неструктурованих даних потребує поєднання класичних методів статистичної обробки з сучасними підходами глибокого навчання та трансформерних моделей. Це дозволяє досягнути балансу між точністю, швидкодією та інтерпретованістю результатів.
Саме тому у подальших розділах дипломної роботи буде розглянуто розробку універсального методу обробки неструктурованих текстових даних, який інтегрує сучасні бібліотеки (Unstructured, Transformers, SentenceTransformers, FAISS) у єдиний програмний пайплайн UXText Pipeline. Його метою є підвищення ефективності аналізу текстів, забезпечення автоматизованої обробки великих обсягів інформації та створення інструменту, придатного для практичного використання у системах аналітики, досліджень і бізнесу.

